\chapter{Introduction}

\section{Introduction and Motivation}

Gamma-ray spectroscopy is an important part of national security. Typically, this task is performed by a trained operator or a hand-held radioisotope identification device (RIID). 

This problem is made more difficult by 

The main question addressed in this work is: Can artificial neural networks (ANNs) using simulated spectra as training data automate isotope identification and quantification? Gamma-ray spectroscopists often use intuition when identifying isotopes in spectra. ANNs mimic this abstract analysis, synthesizing features of a gamma-ray spectrum in non-intuitive ways. Exploiting this intuition may overcome common hurdles encountered by other isotope identification algorithms. This work will demonstrate the performance of various ANNs for automated gamma-ray spectroscopy in low-resolution spectra. The low-resolution detector of interest in this work is a 2-inch by 2-inch sodium iodide (NaI) cylindrical scintillation detector. This detector is industry standard due to its ease of use, low cost, and acceptable resolution for gamma-ray spectroscopy. 

Despite it's widespread use, NaI detectors have several issues that complicate automated identification. The first issue is poor resolution compared to other more expensive radiation detection material, such as high purity germanium (HPGe). The lower-resolution makes some photopeaks unresolvable from each other, complicating identification. The second issue is calibration drift due to voltage drift in the detectors electronics or temperature changes. This calibration drift changes the locations and shapes of features in a gamma-ray spectrum, possibly confusing identification algorithms. While not specific to NaI detectors, the third issue affecting identification algorithms is background radiation from naturally occurring radioactive material (NORM). Background radiation can change both in intensity and composition based on location. This background signal can obscure features in gamma-ray spectra. Despite these drawbacks, we have demonstrated an ANN capable of identifying multiple isotopes in unknown backgrounds with a wide range of calibration settings \cite{kamudaThesis2017,kamuda2017}. Based on these preliminary results, additional experiments are planned to further explore automated spectroscopy using ANNs.

Planned experiments fall into two categories: new datasets and new training methods. The first new dataset is based on the the American National Standards Institute performance criteria for hand-held instruments for the detection and identification of radionuclides, ANSI N42-34-2006 \cite{ANSI}. This dataset will incorporate the effects of shielding, unknown gain drift, and an unknown background. The second dataset will be based on zero knowledge measurements for automated uranium enrichment calculations. The performance of ANNs trained on both datasets will be assessed on a series of real and simulated gamma-ray spectra. Improvements to the dataset simulation process will also be included. Improvements include new sampling methods for dataset construction and additional detector models to increase the ANNs generalization capabilities.

The new training methods include autoencoders, cross validation for the hyperparameter search, and bagging. Autoencoders will be used to separate the task of background subtraction and recalibration from identification. K-folds cross validation will be used in the hyperparameter search to more accurately determine proper hyperparameter values. Finally, bagging will be used to lower the variance output by the ANN. This lower variance will more accurately display the performance of a given ANN. 

\section{Neural Network History}


Artificial neural networks were first theorized in the 1940s as a model of how complex biological systems like groups of neurons learn and remember \cite{Pitts1943, Hebb1949}. These theories hypothesized that learning took place by reinforcing neural connections corresponding to correct behavior. The first implementation of an ANN came in 1958 in the form of The Perceptron \cite{Rosenblatt1958, Rosenblatt1962}. The Perceptron was a single layer neural network modeling a two-class image recognition problem. A class is a unique category, like dog and cat or on and off. There is no limit on the number of classes a model can attempt to learn. Another single layer neural network design was ADALINE, created in 1960 \cite{Widrow1960}. While the ADALINE algorithm shares a similar architecture with the perceptron, the learning rule is different. Unfortunately, the single-layer perceptron was proven to not work in cases where classes in the data are not linearly separable \cite{Minsky1969}. This realization led to a decline in neural network research.

While a single layer network could only solve linearly separable problems, in 1969 it was also found that a network with multiple layers could solve problems that are non-linearly separable \cite{Minsky1969}. The perceptron algorithm was limited to a single layer network due to its activation function being a non-differentiable step function. It was soon shown that multiple ADALINE neurons could be stacked on top of each other, creating a MADALINE network \cite{Ridgway1962}. Due to its multilayer structure, MADALINE is able to learn non-linearly separable functions. A MADALINE network was and is still used as an adaptive filter that removes echo from phone lines \cite{Widrow1988}.

Further advances in ANNs came in the form of learning improvements. Not long after the success of ADALINE and MADALINE, it was suggested that the concept of error propagation could be applied to ANNs \cite{Werbos1974, Rumelhart1986}. Additionally, it was shown that error backpropogation applied with a differentiable non-linear activation function was an extremely powerful method to train ANNs \cite{Murtagh1991}. Algorithms based on the backpropogation of error are now the most common method to train ANNs.

Currently, ANNs can solve many diverse problems. ANNs have shown promise in everyday problems such as handwritten zip code recognition \cite{LeCun1989}, fingerprint identification \cite{Jeyanthia2015}, and image recognition \cite{Krizhevsky2012}; as well as more complicated problems such as lung cancer classification based on MRI images \cite{Selvakumari2016}, estimating surface soil moisture from high-resolution aerial images of cropland \cite{Hassan-Esfahani2015}, and stock market predition \cite{Rababaah2015}.


\section{Gamma-Ray Spectroscopy for Isotope Identification}

Traditionally, isotope identification is conducted by a trained spectroscopist. Rawool-Sullivan et al. identified a common workflow performed by a group of gamma-ray spectroscopists \cite{Sullivan2010}. This workflow includes discriminating background and source photopeaks, adjusting the calibration using background photopeaks and checking for shielding effects in the low-energy photopeaks. Once photopeaks are identified, the spectroscopist would use their prior knowledge of isotope emissions (or consult a database of these emissions) to match isotopes to the spectrum. The researchers also noted that while  spectroscopists used this book knowledge, they often would use intuition developed from analyzing tens or hundreds of gamma-ray spectrum. The researchers also noted the difficulty in incorporating this subjective analysis into an automated algorithm.
% This is one of the main arguments for using ANNs in automated isotope ID


There are many automated radioisotope identification methods available, but few perform well given a low-resolution gamma-ray spectrum of a mixture of radioisotopes. Common methods include library comparison algorithms, region of interest (ROI) algorithms, principle component analysis (PCA), and template matching.

Library comparison algorithms attempt to match photopeak energies found in a gamma-ray spectrum with those found in a library of known isotope decay energies. Drifts and uncertainties in detector calibration can lead to misidentifying photopeaks, leading to incorrect isotope identifications \cite{burr2009}. To be automated, this method needs an algorithm to extract photopeak centroids despite calibration drift and unknown background radiation. Photopeak extraction algorithms face difficulties when a large number of photopeaks overlap in a spectrum, such as when a mixture of radio-isotopes are measured with a low-resolution detector \cite{xiong2015}.

ROI algorithms search for elevated counts compared to background in a region where target radioisotope photopeaks are expected. ROI algorithms may also operate poorly when photopeaks of different radioisotopes overlap \cite{burr2009}. For this reason, large isotope libraries will perform poorly using this method. Similarly to the library comparison algorithm, calibration drift may shift photopeaks into different neighboring ROIs, leading to incorrect identification. The ROI method has been used to differentiate normally occurring radioactive material (NORM) from special nuclear material (SNM) using plastic scintillators \cite{Ely2006}.

PCA can also be applied to radioisotope identification. The goal of PCA is to reduce the dimensionality of a dataset into uncorrelated variables \cite{Jolliffe2002}. Using a few of these principle components, the data may be represented in a reduced space that contains most of the information present in the original data. The transformed data can then be clustered based on isotope identity. Clustering algorithms may include K-means or Mahalanobis distance \cite{Kanungo2002, Kumari2012}. PCA has been applied to isotope identification using plastic scintillators \cite{Boardman2012} and anomaly detection using both plastic scintillators and NaI detectors \cite{runkle2006b}. Despite the progress of PCA in some isotope identification problems, there has not been significant progress in applying PCA to separating mixtures of isotopes in gamma-ray spectra.

Template matching algorithms find an example in a database of gamma-ray spectra that most closely matches a measured spectrum \cite{burr2009}. The database of spectra can contain multiple detector calibration settings, shielding materials, and source-to-detector distances. Goodness of fit can be measured using a hypothesis test such as chi-squared test, euclidean distance, or Mahalanobis distance. While a sufficient amount of example spectra can be used to identify almost any measured spectrum, the drawback of this method is the time necessary to compare a measured spectrum to the library and the computer memory necessary to store said library. This method also may have difficulty when mixtures of isotopes are considered, although work is being done to correct this \cite{mattingly2010}.

These algorithms largely incorporate book knowledge. By further incorporating the intuition identified by Rawool-Sullivan et al., these algorithms may be improved. A machine learning approach to automated gamma-ray spectroscopy may be able to marry book knowledge and a trained spectroscopists intuition.


\section{Automated Isotope Identification Using ANNs}

There have been a number of published papers which apply ANNs to automated isotope identification. ANNs have been applied to peak fitting \cite{Abdel-Aal2002}, isotope identification \cite{Abdel-Aal1996, Medhat2012}, and activity estimation \cite{Abdel-Aal1996, Vigneron1996}. Many of these works rely on ROI methods \cite{Pilato1999}, feature extraction \cite{Chen2009}, high-resolution gamma-ray spectra as the input to the ANN \cite{Yoshida2002}, small libraries of isotopes, and assume perfectly calibrated detectors. ANN training methods created for high-resolution gamma-ray spectra may not perform well when trained using low-resolution spectra. Because of the large discrepancy in resolution, the features exploited by a ANN trained on high-resolution spectra would be different than low-resolution spectra. In addition to this, ANN training that relies on ROI methods may not perform well when ROIs overlap significantly with large libraries of isotopes. Feature extraction and ROI methods may also falter when the background radiation field is unknown or the detector's calibration is unreliable.  


\section{Chapter Conclusion}

To overcome the issues outlined above, instead of training an ANN using predetermined ROIs or feature extraction, it may be better to train the ANN with an entire gamma-ray spectrum. Due to perceived training issues associated with using the entire spectrum \cite{Pilato1999,Yoshida2002}, this approach has been avoided in previous works. Despite this, we have shown that training an ANN using the full spectrum is a viable method to identify and quantify isotopes in gamma-ray spectra \cite{kamuda2017,kamudaThesis2017}. There is also evidence that this method can overcome automated spectroscopy issues like gain shifts and identifying isotopes in spectra without clear isotopic features.

The solution outlined in this thesis is to perform radioisotope identification and quantification simultaneously using an ANN. Skipping automated peak-fitting routines releases the algorithm from the burden of determining proper peak-fitting subroutines for the variety of cases in which a peak may be seen. A photopeak can typically be fit using a Gaussian distribution added to a linear baseline. This baseline changes based on its location in another photopeak's Compton continuum. In spectra where peaks overlap, deconvolution techniques may be needed to resolve constituent peaks. An unknown gain or poorly calibrated detector will shift photopeaks, further complicating a spectrum. Instead of making deterministic rules for each of these cases, a machine learning algorithm can be taught to recognize and handle them automatically.

Traditionally, once photopeaks are found, another algorithm is needed to measure the locations of peak centroids and additional information (peak area, area uncertainty) to identify what isotopes are present in a spectrum. This process adds computation time and again suffers from the need to be modified to handle changes in a spectrum as described above. Also, the algorithm that performs identification based on peak information must be tailored to the peak-fitting routine. This requires unique algorithms to be created for different detector materials and sizes, as they change the shape of the spectrum. Using an ANN with an appropriate training set, these problems can be avoided.

By allowing a machine to learn the important features of a gamma-ray spectrum, the problem of determining the best analysis technique given a spectrum with significant features overlap and unknown calibration is avoided. This method is ideal for low-resolution gamma-ray spectroscopy for mixtures of radioisotopes for a number of reasons. Because this method uses simulated gamma-ray spectra to train the ANN, there is no restriction on the number of isotopes allowed in the library or their identity. This allows the ANN training set to be cheaply generated using mixtures of exotic, dangerous, or short-lived isotopes that are not easily accessible in an academic setting. 

Another benefit of using a training set of simulated spectra is that an ANN may be trained for a specific scenario using a custom isotope library. The isotope library requirements for a border patrol, which may focus on distinguishing medical isotopes and NORM from SNM, are very different from the isotope library needed to perform post-detonation nuclear forensics. A simulated training set also allows the same ANN creation process to be applied to different detector materials. This is because different gamma-ray detector materials, such as CZT or HPGe, create spectra with different features. Because spectra can be generated with different photomultiplier tube (PMT) gain values, this method can be insensitive to a range of gain shift. This insensitivity would allow isotope identification and quantification to be performed without prior knowledge of the detectors calibration.


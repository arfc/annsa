{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/ubuntu/Notebooks/annsa/\")\n",
    "\n",
    "import tensorflow.contrib.eager as tfe\n",
    "from sklearn import datasets, preprocessing, model_selection\n",
    "\n",
    "\n",
    "import seaborn as sn\n",
    "import annsa as an\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.init_ops import glorot_uniform_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class simple_nn(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(simple_nn, self).__init__()\n",
    "        \"\"\" Define here the layers used during the forward-pass \n",
    "            of the neural network.\n",
    "        \"\"\"\n",
    "        l2_regularization_scale=1e-1\n",
    "        nodes_layer_1=500\n",
    "        nodes_layer_2=200\n",
    "        dropout_probability=0.5 \n",
    "        \n",
    "        # define l2 regularization\n",
    "        self.regularizer = tf.keras.regularizers.l2(l=l2_regularization_scale)\n",
    "        #tf.contrib.layers.l2_regularizer(scale=10.0,scope='simple_nn')\n",
    "        \n",
    "        # Hidden layer.\n",
    "        self.dense_layer1 = tf.layers.Dense(nodes_layer_1, \n",
    "                                            activation=tf.nn.relu,\n",
    "                                            kernel_initializer=glorot_uniform_initializer(),\n",
    "                                            kernel_regularizer=self.regularizer)\n",
    "        self.drop1 = tf.layers.Dropout(dropout_probability)\n",
    "        self.dense_layer2 = tf.layers.Dense(nodes_layer_2,\n",
    "                                            activation=tf.nn.relu,\n",
    "                                            kernel_initializer=glorot_uniform_initializer(),\n",
    "                                            kernel_regularizer=self.regularizer)\n",
    "        self.drop2 = tf.layers.Dropout(dropout_probability)\n",
    "        # Output layer. No activation.\n",
    "        self.output_layer = tf.layers.Dense(32, activation=None)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict_logits(self, input_data):\n",
    "        \"\"\" Runs a forward-pass through the network.     \n",
    "            Args:\n",
    "                input_data: 2D tensor of shape (n_samples, n_features).   \n",
    "            Returns:\n",
    "                logits: unnormalized predictions.\n",
    "        \"\"\"\n",
    "        # Reshape input data\n",
    "        input_data = tf.reshape(input_data,[-1,1,1024])\n",
    "        hidden_activation1 = self.dense_layer1(input_data)\n",
    "        hidden_activation1_dropout = \n",
    "        \n",
    "        hidden_activation2 = self.dense_layer2(hidden_activation1)\n",
    "        logits = self.output_layer(hidden_activation2)\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def predict(self, input_data):\n",
    "        \"\"\" Runs a forward-pass through the network.     \n",
    "            Args:\n",
    "                input_data: 2D tensor of shape (n_samples, n_features).   \n",
    "            Returns:\n",
    "                logits: unnormalized predictions.\n",
    "        \"\"\"\n",
    "        # Reshape input data\n",
    "        input_data = tf.reshape(input_data,[-1,1,1024])\n",
    "        hidden_activation1 = self.dense_layer1(input_data)\n",
    "        hidden_activation2 = self.dense_layer2(hidden_activation1)\n",
    "        logits = self.output_layer(hidden_activation2)\n",
    "        outputs = tf.nn.softmax(logits)\n",
    "        return outputs\n",
    "    \n",
    "    def loss_fn(self, input_data, target):\n",
    "        \"\"\" Defines the loss function used during \n",
    "            training.         \n",
    "        \"\"\"\n",
    "        logits = self.predict_logits(input_data)\n",
    "        cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=target, logits=logits))\n",
    "        l2_weights = [model.trainable_weights[i] for i in range(len(model.trainable_weights)) if i%2==0]\n",
    "        l2_loss = tf.contrib.layers.apply_regularization(self.regularizer, l2_weights)\n",
    "        loss = cross_entropy_loss+l2_loss\n",
    "        return loss\n",
    "    \n",
    "    def grads_fn(self, input_data, target):\n",
    "        \"\"\" Dynamically computes the gradients of the loss value\n",
    "            with respect to the parameters of the model, in each\n",
    "            forward pass.\n",
    "        \"\"\"\n",
    "        with tfe.GradientTape() as tape:\n",
    "            loss = self.loss_fn(input_data, target) \n",
    "        return tape.gradient(loss, self.variables)\n",
    "    \n",
    "    def fit(self, input_data, target, optimizer, num_epochs=500, verbose=50):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs.\n",
    "        \"\"\"\n",
    "        for i in range(num_epochs):\n",
    "            grads = self.grads_fn(input_data, target)\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Loss at epoch %d: %f' %(i+1, self.loss_fn(input_data, target).numpy()))\n",
    "                \n",
    "    \n",
    "    def fit_batch(self, train_dataset, optimizer, num_epochs=50, verbose=50):\n",
    "        \"\"\" Function to train the model, using the selected optimizer and\n",
    "            for the desired number of epochs.\n",
    "        \"\"\"\n",
    "        for i in range(num_epochs):\n",
    "            for (input_data, target) in tfe.Iterator(train_dataset.shuffle(1000).batch(32)):\n",
    "                grads = self.grads_fn(input_data, target)\n",
    "                optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Loss at epoch %d: %f' %(i+1, self.loss_fn(input_data, target).numpy()))             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = np.load('/home/ubuntu/Notebooks/GADRAS_ANN_work/Project_SORMA18/Dataset_Details/spectrum_data_1-simplex_1E6.npy')[:-10000]\n",
    "training_keys = np.load('/home/ubuntu/Notebooks/GADRAS_ANN_work/Project_SORMA18/Dataset_Details/key_data_1-simplex_1E6.npy')[:-10000]\n",
    "\n",
    "training_data = an.log_normalize(training_data)\n",
    "training_keys = an.normalize_data(training_keys)\n",
    "\n",
    "\n",
    "X_tensor = tf.constant(training_data[0:100])\n",
    "#y_tensor = tf.reshape(tf.constant(training_keys),[-1,32,1])\n",
    "y_tensor = tf.constant(training_keys[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_tensor, y_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an optimizer, model, and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 1: 135567.405527\n",
      "Loss at epoch 50: 142.032700\n",
      "Loss at epoch 100: 23.485587\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(5e-1)\n",
    "model = simple_nn()\n",
    "model.fit_batch(train_dataset, optimizer, num_epochs=100, verbose=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('U233', 0.9613300859883692)\n",
      "('Back_U', 0.01466275659824047)\n",
      "('Back_K', 0.01446393955962026)\n",
      "('Back_Th', 0.009543217853770068)\n",
      "('U235', 0.0)\n",
      "\n",
      "\n",
      "('Back_U', 0.17874386273951262)\n",
      "('Back_Th', 0.1550665890994291)\n",
      "('Back_K', 0.13608594721507095)\n",
      "('Cs137', 0.05527810998808426)\n",
      "('Ir192', 0.0510488632334397)\n"
     ]
    }
   ],
   "source": [
    "index = 6\n",
    "\n",
    "logits_test = model.predict(training_data[index])\n",
    "\n",
    "an.results2(training_keys[index],5)\n",
    "print '\\n'\n",
    "an.results2(logits_test.numpy()[0][0],5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify checkpoint directory\n",
    "checkpoint_directory = './eager-save-test.ckpt'\n",
    "# Create model checkpoint\n",
    "checkpoint = tfe.Checkpoint(optimizer=optimizer,\n",
    "                            model=model,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Checkpoint' object has no attribute 'save_network_checkpoint'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-85511d6b087d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Save trained model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_network_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckpoint_directory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Checkpoint' object has no attribute 'save_network_checkpoint'"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "checkpoint.save_network_checkpoint(file_prefix=checkpoint_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Restore Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver.restore('/tmp/ckpt')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Reinitialize model instance\n",
    "model = simple_nn()\n",
    "optimizer = tf.train.GradientDescentOptimizer(5e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Specify checkpoint directory\n",
    "checkpoint_directory = './eager-save-test.ckpt'\n",
    "# Create model checkpoint\n",
    "checkpoint = tfe.Checkpoint(optimizer=optimizer,\n",
    "                            model=model,\n",
    "                            optimizer_step=tf.train.get_or_create_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.InitializationOnlyStatus at 0x7f838c507690>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restore model from latest chekpoint\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=61732, shape=(), dtype=float64, numpy=0.5215111970901489>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.loss_fn(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.03277973  0.42256092]\n",
      " [-0.17492533  1.08905442]\n",
      " [-0.15110942 -1.01793212]\n",
      " [-0.08231696  1.05194264]\n",
      " [-0.34405381 -0.23703221]\n",
      " [-0.46417323 -0.72362408]\n",
      " [-0.07200287  0.23013546]\n",
      " [-0.09701915  1.09780864]\n",
      " [-0.10120163  1.05055332]\n",
      " [ 0.03282835 -1.1802948 ]\n",
      " [-0.15293705  0.83410911]\n",
      " [-0.12119496  1.18870982]\n",
      " [-0.08622217  1.06893659]\n",
      " [-0.33596162 -0.26602851]\n",
      " [-0.04915239  0.32396122]\n",
      " [-0.31852946 -0.04176902]\n",
      " [-0.49785829 -0.76155645]\n",
      " [-0.33915985 -0.16889622]\n",
      " [-0.08941315  0.23483927]\n",
      " [-0.07538378  1.0626397 ]], shape=(20, 2), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "logits_test = model.predict(X_test)\n",
    "\n",
    "print(logits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "\n",
    "from annsa.template_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model, training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annsa.model_classes import (cnn1d_model_features,\n",
    "                                 CNN1D,\n",
    "                                 generate_random_cnn1d_architecture,\n",
    "                                 save_model,\n",
    "                                 train_earlystop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_dataset = pd.read_csv('../../source-interdiction/training_testing_data/background_template_dataset.csv')\n",
    "source_dataset = pd.read_csv('../../source-interdiction/training_testing_data/shielded_templates_200kev_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourcedist: [50.0, 112.5, 175.0, 237.5, 300.0]\n",
      "sourceheight: [50.0, 75.0, 100.0, 125.0, 150.0]\n",
      "alum shieldingdensity: [1.82, 4.18, 7.49, 13.16]\n",
      "iron shieldingdensity: [1.53, 3.5, 6.28, 11.02]\n",
      "lead shieldingdensity: [0.22, 0.51, 0.92, 1.61]\n",
      "fwhm: [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0]\n"
     ]
    }
   ],
   "source": [
    "print('sourcedist: ' + str(sorted(set(source_dataset['sourcedist']))))\n",
    "print('sourceheight: ' + str(sorted(set(source_dataset['sourceheight']))))\n",
    "print('alum shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='alum']['shieldingdensity']))))\n",
    "print('iron shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='iron']['shieldingdensity']))))\n",
    "print('lead shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='lead']['shieldingdensity']))))\n",
    "print('fwhm: ' + str(sorted(set(source_dataset['fwhm']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = source_dataset[source_dataset['fwhm']==7.5]\n",
    "source_dataset = source_dataset[source_dataset['sourcedist']==175.0]\n",
    "source_dataset = source_dataset[source_dataset['sourceheight']==100.0]\n",
    "\n",
    "# remove 80% shielding\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=13.16]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=11.02]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=1.61]\n",
    "\n",
    "# remove 60% shielding\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=7.49]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=6.28]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=0.92]\n",
    "\n",
    "# remove 40% shielding\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=4.18]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=3.5]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=0.51]\n",
    "\n",
    "# remove 20% shielding\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=1.82]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=1.53]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=0.22]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove empty spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicies dropped: []\n"
     ]
    }
   ],
   "source": [
    "zero_count_indicies = np.argwhere(np.sum(source_dataset.values[:,6:],axis=1) == 0).flatten()\n",
    "\n",
    "print('indicies dropped: ' +str(zero_count_indicies))\n",
    "\n",
    "source_dataset.drop(source_dataset.index[zero_count_indicies], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add empty spectra for background "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_spectra = []\n",
    "for fwhm in set(source_dataset['fwhm']):\n",
    "    num_examples = source_dataset[(source_dataset['fwhm']==fwhm) &\n",
    "                                  (source_dataset['isotope']==source_dataset['isotope'].iloc()[0])].shape[0]\n",
    "    for k in range(num_examples):\n",
    "        blank_spectra_tmp = [0]*1200\n",
    "        blank_spectra_tmp[5] = fwhm\n",
    "        blank_spectra_tmp[0] = 'background'\n",
    "        blank_spectra_tmp[3] = 'background'\n",
    "        blank_spectra.append(blank_spectra_tmp)\n",
    "\n",
    "source_dataset = source_dataset.append(pd.DataFrame(blank_spectra,\n",
    "                                                    columns=source_dataset.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset from spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_dataset = source_dataset.values[:,5:].astype('float64')\n",
    "all_keys = source_dataset['isotope'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    '''\n",
    "    Makes a random model given some parameters.\n",
    "\n",
    "    '''\n",
    "    cnn_filters_choices = (\n",
    "                       (8, 16),\n",
    "                       (8, 16),\n",
    "                      )\n",
    "\n",
    "    cnn_kernel_choices = ((4,),)\n",
    "    pool_size_choices = ((4,),)\n",
    "\n",
    "    \n",
    "    model_features = generate_random_cnn1d_architecture(\n",
    "        cnn_filters_choices= cnn_filters_choices,\n",
    "        cnn_kernel_choices=cnn_kernel_choices,\n",
    "        pool_size_choices=pool_size_choices,\n",
    "    )\n",
    "    model_features.trainable = True\n",
    "    model_features.learining_rate = 10**np.random.uniform(-4,-1)\n",
    "    model_features.batch_size = 2**np.random.randint(4,6)\n",
    "    model_features.output_size = all_keys_binarized.shape[1]\n",
    "    model_features.scaler = choice([make_pipeline(FunctionTransformer(np.log1p, validate=True)),\n",
    "                                    make_pipeline(FunctionTransformer(np.sqrt, validate=True))])\n",
    "    model_features.activation_function = tf.nn.tanh\n",
    "    model_features.output_function = None\n",
    "    model_features.Pooling = tf.layers.MaxPooling1D\n",
    "    model_features.l2_regularization_scale = 10**np.random.uniform(-3,0)\n",
    "    model_features.dropout_probability = np.random.uniform(0,1)\n",
    "    model_features.pool_strides = ((2,2,2))\n",
    "    number_layers = choice([1, 2, 3])\n",
    "    dense_nodes = 2**np.random.randint(4,8,number_layers)\n",
    "    dense_nodes = np.sort(dense_nodes)\n",
    "    dense_nodes = np.flipud(dense_nodes)\n",
    "    \n",
    "    model_features.dense_nodes = dense_nodes\n",
    "\n",
    "    model = CNN1D(model_features)\n",
    "\n",
    "    return model, model_features \n",
    "\n",
    "model_class = CNN1D\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define online data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration_time():\n",
    "    return np.random.uniform(np.log10(60),np.log10(600))\n",
    "\n",
    "def background_cps():\n",
    "    return np.random.poisson(200)\n",
    "\n",
    "def signal_to_background():\n",
    "    return np.random.uniform(0.5,2)\n",
    "\n",
    "def calibration():\n",
    "    return [np.random.uniform(0,10),\n",
    "            np.random.uniform(2800/3000,3200/3000),\n",
    "            0]\n",
    "\n",
    "online_data_augmentation = online_data_augmentation_vanilla(background_dataset,\n",
    "                                background_cps,\n",
    "                                integration_time,\n",
    "                                signal_to_background,\n",
    "                                calibration,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create temporary testing dataset based on training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_isotope=10\n",
    "\n",
    "testing_spectra = []\n",
    "testing_keys = []\n",
    "\n",
    "for key in set(all_keys):\n",
    "    for _ in range(examples_per_isotope):\n",
    "        testing_spectra_tmp = source_dataset[source_dataset['isotope']==key].sample().values[:,5:].astype('float64')\n",
    "        testing_spectra_tmp = online_data_augmentation(testing_spectra_tmp).numpy()[0]\n",
    "        testing_keys.append(key)\n",
    "        testing_spectra.append(testing_spectra_tmp)\n",
    "\n",
    "testing_spectra = np.array(testing_spectra)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='CNN-kfoldseasy_1'\n",
    "with open('../hyperparameter_search/hyperparameter-search-results/'+model_id,\"rb\" ) as f:\n",
    "        model_features=pickle.load(f)\n",
    "\n",
    "\n",
    "model_class = CNN1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='CNN_onlinedataaugeasy_updates_log10integrationtime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-8cbddad4a35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0maugment_testing_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mprint_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         record_train_errors=False)\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtraining_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/annsa-0.1.dev0-py3.6.egg/annsa/model_classes.py\u001b[0m in \u001b[0;36mfit_batch\u001b[0;34m(self, train_dataset, test_dataset, optimizer, num_epochs, verbose, print_errors, earlystop_patience, max_time, not_learning_patience, not_learning_threshold, obj_cost, earlystop_cost_fn, data_augmentation, augment_testing_data, record_train_errors)\u001b[0m\n\u001b[1;32m    366\u001b[0m                              \u001b[0mobj_cost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                              \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 368\u001b[0;31m                              data_augmentation)\n\u001b[0m\u001b[1;32m    369\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecord_train_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                 \u001b[0mtraining_data_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/annsa-0.1.dev0-py3.6.egg/annsa/model_classes.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_dataset_tensor, obj_cost, optimizer, data_augmentation)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \"\"\"\n\u001b[1;32m    252\u001b[0m         for (input_data, target) in tfe.Iterator(\n\u001b[0;32m--> 253\u001b[0;31m                 train_dataset_tensor.shuffle(int(1e8)).batch(self.batch_size)):\n\u001b[0m\u001b[1;32m    254\u001b[0m                 \u001b[0minput_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0;31m# check if data_augmentation returns separate source and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/contrib/eager/python/datasets.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m     70\u001b[0m             context.context().device_name))\n\u001b[1;32m     71\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             output_shapes=self._flat_output_shapes)\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m         \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         self._resource_deleter = resource_variable_ops.EagerResourceDeleter(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2551\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   2552\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MakeIterator\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2553\u001b[0;31m         name, _ctx._post_execution_callbacks, dataset, iterator)\n\u001b[0m\u001b[1;32m   2554\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2555\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlb=LabelBinarizer()\n",
    "\n",
    "training_errors = []\n",
    "total_networks = 10\n",
    "for network_id in range(total_networks):\n",
    "    # reset model on each iteration\n",
    "\n",
    "    all_keys_binarized = mlb.fit_transform(all_keys.reshape([all_keys.shape[0],1]))\n",
    "    testing_keys_binarized = mlb.transform(testing_keys)\n",
    "    #model, model_features = make_model()\n",
    "    model = model_class(model_features)\n",
    "    optimizer = tf.train.AdamOptimizer(model_features.learining_rate)\n",
    "    \n",
    "    _, f1_error = model.fit_batch(\n",
    "        (spectra_dataset, all_keys_binarized),\n",
    "        (testing_spectra, testing_keys_binarized),\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=1500,\n",
    "        verbose=10,\n",
    "        obj_cost=model.cross_entropy,\n",
    "        earlystop_cost_fn=model.f1_error,\n",
    "        earlystop_patience=1000,\n",
    "        data_augmentation=online_data_augmentation,\n",
    "        augment_testing_data=False,\n",
    "        print_errors=True,\n",
    "        record_train_errors=False)\n",
    "\n",
    "    training_errors.append(f1_error['test'])\n",
    "    np.save('./final-models/final_test_errors_'+model_id, training_errors)\n",
    "    model.save_weights('./final-models/'+model_id+'_checkpoint_'+str(network_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

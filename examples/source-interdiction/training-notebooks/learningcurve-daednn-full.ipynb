{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_device_id = str(5)\n",
    "model_id_save_as = 'learningcurve-daednn-full-final'\n",
    "architecture_id = 'final-models/learningcurve-dnn-full-final-features'\n",
    "model_class_id = 'DNN'\n",
    "testing_dataset_id = '../../source-interdiction/dataset_generation/validation_dataset_full_200keV_log10time_100.npy'\n",
    "training_dataset_id = '../../source-interdiction/dataset_generation/training_dataset_full_200keV_log10time_10000.npy'\n",
    "difficulty_setting = 'full'\n",
    "\n",
    "train_sizes = [50, 100, 500, 1000, 5000, 10000, 15000, 20000,]\n",
    "earlystop_patience = 10\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_device_id\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(5)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model, training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from annsa.model_classes import build_dnn_model, compile_model, f1\n",
    "from annsa.load_dataset import load_easy, load_full, dataset_to_spectrakeys\n",
    "from annsa.load_pretrained_network import load_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.load(training_dataset_id)\n",
    "training_spectra, training_keys = dataset_to_spectrakeys(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = np.load(testing_dataset_id)\n",
    "testing_spectra, testing_keys = dataset_to_spectrakeys(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = load_features(architecture_id)\n",
    "model_features.input_dim = 1024\n",
    "model_features.dropout_rate = model_features.dropout_probability\n",
    "model_features.loss = tf.keras.losses.categorical_crossentropy\n",
    "model_features.optimizer = tf.keras.optimizers.Adam\n",
    "model_features.output_function = tf.nn.softmax\n",
    "model_features.metrics = [f1]\n",
    "model_features.learning_rate = model_features.learining_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_model = load_model('./final-models-keras/daepretrain-full-final.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_spectra_scaled = model_features.scaler.transform(training_spectra)\n",
    "testing_spectra_scaled = model_features.scaler.transform(testing_spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_f1',\n",
    "    patience=earlystop_patience,\n",
    "    mode='max',\n",
    "    min_delta=0.01,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running through training size 50\n",
      "Running through fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36_update/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 13ms/sample - loss: 53.4494 - f1: 0.0161 - val_loss: 51.2853 - val_f1: 0.0173\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 51.5896 - f1: 0.0588 - val_loss: 50.8079 - val_f1: 0.0440\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 51.2827 - f1: 0.0345 - val_loss: 50.4561 - val_f1: 0.0437\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.7056 - f1: 0.0172 - val_loss: 50.0784 - val_f1: 0.0344\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.0853 - f1: 0.0303 - val_loss: 50.1401 - val_f1: 0.0322\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.0501 - f1: 0.0631 - val_loss: 50.0303 - val_f1: 0.0395\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.3584 - f1: 0.0752 - val_loss: 49.7952 - val_f1: 0.0517\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.2375 - f1: 0.0922 - val_loss: 49.3698 - val_f1: 0.0527\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 48.8181 - f1: 0.0625 - val_loss: 48.8747 - val_f1: 0.0531\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.9709 - f1: 0.1146 - val_loss: 48.2506 - val_f1: 0.0543\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.9532 - f1: 0.0839 - val_loss: 48.0747 - val_f1: 0.0593\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.6543 - f1: 0.0690 - val_loss: 47.9630 - val_f1: 0.0590\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.7587 - f1: 0.1696 - val_loss: 47.6639 - val_f1: 0.0620\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.9545 - f1: 0.0648 - val_loss: 47.2020 - val_f1: 0.0765\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.1904 - f1: 0.1760 - val_loss: 47.4287 - val_f1: 0.0651\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.2752 - f1: 0.1107 - val_loss: 47.2954 - val_f1: 0.0668\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 45.9460 - f1: 0.1323 - val_loss: 46.9362 - val_f1: 0.0787\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.8536 - f1: 0.1902 - val_loss: 46.5763 - val_f1: 0.0931\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.5682 - f1: 0.2894 - val_loss: 46.5591 - val_f1: 0.0934\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.5507 - f1: 0.1813 - val_loss: 46.0908 - val_f1: 0.0996\n",
      "Epoch 21/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.5143 - f1: 0.2478 - val_loss: 45.7248 - val_f1: 0.0927\n",
      "Epoch 22/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.8748 - f1: 0.2437 - val_loss: 45.4900 - val_f1: 0.0802\n",
      "Epoch 23/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.4194 - f1: 0.2829 - val_loss: 45.2948 - val_f1: 0.0784\n",
      "Epoch 24/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.8065 - f1: 0.2116 - val_loss: 45.3050 - val_f1: 0.0611\n",
      "Epoch 25/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.7368 - f1: 0.1800 - val_loss: 45.4011 - val_f1: 0.0623\n",
      "Epoch 26/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.7316 - f1: 0.2010 - val_loss: 45.3540 - val_f1: 0.0692\n",
      "Epoch 27/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.7199 - f1: 0.2159 - val_loss: 45.2884 - val_f1: 0.0770\n",
      "Epoch 28/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.4821 - f1: 0.1499 - val_loss: 45.0139 - val_f1: 0.0782\n",
      "Running through fold 1\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 11ms/sample - loss: 52.3379 - f1: 0.0455 - val_loss: 50.5486 - val_f1: 0.0518\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.9232 - f1: 0.0670 - val_loss: 50.5126 - val_f1: 0.0444\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 51.0387 - f1: 0.0167 - val_loss: 50.3866 - val_f1: 0.0433\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.2022 - f1: 0.0734 - val_loss: 50.5386 - val_f1: 0.0506\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.1415 - f1: 0.0492 - val_loss: 50.3422 - val_f1: 0.0535\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.4432 - f1: 0.0792 - val_loss: 49.4109 - val_f1: 0.0721\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.3808 - f1: 0.0820 - val_loss: 49.3455 - val_f1: 0.0604\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.2641 - f1: 0.0517 - val_loss: 48.7720 - val_f1: 0.0616\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 48.5980 - f1: 0.0629 - val_loss: 48.4857 - val_f1: 0.0593\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 48.5142 - f1: 0.0642 - val_loss: 48.1269 - val_f1: 0.0765\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.4242 - f1: 0.0485 - val_loss: 47.7860 - val_f1: 0.0881\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.1761 - f1: 0.0869 - val_loss: 47.7091 - val_f1: 0.0989\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.1528 - f1: 0.0820 - val_loss: 47.6554 - val_f1: 0.0906\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.6903 - f1: 0.1721 - val_loss: 47.4551 - val_f1: 0.0839\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.2475 - f1: 0.1758 - val_loss: 47.0625 - val_f1: 0.0875\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.6469 - f1: 0.1212 - val_loss: 47.5701 - val_f1: 0.0703\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.7196 - f1: 0.1483 - val_loss: 47.4624 - val_f1: 0.0692\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.5261 - f1: 0.1572 - val_loss: 46.7400 - val_f1: 0.0785\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 45.7050 - f1: 0.1734 - val_loss: 46.0442 - val_f1: 0.0905\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 45.5321 - f1: 0.2216 - val_loss: 45.8636 - val_f1: 0.0911\n",
      "Epoch 21/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.7655 - f1: 0.2150 - val_loss: 45.6279 - val_f1: 0.0991\n",
      "Epoch 22/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.6250 - f1: 0.2532 - val_loss: 45.4610 - val_f1: 0.1044\n",
      "Running through fold 2\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 11ms/sample - loss: 52.4117 - f1: 0.0000e+00 - val_loss: 51.3462 - val_f1: 0.0472\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 51.4381 - f1: 0.0000e+00 - val_loss: 51.3860 - val_f1: 0.0498\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 51.0094 - f1: 0.0455 - val_loss: 51.0931 - val_f1: 0.0441\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.7166 - f1: 0.0612 - val_loss: 50.2440 - val_f1: 0.0497\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 48.6995 - f1: 0.0678 - val_loss: 49.5111 - val_f1: 0.0437\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 5ms/sample - loss: 48.5442 - f1: 0.0842 - val_loss: 49.1037 - val_f1: 0.0370\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 47.9276 - f1: 0.0520 - val_loss: 48.8104 - val_f1: 0.0339\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.5919 - f1: 0.0345 - val_loss: 48.5313 - val_f1: 0.0308\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.1675 - f1: 0.0804 - val_loss: 47.8828 - val_f1: 0.0272\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.9596 - f1: 0.0333 - val_loss: 47.1568 - val_f1: 0.0459\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.3894 - f1: 0.0392 - val_loss: 47.1183 - val_f1: 0.0459\n",
      "Running through fold 3\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 12ms/sample - loss: 53.3345 - f1: 0.0000e+00 - val_loss: 51.2121 - val_f1: 0.0349\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 51.9491 - f1: 0.0937 - val_loss: 50.1895 - val_f1: 0.0414\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.8836 - f1: 0.0303 - val_loss: 49.7401 - val_f1: 0.0355\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.3190 - f1: 0.0475 - val_loss: 49.3412 - val_f1: 0.0509\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.0864 - f1: 0.0508 - val_loss: 49.0555 - val_f1: 0.0552\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.7163 - f1: 0.0812 - val_loss: 48.8808 - val_f1: 0.0583\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.6903 - f1: 0.0794 - val_loss: 48.4372 - val_f1: 0.0607\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.3351 - f1: 0.0622 - val_loss: 48.2639 - val_f1: 0.0635\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 48.1550 - f1: 0.0933 - val_loss: 48.3944 - val_f1: 0.0578\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 48.1364 - f1: 0.0725 - val_loss: 48.2579 - val_f1: 0.0546\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.3777 - f1: 0.0351 - val_loss: 47.6456 - val_f1: 0.0585\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.5709 - f1: 0.0588 - val_loss: 47.0979 - val_f1: 0.0652\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.7988 - f1: 0.0945 - val_loss: 46.7826 - val_f1: 0.0640\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.4798 - f1: 0.1599 - val_loss: 46.9528 - val_f1: 0.0745\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.3583 - f1: 0.1774 - val_loss: 46.8717 - val_f1: 0.0801\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.6429 - f1: 0.1114 - val_loss: 46.7089 - val_f1: 0.0804\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.1515 - f1: 0.2005 - val_loss: 46.3293 - val_f1: 0.0834\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 45.8494 - f1: 0.1623 - val_loss: 45.8444 - val_f1: 0.0857\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 45.9255 - f1: 0.1255 - val_loss: 45.5618 - val_f1: 0.0877\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 45.4957 - f1: 0.1899 - val_loss: 45.4462 - val_f1: 0.0886\n",
      "Epoch 21/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 45.0868 - f1: 0.2143 - val_loss: 45.3077 - val_f1: 0.0772\n",
      "Epoch 22/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.8234 - f1: 0.1990 - val_loss: 45.3180 - val_f1: 0.0784\n",
      "Epoch 23/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.6132 - f1: 0.2211 - val_loss: 45.3761 - val_f1: 0.0788\n",
      "Epoch 24/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.0292 - f1: 0.2640 - val_loss: 45.3972 - val_f1: 0.0784\n",
      "Epoch 25/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.1732 - f1: 0.2262 - val_loss: 45.1950 - val_f1: 0.0788\n",
      "Epoch 26/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 44.1986 - f1: 0.2057 - val_loss: 44.8445 - val_f1: 0.0952\n",
      "Epoch 27/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.5139 - f1: 0.2483 - val_loss: 44.4144 - val_f1: 0.1187\n",
      "Epoch 28/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.3580 - f1: 0.2329 - val_loss: 44.4181 - val_f1: 0.1103\n",
      "Epoch 29/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.4864 - f1: 0.2030 - val_loss: 44.5248 - val_f1: 0.1073\n",
      "Epoch 30/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.0347 - f1: 0.2030 - val_loss: 44.2246 - val_f1: 0.1078\n",
      "Epoch 31/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.8849 - f1: 0.2663 - val_loss: 43.8138 - val_f1: 0.1170\n",
      "Epoch 32/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.0360 - f1: 0.2124 - val_loss: 43.3094 - val_f1: 0.1316\n",
      "Epoch 33/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.0997 - f1: 0.2894 - val_loss: 42.8399 - val_f1: 0.1347\n",
      "Epoch 34/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.1504 - f1: 0.2628 - val_loss: 42.6993 - val_f1: 0.1138\n",
      "Epoch 35/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.0188 - f1: 0.2262 - val_loss: 42.6769 - val_f1: 0.1025\n",
      "Epoch 36/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.1986 - f1: 0.2242 - val_loss: 42.1028 - val_f1: 0.1118\n",
      "Epoch 37/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 41.8952 - f1: 0.2618 - val_loss: 41.8561 - val_f1: 0.1329\n",
      "Epoch 38/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.8758 - f1: 0.3613 - val_loss: 41.9088 - val_f1: 0.1220\n",
      "Epoch 39/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.8547 - f1: 0.3128 - val_loss: 42.1611 - val_f1: 0.0993\n",
      "Epoch 40/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 41.0580 - f1: 0.2597 - val_loss: 42.2516 - val_f1: 0.0901\n",
      "Epoch 41/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.4094 - f1: 0.3240 - val_loss: 42.3713 - val_f1: 0.0886\n",
      "Epoch 42/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 41.0491 - f1: 0.2406 - val_loss: 42.1010 - val_f1: 0.0939\n",
      "Running through fold 4\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 13ms/sample - loss: 51.5604 - f1: 0.0303 - val_loss: 50.2466 - val_f1: 0.0747\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.9688 - f1: 0.0000e+00 - val_loss: 49.7005 - val_f1: 0.0502\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 50.1758 - f1: 0.0294 - val_loss: 49.5291 - val_f1: 0.0417\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.5186 - f1: 0.0169 - val_loss: 49.2790 - val_f1: 0.0343\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.3051 - f1: 0.0303 - val_loss: 49.4740 - val_f1: 0.0427\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 48.6423 - f1: 0.0625 - val_loss: 49.6199 - val_f1: 0.0477\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 49.0562 - f1: 0.0458 - val_loss: 49.1698 - val_f1: 0.0493\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.7556 - f1: 0.0830 - val_loss: 48.8069 - val_f1: 0.0337\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 48.1511 - f1: 0.0606 - val_loss: 48.2543 - val_f1: 0.0441\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 47.1520 - f1: 0.1571 - val_loss: 47.9247 - val_f1: 0.0571\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 46.2787 - f1: 0.0807 - val_loss: 47.8194 - val_f1: 0.0615\n",
      "\n",
      "\n",
      "Running through training size 100\n",
      "Running through fold 0\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 8ms/sample - loss: 51.5043 - f1: 0.0334 - val_loss: 50.7844 - val_f1: 0.0245\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 50.7535 - f1: 0.0172 - val_loss: 50.3061 - val_f1: 0.0429\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 49.9305 - f1: 0.0259 - val_loss: 50.5400 - val_f1: 0.0382\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 50.0415 - f1: 0.0253 - val_loss: 49.9555 - val_f1: 0.0438\n",
      "Epoch 5/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/sample - loss: 49.7545 - f1: 0.0082 - val_loss: 49.1219 - val_f1: 0.0434\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 48.1792 - f1: 0.0977 - val_loss: 47.8521 - val_f1: 0.0492\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.6602 - f1: 0.0544 - val_loss: 47.6522 - val_f1: 0.0592\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.0933 - f1: 0.0358 - val_loss: 47.7683 - val_f1: 0.0594\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.1664 - f1: 0.0535 - val_loss: 47.2606 - val_f1: 0.0606\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.7578 - f1: 0.0875 - val_loss: 47.1412 - val_f1: 0.0495\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.5983 - f1: 0.0598 - val_loss: 46.2849 - val_f1: 0.0435\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.8868 - f1: 0.0364 - val_loss: 45.6724 - val_f1: 0.0557\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.0989 - f1: 0.1084 - val_loss: 45.2988 - val_f1: 0.0677\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.9228 - f1: 0.0980 - val_loss: 45.5421 - val_f1: 0.0747\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.6233 - f1: 0.1037 - val_loss: 45.8319 - val_f1: 0.0658\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.1162 - f1: 0.1570 - val_loss: 45.3366 - val_f1: 0.0659\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 43.9446 - f1: 0.1050 - val_loss: 44.3072 - val_f1: 0.0907\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 43.1009 - f1: 0.1048 - val_loss: 43.8592 - val_f1: 0.0849\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 43.0120 - f1: 0.1535 - val_loss: 43.5242 - val_f1: 0.0764\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.4901 - f1: 0.2297 - val_loss: 43.0121 - val_f1: 0.0908\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.3167 - f1: 0.0777 - val_loss: 42.6088 - val_f1: 0.0968\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 41.5010 - f1: 0.2649 - val_loss: 42.4139 - val_f1: 0.0910\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 41.1452 - f1: 0.1805 - val_loss: 42.1757 - val_f1: 0.0967\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.8400 - f1: 0.2679 - val_loss: 42.0995 - val_f1: 0.0933\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.8905 - f1: 0.0989 - val_loss: 41.4954 - val_f1: 0.0896\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.7683 - f1: 0.1316 - val_loss: 41.7724 - val_f1: 0.0929\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.9728 - f1: 0.2150 - val_loss: 41.2003 - val_f1: 0.1020\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.1159 - f1: 0.1403 - val_loss: 40.8105 - val_f1: 0.1068\n",
      "Epoch 29/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.2672 - f1: 0.1841 - val_loss: 40.4210 - val_f1: 0.1123\n",
      "Epoch 30/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.3076 - f1: 0.1639 - val_loss: 40.4061 - val_f1: 0.1122\n",
      "Epoch 31/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.9922 - f1: 0.1667 - val_loss: 40.0901 - val_f1: 0.1093\n",
      "Epoch 32/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.6562 - f1: 0.2678 - val_loss: 39.6840 - val_f1: 0.1216\n",
      "Epoch 33/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.5724 - f1: 0.1859 - val_loss: 39.8032 - val_f1: 0.1248\n",
      "Epoch 34/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.1416 - f1: 0.1839 - val_loss: 40.6423 - val_f1: 0.0838\n",
      "Epoch 35/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.2644 - f1: 0.2767 - val_loss: 40.3432 - val_f1: 0.0931\n",
      "Epoch 36/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.2846 - f1: 0.2933 - val_loss: 40.1511 - val_f1: 0.0864\n",
      "Epoch 37/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.4135 - f1: 0.2370 - val_loss: 39.2800 - val_f1: 0.1132\n",
      "Epoch 38/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.9122 - f1: 0.1823 - val_loss: 40.4852 - val_f1: 0.0682\n",
      "Epoch 39/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.0757 - f1: 0.1730 - val_loss: 40.0279 - val_f1: 0.0764\n",
      "Epoch 40/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.4071 - f1: 0.1945 - val_loss: 38.9486 - val_f1: 0.1028\n",
      "Epoch 41/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.6279 - f1: 0.2536 - val_loss: 38.6791 - val_f1: 0.1088\n",
      "Epoch 42/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.5326 - f1: 0.2340 - val_loss: 38.7939 - val_f1: 0.0915\n",
      "Epoch 43/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.1849 - f1: 0.1721 - val_loss: 38.6857 - val_f1: 0.0801\n",
      "Running through fold 1\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 51.2986 - f1: 0.0179 - val_loss: 50.3064 - val_f1: 0.0361\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 50.0322 - f1: 0.0337 - val_loss: 49.4320 - val_f1: 0.0472\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 49.2136 - f1: 0.0427 - val_loss: 49.2070 - val_f1: 0.0632\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 48.7036 - f1: 0.0350 - val_loss: 48.4531 - val_f1: 0.0527\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.8058 - f1: 0.0433 - val_loss: 47.1790 - val_f1: 0.0334\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.7631 - f1: 0.0613 - val_loss: 46.5212 - val_f1: 0.0295\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.1786 - f1: 0.0448 - val_loss: 46.4077 - val_f1: 0.0303\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.3577 - f1: 0.0726 - val_loss: 47.5653 - val_f1: 0.0389\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.8512 - f1: 0.0970 - val_loss: 47.2741 - val_f1: 0.0603\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.5964 - f1: 0.1210 - val_loss: 46.7518 - val_f1: 0.0606\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.7212 - f1: 0.1573 - val_loss: 45.7655 - val_f1: 0.0715\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.2849 - f1: 0.1588 - val_loss: 45.6417 - val_f1: 0.0590\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.7437 - f1: 0.0688 - val_loss: 44.7132 - val_f1: 0.0468\n",
      "Running through fold 2\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 51.5426 - f1: 0.0890 - val_loss: 50.3743 - val_f1: 0.0533\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 49.9830 - f1: 0.0260 - val_loss: 49.6601 - val_f1: 0.0270\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 49.0445 - f1: 0.0267 - val_loss: 48.8270 - val_f1: 0.0260\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 48.4513 - f1: 0.0174 - val_loss: 47.3595 - val_f1: 0.0475\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.8705 - f1: 0.0552 - val_loss: 46.9065 - val_f1: 0.0735\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.0965 - f1: 0.0547 - val_loss: 46.9671 - val_f1: 0.0639\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.8193 - f1: 0.0438 - val_loss: 46.2499 - val_f1: 0.0747\n",
      "Epoch 8/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.8819 - f1: 0.0738 - val_loss: 45.6040 - val_f1: 0.0837\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.5950 - f1: 0.0548 - val_loss: 45.4595 - val_f1: 0.0959\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.3204 - f1: 0.1574 - val_loss: 46.9731 - val_f1: 0.0590\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.1404 - f1: 0.1148 - val_loss: 45.0759 - val_f1: 0.0747\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 43.8679 - f1: 0.1033 - val_loss: 43.4927 - val_f1: 0.1493\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.7347 - f1: 0.1695 - val_loss: 43.4781 - val_f1: 0.1477\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.7977 - f1: 0.1187 - val_loss: 43.0658 - val_f1: 0.1242\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.4863 - f1: 0.1653 - val_loss: 43.4783 - val_f1: 0.1001\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.5182 - f1: 0.1149 - val_loss: 43.0446 - val_f1: 0.1146\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.1279 - f1: 0.2419 - val_loss: 42.6630 - val_f1: 0.1256\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 41.5624 - f1: 0.1536 - val_loss: 41.8041 - val_f1: 0.1393\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 41.4032 - f1: 0.2237 - val_loss: 41.6189 - val_f1: 0.1499\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.3277 - f1: 0.2657 - val_loss: 41.0701 - val_f1: 0.1675\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.9907 - f1: 0.2166 - val_loss: 40.5077 - val_f1: 0.1698\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.8225 - f1: 0.1754 - val_loss: 40.9526 - val_f1: 0.1290\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.0545 - f1: 0.1808 - val_loss: 40.4990 - val_f1: 0.1460\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.3538 - f1: 0.2092 - val_loss: 40.5823 - val_f1: 0.1209\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.6054 - f1: 0.2168 - val_loss: 39.6207 - val_f1: 0.1418\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.9272 - f1: 0.1886 - val_loss: 38.8826 - val_f1: 0.1682\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.5375 - f1: 0.1470 - val_loss: 38.7805 - val_f1: 0.1584\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.5804 - f1: 0.3601 - val_loss: 38.8469 - val_f1: 0.1503\n",
      "Epoch 29/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.1861 - f1: 0.3689 - val_loss: 38.4906 - val_f1: 0.1627\n",
      "Epoch 30/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.7749 - f1: 0.3133 - val_loss: 37.9265 - val_f1: 0.1708\n",
      "Running through fold 3\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 50.7365 - f1: 0.0344 - val_loss: 49.7315 - val_f1: 0.0430\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 50.1513 - f1: 0.0176 - val_loss: 49.8229 - val_f1: 0.0505\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 48.9146 - f1: 0.0603 - val_loss: 49.2305 - val_f1: 0.0490\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 49.4463 - f1: 0.0431 - val_loss: 48.8215 - val_f1: 0.0620\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 48.6971 - f1: 0.0436 - val_loss: 47.7715 - val_f1: 0.0600\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.2276 - f1: 0.0608 - val_loss: 47.8036 - val_f1: 0.0385\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.9160 - f1: 0.1081 - val_loss: 47.5652 - val_f1: 0.0444\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.0169 - f1: 0.1060 - val_loss: 46.5313 - val_f1: 0.0499\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.7308 - f1: 0.0628 - val_loss: 45.4692 - val_f1: 0.0602\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.6561 - f1: 0.0635 - val_loss: 45.1746 - val_f1: 0.0951\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.6938 - f1: 0.1749 - val_loss: 45.9975 - val_f1: 0.0708\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.5387 - f1: 0.0514 - val_loss: 45.2157 - val_f1: 0.0808\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.1723 - f1: 0.0647 - val_loss: 44.1917 - val_f1: 0.0822\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.7235 - f1: 0.1319 - val_loss: 43.2018 - val_f1: 0.0937\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.4269 - f1: 0.1047 - val_loss: 43.1355 - val_f1: 0.0968\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.6913 - f1: 0.1085 - val_loss: 43.2157 - val_f1: 0.0874\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.2867 - f1: 0.1879 - val_loss: 42.5564 - val_f1: 0.0995\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 41.6314 - f1: 0.1997 - val_loss: 42.0176 - val_f1: 0.1002\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.8812 - f1: 0.1820 - val_loss: 41.3436 - val_f1: 0.1204\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.6842 - f1: 0.1373 - val_loss: 42.5774 - val_f1: 0.0960\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.7773 - f1: 0.1860 - val_loss: 40.7286 - val_f1: 0.1362\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.5893 - f1: 0.3104 - val_loss: 40.3249 - val_f1: 0.1412\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.0257 - f1: 0.1545 - val_loss: 39.7630 - val_f1: 0.1521\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.5301 - f1: 0.1875 - val_loss: 40.2533 - val_f1: 0.1413\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.6246 - f1: 0.2495 - val_loss: 40.2857 - val_f1: 0.1374\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.1304 - f1: 0.1808 - val_loss: 40.0127 - val_f1: 0.1458\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.1549 - f1: 0.1684 - val_loss: 39.7574 - val_f1: 0.1416\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.4707 - f1: 0.1724 - val_loss: 39.4116 - val_f1: 0.1489\n",
      "Epoch 29/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.1367 - f1: 0.3330 - val_loss: 39.0257 - val_f1: 0.1495\n",
      "Epoch 30/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.3232 - f1: 0.3062 - val_loss: 39.2639 - val_f1: 0.1365\n",
      "Epoch 31/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.8438 - f1: 0.3752 - val_loss: 38.2976 - val_f1: 0.1776\n",
      "Epoch 32/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.8829 - f1: 0.2856 - val_loss: 38.6869 - val_f1: 0.1505\n",
      "Epoch 33/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.3419 - f1: 0.2050 - val_loss: 39.1573 - val_f1: 0.1341\n",
      "Epoch 34/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.3700 - f1: 0.3889 - val_loss: 38.6017 - val_f1: 0.1342\n",
      "Epoch 35/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.8587 - f1: 0.2617 - val_loss: 37.8926 - val_f1: 0.1552\n",
      "Epoch 36/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.0061 - f1: 0.2948 - val_loss: 38.2590 - val_f1: 0.1350\n",
      "Epoch 37/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.4703 - f1: 0.2488 - val_loss: 37.2425 - val_f1: 0.1847\n",
      "Epoch 38/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 35.4116 - f1: 0.3166 - val_loss: 37.3886 - val_f1: 0.1785\n",
      "Epoch 39/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 35.1260 - f1: 0.4464 - val_loss: 37.3795 - val_f1: 0.1641\n",
      "Epoch 40/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 35.4103 - f1: 0.2968 - val_loss: 36.8802 - val_f1: 0.1765\n",
      "Epoch 41/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 34.8779 - f1: 0.2482 - val_loss: 36.4324 - val_f1: 0.1710\n",
      "Running through fold 4\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 7ms/sample - loss: 50.5335 - f1: 0.1041 - val_loss: 50.4912 - val_f1: 0.0460\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 50.6271 - f1: 0.0348 - val_loss: 50.7118 - val_f1: 0.0379\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 49.6958 - f1: 0.0672 - val_loss: 48.8085 - val_f1: 0.0600\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 48.9160 - f1: 0.0430 - val_loss: 49.1933 - val_f1: 0.0532\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 48.5846 - f1: 0.1055 - val_loss: 48.4175 - val_f1: 0.0623\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.9649 - f1: 0.0424 - val_loss: 47.7217 - val_f1: 0.0556\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 47.5123 - f1: 0.0343 - val_loss: 46.3793 - val_f1: 0.0764\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 46.6426 - f1: 0.0541 - val_loss: 45.7754 - val_f1: 0.0788\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.6391 - f1: 0.1366 - val_loss: 46.6533 - val_f1: 0.0656\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.7963 - f1: 0.1394 - val_loss: 46.0097 - val_f1: 0.0761\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 45.1071 - f1: 0.0873 - val_loss: 44.9423 - val_f1: 0.1035\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.2144 - f1: 0.1488 - val_loss: 44.4612 - val_f1: 0.0990\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 44.1370 - f1: 0.0893 - val_loss: 44.2136 - val_f1: 0.1163\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 43.6970 - f1: 0.1308 - val_loss: 43.9244 - val_f1: 0.1078\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 43.4230 - f1: 0.1423 - val_loss: 43.3813 - val_f1: 0.1057\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.5279 - f1: 0.1778 - val_loss: 43.1878 - val_f1: 0.1092\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.2391 - f1: 0.2534 - val_loss: 42.8875 - val_f1: 0.1086\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 42.0890 - f1: 0.1173 - val_loss: 42.6160 - val_f1: 0.1143\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 41.4190 - f1: 0.1453 - val_loss: 42.0870 - val_f1: 0.1115\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 41.4183 - f1: 0.0892 - val_loss: 41.5267 - val_f1: 0.1152\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.5800 - f1: 0.1930 - val_loss: 41.3521 - val_f1: 0.1019\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.2954 - f1: 0.2562 - val_loss: 41.0735 - val_f1: 0.1083\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 39.9186 - f1: 0.1681 - val_loss: 40.6617 - val_f1: 0.1076\n",
      "\n",
      "\n",
      "Running through training size 500\n",
      "Running through fold 0\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 1ms/sample - loss: 49.9187 - f1: 0.0345 - val_loss: 47.7835 - val_f1: 0.0847\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 46.9485 - f1: 0.0556 - val_loss: 45.0521 - val_f1: 0.0719\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 44.8007 - f1: 0.0898 - val_loss: 42.8170 - val_f1: 0.0982\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 42.9632 - f1: 0.0861 - val_loss: 41.3975 - val_f1: 0.1334\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 41.3578 - f1: 0.0897 - val_loss: 39.8203 - val_f1: 0.1390\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 39.8688 - f1: 0.1391 - val_loss: 37.5502 - val_f1: 0.1780\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 38.1891 - f1: 0.1723 - val_loss: 37.0602 - val_f1: 0.1996\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 37.0980 - f1: 0.1730 - val_loss: 35.5448 - val_f1: 0.2276\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 35.4538 - f1: 0.2435 - val_loss: 34.1312 - val_f1: 0.2461\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 453us/sample - loss: 34.0724 - f1: 0.2503 - val_loss: 33.1567 - val_f1: 0.2592\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 33.3639 - f1: 0.2262 - val_loss: 31.8304 - val_f1: 0.2705\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 32.3876 - f1: 0.2515 - val_loss: 31.1536 - val_f1: 0.2716\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 31.2142 - f1: 0.2801 - val_loss: 30.7873 - val_f1: 0.2780\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 30.6762 - f1: 0.3016 - val_loss: 29.6991 - val_f1: 0.3105\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 553us/sample - loss: 29.5811 - f1: 0.3315 - val_loss: 28.8911 - val_f1: 0.2785\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 514us/sample - loss: 28.6955 - f1: 0.3383 - val_loss: 28.3418 - val_f1: 0.3161\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 27.6365 - f1: 0.3662 - val_loss: 27.1752 - val_f1: 0.3323\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 451us/sample - loss: 27.0440 - f1: 0.3515 - val_loss: 27.0017 - val_f1: 0.2911\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 443us/sample - loss: 26.7410 - f1: 0.3515 - val_loss: 26.6278 - val_f1: 0.3338\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 446us/sample - loss: 25.9121 - f1: 0.3623 - val_loss: 26.1491 - val_f1: 0.3320\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 447us/sample - loss: 25.5490 - f1: 0.3543 - val_loss: 25.5738 - val_f1: 0.3432\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 25.2831 - f1: 0.3898 - val_loss: 24.7776 - val_f1: 0.3509\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 538us/sample - loss: 23.9524 - f1: 0.4292 - val_loss: 23.9163 - val_f1: 0.3629\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 23.2790 - f1: 0.4194 - val_loss: 23.3728 - val_f1: 0.3702\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 444us/sample - loss: 23.0386 - f1: 0.4373 - val_loss: 23.1984 - val_f1: 0.3698\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 448us/sample - loss: 22.4406 - f1: 0.4651 - val_loss: 22.3722 - val_f1: 0.4131\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 448us/sample - loss: 21.6783 - f1: 0.4942 - val_loss: 22.0457 - val_f1: 0.3762\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 450us/sample - loss: 21.0611 - f1: 0.5156 - val_loss: 21.5564 - val_f1: 0.4070\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 445us/sample - loss: 20.8291 - f1: 0.4836 - val_loss: 21.4780 - val_f1: 0.3780\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 442us/sample - loss: 20.7173 - f1: 0.4739 - val_loss: 20.9484 - val_f1: 0.4203\n",
      "Epoch 31/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 450us/sample - loss: 20.1595 - f1: 0.5035 - val_loss: 20.5527 - val_f1: 0.3957\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 444us/sample - loss: 19.5167 - f1: 0.5514 - val_loss: 20.2395 - val_f1: 0.4012\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 447us/sample - loss: 19.2978 - f1: 0.5060 - val_loss: 19.6715 - val_f1: 0.4311\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 18.8605 - f1: 0.5491 - val_loss: 19.6252 - val_f1: 0.4204\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 451us/sample - loss: 18.6371 - f1: 0.5434 - val_loss: 19.3615 - val_f1: 0.4119\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 446us/sample - loss: 18.3148 - f1: 0.5353 - val_loss: 18.9296 - val_f1: 0.4005\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 445us/sample - loss: 17.9652 - f1: 0.5457 - val_loss: 18.5924 - val_f1: 0.3976\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 453us/sample - loss: 17.7753 - f1: 0.5470 - val_loss: 18.2061 - val_f1: 0.4267\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 17.3467 - f1: 0.5578 - val_loss: 17.9578 - val_f1: 0.4423\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 17.1065 - f1: 0.6020 - val_loss: 17.8222 - val_f1: 0.4385\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 17.0217 - f1: 0.5899 - val_loss: 17.6717 - val_f1: 0.4377\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 16.6041 - f1: 0.5882 - val_loss: 17.3576 - val_f1: 0.4488\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 16.4543 - f1: 0.5599 - val_loss: 17.4498 - val_f1: 0.4364\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 16.2996 - f1: 0.6000 - val_loss: 17.0621 - val_f1: 0.4209\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 15.9942 - f1: 0.5817 - val_loss: 16.6585 - val_f1: 0.4593\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 15.6920 - f1: 0.5978 - val_loss: 16.3774 - val_f1: 0.4672\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 15.5508 - f1: 0.6020 - val_loss: 16.3110 - val_f1: 0.4347\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 15.2603 - f1: 0.5955 - val_loss: 16.0035 - val_f1: 0.4781\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 15.1985 - f1: 0.5951 - val_loss: 16.2721 - val_f1: 0.4052\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 458us/sample - loss: 15.0501 - f1: 0.5670 - val_loss: 15.6511 - val_f1: 0.4638\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 14.6423 - f1: 0.6506 - val_loss: 15.4701 - val_f1: 0.4574\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 446us/sample - loss: 14.4159 - f1: 0.6358 - val_loss: 15.1259 - val_f1: 0.4846\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 14.2234 - f1: 0.6406 - val_loss: 14.9775 - val_f1: 0.4788\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 14.1112 - f1: 0.6314 - val_loss: 14.8655 - val_f1: 0.4743\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 450us/sample - loss: 13.9812 - f1: 0.6152 - val_loss: 14.8965 - val_f1: 0.4684\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 13.9283 - f1: 0.6251 - val_loss: 14.6657 - val_f1: 0.4801\n",
      "Epoch 57/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 13.5107 - f1: 0.6862 - val_loss: 14.4183 - val_f1: 0.4829\n",
      "Epoch 58/2000\n",
      "500/500 [==============================] - 0s 523us/sample - loss: 13.5198 - f1: 0.6486 - val_loss: 14.3973 - val_f1: 0.4625\n",
      "Running through fold 1\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 50.3093 - f1: 0.0297 - val_loss: 47.0768 - val_f1: 0.0623\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 549us/sample - loss: 46.6921 - f1: 0.0779 - val_loss: 44.6866 - val_f1: 0.0846\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 516us/sample - loss: 44.4865 - f1: 0.1028 - val_loss: 42.8238 - val_f1: 0.1084\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 450us/sample - loss: 42.6720 - f1: 0.1078 - val_loss: 40.8080 - val_f1: 0.1709\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 448us/sample - loss: 40.8663 - f1: 0.1459 - val_loss: 38.9448 - val_f1: 0.1944\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 453us/sample - loss: 39.3486 - f1: 0.1585 - val_loss: 37.5718 - val_f1: 0.2001\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 37.8981 - f1: 0.1670 - val_loss: 36.9405 - val_f1: 0.2175\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 450us/sample - loss: 36.2630 - f1: 0.2342 - val_loss: 35.3806 - val_f1: 0.2030\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 35.1046 - f1: 0.2477 - val_loss: 34.3324 - val_f1: 0.2273\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 33.8962 - f1: 0.2288 - val_loss: 33.1376 - val_f1: 0.2413\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 33.2790 - f1: 0.2439 - val_loss: 32.1500 - val_f1: 0.2603\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 32.1860 - f1: 0.2387 - val_loss: 31.0031 - val_f1: 0.2621\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 30.8224 - f1: 0.3038 - val_loss: 30.7136 - val_f1: 0.2762\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 30.3062 - f1: 0.2958 - val_loss: 29.5226 - val_f1: 0.2927\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 28.9609 - f1: 0.3590 - val_loss: 28.7562 - val_f1: 0.3124\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 28.0642 - f1: 0.3855 - val_loss: 28.0555 - val_f1: 0.3375\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 451us/sample - loss: 27.5672 - f1: 0.3586 - val_loss: 27.3233 - val_f1: 0.3348\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 26.8738 - f1: 0.4076 - val_loss: 26.9279 - val_f1: 0.3389\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 444us/sample - loss: 25.8368 - f1: 0.3895 - val_loss: 26.2383 - val_f1: 0.3317\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 444us/sample - loss: 25.1856 - f1: 0.4597 - val_loss: 25.7056 - val_f1: 0.3566\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 443us/sample - loss: 24.6350 - f1: 0.4213 - val_loss: 25.2507 - val_f1: 0.3464\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 442us/sample - loss: 24.2737 - f1: 0.4220 - val_loss: 24.6399 - val_f1: 0.3620\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 447us/sample - loss: 23.5753 - f1: 0.4474 - val_loss: 24.0785 - val_f1: 0.3705\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 445us/sample - loss: 22.9378 - f1: 0.5014 - val_loss: 23.4486 - val_f1: 0.3898\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 451us/sample - loss: 22.2620 - f1: 0.5127 - val_loss: 22.9452 - val_f1: 0.4001\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 21.9878 - f1: 0.4959 - val_loss: 22.6616 - val_f1: 0.3867\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 21.7006 - f1: 0.5071 - val_loss: 22.4873 - val_f1: 0.3942\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 21.0540 - f1: 0.5118 - val_loss: 21.7454 - val_f1: 0.3873\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 442us/sample - loss: 20.8772 - f1: 0.5179 - val_loss: 21.6661 - val_f1: 0.4040\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 440us/sample - loss: 20.4185 - f1: 0.5408 - val_loss: 21.2210 - val_f1: 0.3745\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 438us/sample - loss: 19.8690 - f1: 0.5669 - val_loss: 20.7397 - val_f1: 0.4054\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 446us/sample - loss: 19.4447 - f1: 0.5671 - val_loss: 20.2924 - val_f1: 0.4258\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 442us/sample - loss: 19.0725 - f1: 0.5807 - val_loss: 19.9733 - val_f1: 0.4242\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 18.8580 - f1: 0.5903 - val_loss: 19.5327 - val_f1: 0.4357\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 551us/sample - loss: 18.4148 - f1: 0.5617 - val_loss: 19.2729 - val_f1: 0.4335\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 553us/sample - loss: 18.2675 - f1: 0.5794 - val_loss: 19.0270 - val_f1: 0.4382\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 17.9406 - f1: 0.5859 - val_loss: 18.8568 - val_f1: 0.4259\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 449us/sample - loss: 17.5113 - f1: 0.6118 - val_loss: 18.4496 - val_f1: 0.4342\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 450us/sample - loss: 17.2986 - f1: 0.5987 - val_loss: 18.2181 - val_f1: 0.4345\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 447us/sample - loss: 16.9916 - f1: 0.6097 - val_loss: 18.0405 - val_f1: 0.4232\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 16.7785 - f1: 0.6005 - val_loss: 17.9263 - val_f1: 0.4227\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 453us/sample - loss: 16.6008 - f1: 0.6380 - val_loss: 17.5670 - val_f1: 0.4482\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 446us/sample - loss: 16.4243 - f1: 0.6132 - val_loss: 17.2874 - val_f1: 0.4349\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 453us/sample - loss: 16.2214 - f1: 0.5968 - val_loss: 17.0885 - val_f1: 0.4363\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 451us/sample - loss: 15.9044 - f1: 0.6319 - val_loss: 16.9407 - val_f1: 0.4342\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 15.5441 - f1: 0.6262 - val_loss: 16.4332 - val_f1: 0.4698\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 15.3191 - f1: 0.6648 - val_loss: 16.3082 - val_f1: 0.4558\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 445us/sample - loss: 15.0558 - f1: 0.6793 - val_loss: 16.1401 - val_f1: 0.4669\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 442us/sample - loss: 14.8735 - f1: 0.7004 - val_loss: 16.0155 - val_f1: 0.4696\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 444us/sample - loss: 14.5686 - f1: 0.7138 - val_loss: 15.7232 - val_f1: 0.4617\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 437us/sample - loss: 14.5012 - f1: 0.7001 - val_loss: 15.6864 - val_f1: 0.4589\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 436us/sample - loss: 14.3468 - f1: 0.6616 - val_loss: 15.4050 - val_f1: 0.4657\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 432us/sample - loss: 14.1810 - f1: 0.6831 - val_loss: 15.2326 - val_f1: 0.4710\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 434us/sample - loss: 14.1122 - f1: 0.6725 - val_loss: 15.0908 - val_f1: 0.4764\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 436us/sample - loss: 13.8456 - f1: 0.6965 - val_loss: 14.8909 - val_f1: 0.4700\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 435us/sample - loss: 13.6622 - f1: 0.7298 - val_loss: 14.7455 - val_f1: 0.4758\n",
      "Running through fold 2\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 50.7413 - f1: 0.0361 - val_loss: 48.0082 - val_f1: 0.0560\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 436us/sample - loss: 47.8443 - f1: 0.0425 - val_loss: 45.1780 - val_f1: 0.1103\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 440us/sample - loss: 45.4099 - f1: 0.0956 - val_loss: 43.1748 - val_f1: 0.0941\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 447us/sample - loss: 43.9160 - f1: 0.0827 - val_loss: 42.2576 - val_f1: 0.1479\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 437us/sample - loss: 41.9795 - f1: 0.1290 - val_loss: 39.8745 - val_f1: 0.1462\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 442us/sample - loss: 40.6093 - f1: 0.1304 - val_loss: 38.0507 - val_f1: 0.2160\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 438us/sample - loss: 39.3405 - f1: 0.1601 - val_loss: 37.8187 - val_f1: 0.2013\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 440us/sample - loss: 37.5979 - f1: 0.2407 - val_loss: 36.4471 - val_f1: 0.2397\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 443us/sample - loss: 36.5254 - f1: 0.2147 - val_loss: 34.8588 - val_f1: 0.2271\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 445us/sample - loss: 35.4070 - f1: 0.2560 - val_loss: 34.5317 - val_f1: 0.2439\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 446us/sample - loss: 34.3757 - f1: 0.2290 - val_loss: 32.7669 - val_f1: 0.2181\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 33.0143 - f1: 0.2695 - val_loss: 32.1798 - val_f1: 0.2562\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 449us/sample - loss: 32.4108 - f1: 0.2387 - val_loss: 31.4669 - val_f1: 0.2507\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 449us/sample - loss: 31.1951 - f1: 0.2772 - val_loss: 29.8723 - val_f1: 0.3082\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 453us/sample - loss: 30.1982 - f1: 0.3052 - val_loss: 29.6435 - val_f1: 0.2942\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 29.5925 - f1: 0.3391 - val_loss: 29.1523 - val_f1: 0.2879\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 28.5979 - f1: 0.3535 - val_loss: 28.5736 - val_f1: 0.3155\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 28.1273 - f1: 0.3554 - val_loss: 27.6415 - val_f1: 0.3304\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 458us/sample - loss: 27.2246 - f1: 0.3922 - val_loss: 26.8155 - val_f1: 0.3450\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 26.2868 - f1: 0.4014 - val_loss: 26.2486 - val_f1: 0.3413\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 25.8251 - f1: 0.3803 - val_loss: 25.5512 - val_f1: 0.3614\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 468us/sample - loss: 25.0611 - f1: 0.3991 - val_loss: 24.9482 - val_f1: 0.3484\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 24.8964 - f1: 0.3867 - val_loss: 24.8073 - val_f1: 0.3431\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 23.8677 - f1: 0.4347 - val_loss: 24.0252 - val_f1: 0.3627\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 23.1806 - f1: 0.4877 - val_loss: 23.1952 - val_f1: 0.4024\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 22.6382 - f1: 0.4730 - val_loss: 22.7855 - val_f1: 0.4069\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 473us/sample - loss: 22.2832 - f1: 0.4725 - val_loss: 22.7068 - val_f1: 0.3896\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 474us/sample - loss: 21.8270 - f1: 0.4862 - val_loss: 22.1281 - val_f1: 0.4049\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 469us/sample - loss: 21.3770 - f1: 0.5150 - val_loss: 21.7983 - val_f1: 0.3855\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 20.8531 - f1: 0.5226 - val_loss: 21.2194 - val_f1: 0.4216\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 20.7011 - f1: 0.5233 - val_loss: 20.9788 - val_f1: 0.4189\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 20.2537 - f1: 0.5237 - val_loss: 20.6596 - val_f1: 0.4023\n",
      "Epoch 33/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 465us/sample - loss: 19.9450 - f1: 0.5332 - val_loss: 20.4679 - val_f1: 0.4066\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 19.8310 - f1: 0.5105 - val_loss: 19.9170 - val_f1: 0.4146\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 19.0255 - f1: 0.5644 - val_loss: 19.8221 - val_f1: 0.4094\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 18.7964 - f1: 0.5477 - val_loss: 19.4141 - val_f1: 0.4249\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 18.6105 - f1: 0.5691 - val_loss: 19.2139 - val_f1: 0.4066\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 18.3270 - f1: 0.5552 - val_loss: 19.0749 - val_f1: 0.4189\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 17.9642 - f1: 0.5473 - val_loss: 18.7796 - val_f1: 0.3926\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 17.6120 - f1: 0.5616 - val_loss: 18.2406 - val_f1: 0.4369\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 17.2667 - f1: 0.5944 - val_loss: 18.1799 - val_f1: 0.4329\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 17.1503 - f1: 0.5986 - val_loss: 17.8818 - val_f1: 0.4285\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 16.7125 - f1: 0.6097 - val_loss: 17.5938 - val_f1: 0.4304\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 16.7419 - f1: 0.5749 - val_loss: 17.5018 - val_f1: 0.4217\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 16.4222 - f1: 0.6129 - val_loss: 17.2667 - val_f1: 0.4304\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 16.2644 - f1: 0.6016 - val_loss: 17.0918 - val_f1: 0.4431\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 15.9805 - f1: 0.6088 - val_loss: 16.9945 - val_f1: 0.4049\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 554us/sample - loss: 15.6735 - f1: 0.6037 - val_loss: 16.8010 - val_f1: 0.4109\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 547us/sample - loss: 15.8279 - f1: 0.5816 - val_loss: 16.4815 - val_f1: 0.4368\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 15.3950 - f1: 0.6296 - val_loss: 16.2865 - val_f1: 0.4363\n",
      "Running through fold 3\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 50.4598 - f1: 0.0330 - val_loss: 48.6901 - val_f1: 0.0396\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 451us/sample - loss: 47.8420 - f1: 0.0411 - val_loss: 45.9865 - val_f1: 0.0631\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 451us/sample - loss: 45.7100 - f1: 0.0592 - val_loss: 43.9364 - val_f1: 0.0659\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 43.3290 - f1: 0.0640 - val_loss: 41.9779 - val_f1: 0.0955\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 458us/sample - loss: 41.5672 - f1: 0.0902 - val_loss: 39.7425 - val_f1: 0.1297\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 39.9276 - f1: 0.1230 - val_loss: 37.7440 - val_f1: 0.1755\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 452us/sample - loss: 38.6685 - f1: 0.1395 - val_loss: 37.1424 - val_f1: 0.1625\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 37.7655 - f1: 0.1257 - val_loss: 36.4222 - val_f1: 0.1531\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 36.3848 - f1: 0.1618 - val_loss: 34.9125 - val_f1: 0.2026\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 449us/sample - loss: 34.9745 - f1: 0.2009 - val_loss: 33.7823 - val_f1: 0.2125\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 33.5130 - f1: 0.2377 - val_loss: 32.4928 - val_f1: 0.2410\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 32.5299 - f1: 0.2123 - val_loss: 30.9845 - val_f1: 0.2816\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 456us/sample - loss: 31.0950 - f1: 0.2563 - val_loss: 30.4604 - val_f1: 0.2652\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 468us/sample - loss: 30.7345 - f1: 0.2864 - val_loss: 30.0566 - val_f1: 0.3008\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 29.7939 - f1: 0.3241 - val_loss: 28.9898 - val_f1: 0.2900\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 28.8528 - f1: 0.3421 - val_loss: 28.3235 - val_f1: 0.2992\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 27.6532 - f1: 0.3520 - val_loss: 27.7954 - val_f1: 0.3416\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 26.9684 - f1: 0.3893 - val_loss: 27.1191 - val_f1: 0.3171\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 458us/sample - loss: 26.4227 - f1: 0.3345 - val_loss: 26.3231 - val_f1: 0.3435\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 25.5029 - f1: 0.4050 - val_loss: 25.4148 - val_f1: 0.3600\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 24.8852 - f1: 0.4314 - val_loss: 24.6469 - val_f1: 0.3617\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 457us/sample - loss: 24.0167 - f1: 0.4376 - val_loss: 24.3328 - val_f1: 0.3929\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 23.8273 - f1: 0.4431 - val_loss: 24.1403 - val_f1: 0.3959\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 23.3128 - f1: 0.4555 - val_loss: 23.4210 - val_f1: 0.3878\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 455us/sample - loss: 22.9358 - f1: 0.4386 - val_loss: 23.1401 - val_f1: 0.3969\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 22.2327 - f1: 0.4752 - val_loss: 22.4034 - val_f1: 0.4054\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 454us/sample - loss: 21.9085 - f1: 0.4589 - val_loss: 22.1728 - val_f1: 0.4083\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 21.7031 - f1: 0.4446 - val_loss: 21.6832 - val_f1: 0.4107\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 21.0952 - f1: 0.4810 - val_loss: 21.1785 - val_f1: 0.4055\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 468us/sample - loss: 20.3768 - f1: 0.5540 - val_loss: 20.8535 - val_f1: 0.4479\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 20.1650 - f1: 0.5404 - val_loss: 20.6588 - val_f1: 0.4153\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 19.6255 - f1: 0.5188 - val_loss: 20.3684 - val_f1: 0.4158\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 19.3444 - f1: 0.5130 - val_loss: 19.8978 - val_f1: 0.4025\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 19.0165 - f1: 0.5421 - val_loss: 19.6009 - val_f1: 0.4183\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 471us/sample - loss: 18.5410 - f1: 0.5495 - val_loss: 19.2949 - val_f1: 0.4626\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 469us/sample - loss: 18.2030 - f1: 0.5936 - val_loss: 18.9717 - val_f1: 0.4505\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 468us/sample - loss: 18.1972 - f1: 0.5472 - val_loss: 18.7998 - val_f1: 0.4489\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 17.5932 - f1: 0.5772 - val_loss: 18.5191 - val_f1: 0.4567\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 473us/sample - loss: 17.5414 - f1: 0.5650 - val_loss: 18.2547 - val_f1: 0.4322\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 17.3601 - f1: 0.5545 - val_loss: 18.0979 - val_f1: 0.4492\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 17.0106 - f1: 0.5828 - val_loss: 18.0844 - val_f1: 0.4169\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 470us/sample - loss: 16.7843 - f1: 0.5956 - val_loss: 17.4213 - val_f1: 0.4827\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 469us/sample - loss: 16.6190 - f1: 0.6195 - val_loss: 17.1877 - val_f1: 0.4742\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 473us/sample - loss: 16.1789 - f1: 0.6166 - val_loss: 17.1435 - val_f1: 0.4422\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 16.1136 - f1: 0.6060 - val_loss: 17.0394 - val_f1: 0.4510\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 15.9545 - f1: 0.6050 - val_loss: 16.6946 - val_f1: 0.4491\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 15.7227 - f1: 0.5910 - val_loss: 16.4676 - val_f1: 0.4546\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 15.3969 - f1: 0.6158 - val_loss: 16.2355 - val_f1: 0.4778\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 15.1218 - f1: 0.6530 - val_loss: 16.3445 - val_f1: 0.4622\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 15.0877 - f1: 0.6238 - val_loss: 15.8510 - val_f1: 0.4772\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 14.9196 - f1: 0.6256 - val_loss: 15.8255 - val_f1: 0.4766\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 471us/sample - loss: 14.6781 - f1: 0.6572 - val_loss: 15.5818 - val_f1: 0.4855\n",
      "Running through fold 4\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 50.1987 - f1: 0.0432 - val_loss: 48.4644 - val_f1: 0.0593\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 47.4099 - f1: 0.0755 - val_loss: 45.2995 - val_f1: 0.0283\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 45.6388 - f1: 0.0449 - val_loss: 44.7358 - val_f1: 0.0721\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 470us/sample - loss: 43.9391 - f1: 0.0691 - val_loss: 41.8962 - val_f1: 0.1343\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 42.1643 - f1: 0.1169 - val_loss: 40.7066 - val_f1: 0.1393\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 477us/sample - loss: 40.2009 - f1: 0.1377 - val_loss: 38.0708 - val_f1: 0.1777\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 38.3233 - f1: 0.1543 - val_loss: 36.8881 - val_f1: 0.1833\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 476us/sample - loss: 37.2234 - f1: 0.1711 - val_loss: 35.5627 - val_f1: 0.2172\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 470us/sample - loss: 36.3581 - f1: 0.1828 - val_loss: 34.6373 - val_f1: 0.2318\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 472us/sample - loss: 35.2120 - f1: 0.2022 - val_loss: 33.7390 - val_f1: 0.2447\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 473us/sample - loss: 34.1274 - f1: 0.2276 - val_loss: 33.4401 - val_f1: 0.2565\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 472us/sample - loss: 33.1200 - f1: 0.2501 - val_loss: 31.3723 - val_f1: 0.3109\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 31.3617 - f1: 0.3053 - val_loss: 30.4412 - val_f1: 0.2973\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 468us/sample - loss: 30.4403 - f1: 0.3206 - val_loss: 29.8709 - val_f1: 0.3320\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 471us/sample - loss: 29.7242 - f1: 0.3276 - val_loss: 28.8560 - val_f1: 0.3494\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 28.4637 - f1: 0.3734 - val_loss: 28.1530 - val_f1: 0.3414\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 28.0216 - f1: 0.3460 - val_loss: 27.8846 - val_f1: 0.3257\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 27.2421 - f1: 0.3687 - val_loss: 27.0127 - val_f1: 0.3435\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 26.4842 - f1: 0.4285 - val_loss: 26.6157 - val_f1: 0.3460\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 25.7823 - f1: 0.3823 - val_loss: 26.0518 - val_f1: 0.3382\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 25.3059 - f1: 0.4093 - val_loss: 25.3054 - val_f1: 0.3777\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 470us/sample - loss: 24.3678 - f1: 0.4566 - val_loss: 24.7614 - val_f1: 0.3749\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 524us/sample - loss: 23.9490 - f1: 0.4698 - val_loss: 24.2472 - val_f1: 0.4009\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 552us/sample - loss: 23.3955 - f1: 0.4415 - val_loss: 23.7925 - val_f1: 0.3873\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 558us/sample - loss: 22.7478 - f1: 0.4896 - val_loss: 23.5245 - val_f1: 0.3908\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 22.3203 - f1: 0.4791 - val_loss: 22.7022 - val_f1: 0.4140\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 21.6663 - f1: 0.5219 - val_loss: 22.2405 - val_f1: 0.4315\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 21.3018 - f1: 0.4982 - val_loss: 21.8060 - val_f1: 0.4288\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 20.8246 - f1: 0.5205 - val_loss: 21.5962 - val_f1: 0.4112\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 477us/sample - loss: 20.5109 - f1: 0.5518 - val_loss: 21.1907 - val_f1: 0.4196\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 20.0247 - f1: 0.5398 - val_loss: 20.7871 - val_f1: 0.4276\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 19.6928 - f1: 0.5544 - val_loss: 20.4353 - val_f1: 0.4303\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 19.2748 - f1: 0.5448 - val_loss: 20.0427 - val_f1: 0.4481\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 19.0081 - f1: 0.5508 - val_loss: 19.9136 - val_f1: 0.4146\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 18.6232 - f1: 0.6184 - val_loss: 19.5801 - val_f1: 0.4329\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 18.3699 - f1: 0.5708 - val_loss: 19.1231 - val_f1: 0.4632\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 459us/sample - loss: 17.9356 - f1: 0.6223 - val_loss: 18.8978 - val_f1: 0.4441\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 17.6298 - f1: 0.6395 - val_loss: 18.6188 - val_f1: 0.4545\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 17.4764 - f1: 0.6233 - val_loss: 18.4755 - val_f1: 0.4354\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 17.1777 - f1: 0.6230 - val_loss: 18.1286 - val_f1: 0.4550\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 16.9019 - f1: 0.6104 - val_loss: 17.7941 - val_f1: 0.4692\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 16.7726 - f1: 0.6158 - val_loss: 17.6154 - val_f1: 0.4725\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 16.3062 - f1: 0.6434 - val_loss: 17.3606 - val_f1: 0.4821\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 461us/sample - loss: 16.1751 - f1: 0.6227 - val_loss: 17.1780 - val_f1: 0.4617\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 462us/sample - loss: 15.9248 - f1: 0.6389 - val_loss: 16.8820 - val_f1: 0.4692\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 15.6941 - f1: 0.6434 - val_loss: 16.8028 - val_f1: 0.4738\n",
      "Epoch 47/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 459us/sample - loss: 15.5547 - f1: 0.6308 - val_loss: 16.5422 - val_f1: 0.4842\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 463us/sample - loss: 15.1812 - f1: 0.6526 - val_loss: 16.4303 - val_f1: 0.4645\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 15.1114 - f1: 0.6472 - val_loss: 16.0664 - val_f1: 0.4825\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 14.9061 - f1: 0.6653 - val_loss: 15.8922 - val_f1: 0.4890\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 14.6455 - f1: 0.6682 - val_loss: 15.8043 - val_f1: 0.4831\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 460us/sample - loss: 14.4649 - f1: 0.6798 - val_loss: 15.6060 - val_f1: 0.4687\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 14.3795 - f1: 0.6736 - val_loss: 15.4849 - val_f1: 0.4818\n",
      "\n",
      "\n",
      "Running through training size 1000\n",
      "Running through fold 0\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 934us/sample - loss: 49.1873 - f1: 0.0351 - val_loss: 45.6490 - val_f1: 0.0317\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 44.8500 - f1: 0.0729 - val_loss: 41.7576 - val_f1: 0.1331\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 41.7162 - f1: 0.1173 - val_loss: 40.2085 - val_f1: 0.1535\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 38.8628 - f1: 0.1572 - val_loss: 36.1476 - val_f1: 0.2093\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 36.4173 - f1: 0.1696 - val_loss: 34.1659 - val_f1: 0.2112\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 34.4186 - f1: 0.1969 - val_loss: 32.7380 - val_f1: 0.2149\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 32.0430 - f1: 0.2171 - val_loss: 30.7469 - val_f1: 0.2610\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 30.2531 - f1: 0.2427 - val_loss: 29.2580 - val_f1: 0.2772\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 28.2765 - f1: 0.2930 - val_loss: 26.9311 - val_f1: 0.3466\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 27.1937 - f1: 0.2931 - val_loss: 25.9918 - val_f1: 0.3117\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 25.3990 - f1: 0.3456 - val_loss: 24.7746 - val_f1: 0.3644\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 24.6444 - f1: 0.3524 - val_loss: 23.9829 - val_f1: 0.3760\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 23.2900 - f1: 0.3828 - val_loss: 22.6124 - val_f1: 0.3959\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 22.2233 - f1: 0.3954 - val_loss: 21.7198 - val_f1: 0.3984\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 21.3625 - f1: 0.4390 - val_loss: 21.1792 - val_f1: 0.3977\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 20.6303 - f1: 0.4273 - val_loss: 20.6223 - val_f1: 0.3776\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 20.0056 - f1: 0.4310 - val_loss: 19.7670 - val_f1: 0.4186\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 19.1986 - f1: 0.4782 - val_loss: 19.1206 - val_f1: 0.4193\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 18.6236 - f1: 0.4522 - val_loss: 18.5077 - val_f1: 0.4377\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 18.1607 - f1: 0.4760 - val_loss: 18.1933 - val_f1: 0.3866\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 17.4946 - f1: 0.4658 - val_loss: 17.3208 - val_f1: 0.4588\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 17.2328 - f1: 0.4636 - val_loss: 17.1395 - val_f1: 0.4312\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 16.4187 - f1: 0.5112 - val_loss: 16.6441 - val_f1: 0.4604\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 16.0894 - f1: 0.5043 - val_loss: 16.2610 - val_f1: 0.4410\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 15.6761 - f1: 0.5032 - val_loss: 15.9287 - val_f1: 0.4479\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 289us/sample - loss: 15.2497 - f1: 0.5070 - val_loss: 15.4620 - val_f1: 0.4711\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 291us/sample - loss: 14.8571 - f1: 0.5231 - val_loss: 15.0974 - val_f1: 0.4792\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 289us/sample - loss: 14.6065 - f1: 0.5217 - val_loss: 14.8270 - val_f1: 0.4549\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 290us/sample - loss: 14.2093 - f1: 0.5472 - val_loss: 14.4954 - val_f1: 0.4775\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 292us/sample - loss: 13.9688 - f1: 0.5301 - val_loss: 14.1319 - val_f1: 0.4688\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 289us/sample - loss: 13.5629 - f1: 0.5491 - val_loss: 13.9405 - val_f1: 0.4689\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 290us/sample - loss: 13.2795 - f1: 0.5565 - val_loss: 13.6313 - val_f1: 0.4767\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 290us/sample - loss: 13.0048 - f1: 0.5410 - val_loss: 13.3173 - val_f1: 0.4942\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 288us/sample - loss: 12.8380 - f1: 0.5693 - val_loss: 13.0580 - val_f1: 0.4950\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 12.4869 - f1: 0.5706 - val_loss: 12.8087 - val_f1: 0.4853\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 12.2893 - f1: 0.5744 - val_loss: 12.5805 - val_f1: 0.4869\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 12.1328 - f1: 0.5716 - val_loss: 12.6053 - val_f1: 0.4896\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 11.9107 - f1: 0.5711 - val_loss: 12.1815 - val_f1: 0.4975\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 11.5488 - f1: 0.5854 - val_loss: 11.9411 - val_f1: 0.5191\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 11.4368 - f1: 0.6018 - val_loss: 11.6744 - val_f1: 0.5161\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 11.2155 - f1: 0.5954 - val_loss: 11.5951 - val_f1: 0.5281\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 10.9875 - f1: 0.6154 - val_loss: 11.3497 - val_f1: 0.5162\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 10.8005 - f1: 0.6058 - val_loss: 11.2322 - val_f1: 0.5110\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 10.5819 - f1: 0.6129 - val_loss: 11.1353 - val_f1: 0.4900\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 352us/sample - loss: 10.4645 - f1: 0.6059 - val_loss: 10.9736 - val_f1: 0.5122\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 281us/sample - loss: 10.2354 - f1: 0.6094 - val_loss: 10.6756 - val_f1: 0.5221\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 10.1079 - f1: 0.6173 - val_loss: 10.7612 - val_f1: 0.5149\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 286us/sample - loss: 10.2707 - f1: 0.5721 - val_loss: 10.4675 - val_f1: 0.5157\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 285us/sample - loss: 9.7733 - f1: 0.6374 - val_loss: 10.2490 - val_f1: 0.5312\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 9.8860 - f1: 0.5811 - val_loss: 10.4548 - val_f1: 0.4951\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 9.6698 - f1: 0.5908 - val_loss: 10.0932 - val_f1: 0.5158\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 9.4278 - f1: 0.6119 - val_loss: 9.8216 - val_f1: 0.5296\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 9.2285 - f1: 0.6302 - val_loss: 9.7485 - val_f1: 0.5240\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 287us/sample - loss: 9.1978 - f1: 0.6284 - val_loss: 9.6488 - val_f1: 0.5208\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 9.0426 - f1: 0.6116 - val_loss: 9.3400 - val_f1: 0.5485\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 8.8576 - f1: 0.6616 - val_loss: 9.2949 - val_f1: 0.5412\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 8.7119 - f1: 0.6485 - val_loss: 9.3763 - val_f1: 0.5395\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 8.5535 - f1: 0.6697 - val_loss: 9.0325 - val_f1: 0.5518\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 8.3792 - f1: 0.6775 - val_loss: 8.7494 - val_f1: 0.5793\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 8.3161 - f1: 0.6618 - val_loss: 8.7236 - val_f1: 0.5758\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 282us/sample - loss: 8.2620 - f1: 0.6625 - val_loss: 8.6076 - val_f1: 0.5713\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 0s 287us/sample - loss: 8.0782 - f1: 0.7081 - val_loss: 8.5056 - val_f1: 0.5732\n",
      "Epoch 63/2000\n",
      "1000/1000 [==============================] - 0s 283us/sample - loss: 7.9699 - f1: 0.7021 - val_loss: 8.6909 - val_f1: 0.5588\n",
      "Epoch 64/2000\n",
      "1000/1000 [==============================] - 0s 286us/sample - loss: 7.9689 - f1: 0.6638 - val_loss: 8.3999 - val_f1: 0.5593\n",
      "Epoch 65/2000\n",
      "1000/1000 [==============================] - 0s 286us/sample - loss: 7.8419 - f1: 0.6670 - val_loss: 8.3772 - val_f1: 0.5671\n",
      "Epoch 66/2000\n",
      "1000/1000 [==============================] - 0s 285us/sample - loss: 8.0254 - f1: 0.6284 - val_loss: 8.3783 - val_f1: 0.5348\n",
      "Epoch 67/2000\n",
      "1000/1000 [==============================] - 0s 284us/sample - loss: 7.7190 - f1: 0.6506 - val_loss: 8.1383 - val_f1: 0.5811\n",
      "Epoch 68/2000\n",
      "1000/1000 [==============================] - 0s 285us/sample - loss: 7.4970 - f1: 0.6908 - val_loss: 8.0000 - val_f1: 0.5749\n",
      "Epoch 69/2000\n",
      "1000/1000 [==============================] - 0s 291us/sample - loss: 7.3509 - f1: 0.6954 - val_loss: 7.9475 - val_f1: 0.5619\n",
      "Running through fold 1\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 954us/sample - loss: 49.0447 - f1: 0.0410 - val_loss: 45.8482 - val_f1: 0.0990\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 44.7927 - f1: 0.0790 - val_loss: 41.4501 - val_f1: 0.1424\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 41.0534 - f1: 0.1444 - val_loss: 38.0783 - val_f1: 0.2210\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 37.8303 - f1: 0.2071 - val_loss: 36.0339 - val_f1: 0.2499\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 36.1300 - f1: 0.2027 - val_loss: 33.6807 - val_f1: 0.2599\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 33.7926 - f1: 0.2519 - val_loss: 31.3049 - val_f1: 0.3412\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 31.8251 - f1: 0.2683 - val_loss: 29.4721 - val_f1: 0.3461\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 30.0132 - f1: 0.3181 - val_loss: 28.7481 - val_f1: 0.3257\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 28.3426 - f1: 0.3268 - val_loss: 27.2855 - val_f1: 0.3642\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 27.0925 - f1: 0.3367 - val_loss: 26.3338 - val_f1: 0.3094\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 25.4833 - f1: 0.3647 - val_loss: 24.8849 - val_f1: 0.3657\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 24.3449 - f1: 0.3906 - val_loss: 23.5153 - val_f1: 0.3901\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 23.0182 - f1: 0.4240 - val_loss: 22.6720 - val_f1: 0.3675\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 22.4523 - f1: 0.3979 - val_loss: 21.9840 - val_f1: 0.4117\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 21.3727 - f1: 0.4192 - val_loss: 21.0066 - val_f1: 0.3948\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 20.4696 - f1: 0.4609 - val_loss: 20.4230 - val_f1: 0.4142\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 19.9441 - f1: 0.4568 - val_loss: 19.5269 - val_f1: 0.4244\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 19.0758 - f1: 0.4631 - val_loss: 18.9347 - val_f1: 0.4430\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 18.4163 - f1: 0.4982 - val_loss: 18.2824 - val_f1: 0.4439\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 17.8955 - f1: 0.4957 - val_loss: 17.8056 - val_f1: 0.4503\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 17.3813 - f1: 0.5084 - val_loss: 17.4176 - val_f1: 0.4317\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 16.9069 - f1: 0.4976 - val_loss: 17.0053 - val_f1: 0.4585\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 16.5506 - f1: 0.5052 - val_loss: 16.6296 - val_f1: 0.4568\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 16.1297 - f1: 0.5075 - val_loss: 16.1965 - val_f1: 0.4333\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 15.5952 - f1: 0.5276 - val_loss: 15.6506 - val_f1: 0.4690\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 15.1571 - f1: 0.5553 - val_loss: 15.4518 - val_f1: 0.4677\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 14.8591 - f1: 0.5541 - val_loss: 14.9239 - val_f1: 0.4892\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 14.5097 - f1: 0.5597 - val_loss: 14.7171 - val_f1: 0.4743\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 14.0081 - f1: 0.5520 - val_loss: 14.4201 - val_f1: 0.5018\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 13.7514 - f1: 0.5860 - val_loss: 14.1041 - val_f1: 0.4981\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 13.4733 - f1: 0.5780 - val_loss: 13.8722 - val_f1: 0.4935\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 13.3287 - f1: 0.5705 - val_loss: 13.5463 - val_f1: 0.5115\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 12.9436 - f1: 0.5943 - val_loss: 13.3203 - val_f1: 0.4871\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 12.7453 - f1: 0.5786 - val_loss: 13.0333 - val_f1: 0.4990\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 12.4703 - f1: 0.5694 - val_loss: 12.8083 - val_f1: 0.5122\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 12.1486 - f1: 0.6124 - val_loss: 12.4356 - val_f1: 0.5223\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 11.8380 - f1: 0.6228 - val_loss: 12.4144 - val_f1: 0.4908\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 11.7548 - f1: 0.6012 - val_loss: 12.2224 - val_f1: 0.5017\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 11.4510 - f1: 0.6115 - val_loss: 11.8664 - val_f1: 0.5159\n",
      "Epoch 40/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 299us/sample - loss: 11.2387 - f1: 0.6103 - val_loss: 11.6101 - val_f1: 0.5397\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 347us/sample - loss: 11.0990 - f1: 0.6097 - val_loss: 11.5776 - val_f1: 0.5251\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 343us/sample - loss: 11.0747 - f1: 0.5995 - val_loss: 11.4332 - val_f1: 0.5095\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 10.9629 - f1: 0.5892 - val_loss: 11.1497 - val_f1: 0.5219\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 10.6788 - f1: 0.5937 - val_loss: 10.9641 - val_f1: 0.5340\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 10.3287 - f1: 0.6336 - val_loss: 10.8139 - val_f1: 0.5264\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 10.2679 - f1: 0.6449 - val_loss: 10.7658 - val_f1: 0.5340\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 10.0565 - f1: 0.6382 - val_loss: 10.5002 - val_f1: 0.5243\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 9.7609 - f1: 0.6441 - val_loss: 10.2656 - val_f1: 0.5405\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 10.0272 - f1: 0.6030 - val_loss: 10.3477 - val_f1: 0.5212\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 9.5420 - f1: 0.6408 - val_loss: 10.0471 - val_f1: 0.5356\n",
      "Running through fold 2\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 972us/sample - loss: 48.7430 - f1: 0.0399 - val_loss: 45.0124 - val_f1: 0.1250\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 44.1166 - f1: 0.1005 - val_loss: 41.5173 - val_f1: 0.1624\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 41.1525 - f1: 0.1221 - val_loss: 38.8473 - val_f1: 0.1604\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 38.2419 - f1: 0.1665 - val_loss: 36.4487 - val_f1: 0.2040\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 35.9062 - f1: 0.1854 - val_loss: 33.3713 - val_f1: 0.2426\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 33.7707 - f1: 0.2052 - val_loss: 32.2521 - val_f1: 0.2862\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 31.7754 - f1: 0.2690 - val_loss: 30.2710 - val_f1: 0.2884\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 29.6678 - f1: 0.2600 - val_loss: 28.1897 - val_f1: 0.3418\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 28.1729 - f1: 0.3145 - val_loss: 26.6790 - val_f1: 0.3196\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 26.4919 - f1: 0.3342 - val_loss: 25.5948 - val_f1: 0.3543\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 25.3130 - f1: 0.3322 - val_loss: 24.7415 - val_f1: 0.3372\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 23.9829 - f1: 0.3730 - val_loss: 23.4287 - val_f1: 0.3755\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 23.0248 - f1: 0.3796 - val_loss: 22.3832 - val_f1: 0.3903\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 21.8488 - f1: 0.3780 - val_loss: 21.2174 - val_f1: 0.4030\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 21.1162 - f1: 0.4109 - val_loss: 20.5686 - val_f1: 0.4128\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 20.1539 - f1: 0.4104 - val_loss: 19.9106 - val_f1: 0.4008\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 19.3915 - f1: 0.4333 - val_loss: 19.1055 - val_f1: 0.4283\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 18.9145 - f1: 0.4215 - val_loss: 18.5985 - val_f1: 0.4202\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 18.1749 - f1: 0.4376 - val_loss: 18.0163 - val_f1: 0.4437\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 17.6756 - f1: 0.4502 - val_loss: 17.5626 - val_f1: 0.4424\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 17.0282 - f1: 0.4661 - val_loss: 16.9694 - val_f1: 0.4600\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 16.5395 - f1: 0.4878 - val_loss: 16.6190 - val_f1: 0.4346\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 15.8800 - f1: 0.5321 - val_loss: 16.0690 - val_f1: 0.4739\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 15.5118 - f1: 0.5126 - val_loss: 15.6845 - val_f1: 0.4751\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 15.1411 - f1: 0.5275 - val_loss: 15.3943 - val_f1: 0.4566\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 14.7430 - f1: 0.5233 - val_loss: 14.9362 - val_f1: 0.4797\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 14.3400 - f1: 0.5291 - val_loss: 14.7726 - val_f1: 0.4791\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 14.1148 - f1: 0.5124 - val_loss: 14.3939 - val_f1: 0.4469\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 13.6069 - f1: 0.5843 - val_loss: 13.7583 - val_f1: 0.5104\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 13.3772 - f1: 0.5562 - val_loss: 13.6765 - val_f1: 0.4950\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 13.0418 - f1: 0.5825 - val_loss: 13.4143 - val_f1: 0.5034\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 12.8030 - f1: 0.5604 - val_loss: 13.1313 - val_f1: 0.4892\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 12.5631 - f1: 0.5683 - val_loss: 13.0493 - val_f1: 0.4615\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 12.2384 - f1: 0.5704 - val_loss: 12.5048 - val_f1: 0.5027\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 11.9739 - f1: 0.5892 - val_loss: 12.2732 - val_f1: 0.5099\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 11.7618 - f1: 0.5867 - val_loss: 12.0960 - val_f1: 0.5239\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 11.4883 - f1: 0.6091 - val_loss: 11.8851 - val_f1: 0.5313\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 11.2219 - f1: 0.6024 - val_loss: 11.5532 - val_f1: 0.5334\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 11.0261 - f1: 0.6125 - val_loss: 11.4469 - val_f1: 0.5282\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 10.7847 - f1: 0.6126 - val_loss: 11.1950 - val_f1: 0.5293\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 10.6366 - f1: 0.6195 - val_loss: 11.0699 - val_f1: 0.5313\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 10.5749 - f1: 0.5966 - val_loss: 10.9126 - val_f1: 0.5147\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 10.3358 - f1: 0.5977 - val_loss: 10.6567 - val_f1: 0.5282\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 10.1336 - f1: 0.6225 - val_loss: 10.4528 - val_f1: 0.5525\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 9.9075 - f1: 0.6258 - val_loss: 10.3836 - val_f1: 0.5360\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 9.8530 - f1: 0.6113 - val_loss: 10.1210 - val_f1: 0.5579\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 9.6656 - f1: 0.6248 - val_loss: 10.0873 - val_f1: 0.5251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 9.4907 - f1: 0.6187 - val_loss: 9.8958 - val_f1: 0.5444\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 9.2677 - f1: 0.6360 - val_loss: 9.7537 - val_f1: 0.5368\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 9.1351 - f1: 0.6372 - val_loss: 9.6147 - val_f1: 0.5613\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 8.9724 - f1: 0.6599 - val_loss: 9.3816 - val_f1: 0.5704\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 8.8221 - f1: 0.6616 - val_loss: 9.1884 - val_f1: 0.5920\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 8.8207 - f1: 0.6449 - val_loss: 9.2119 - val_f1: 0.5638\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 8.6413 - f1: 0.6522 - val_loss: 9.3451 - val_f1: 0.5013\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 8.4301 - f1: 0.6737 - val_loss: 8.8615 - val_f1: 0.5916\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 8.2948 - f1: 0.6875 - val_loss: 8.8919 - val_f1: 0.5629\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 8.2968 - f1: 0.6306 - val_loss: 8.7040 - val_f1: 0.5607\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 8.1522 - f1: 0.6665 - val_loss: 8.6831 - val_f1: 0.5663\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 342us/sample - loss: 8.0800 - f1: 0.6546 - val_loss: 8.6083 - val_f1: 0.5648\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 7.8428 - f1: 0.6798 - val_loss: 8.3170 - val_f1: 0.5780\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 7.7370 - f1: 0.6969 - val_loss: 8.3673 - val_f1: 0.5764\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 0s 334us/sample - loss: 7.6237 - f1: 0.6911 - val_loss: 8.0460 - val_f1: 0.5910\n",
      "Running through fold 3\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 49.0104 - f1: 0.0596 - val_loss: 46.0587 - val_f1: 0.0714\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 44.8314 - f1: 0.0653 - val_loss: 41.4431 - val_f1: 0.1318\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 40.6252 - f1: 0.1529 - val_loss: 38.1904 - val_f1: 0.1803\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 37.9506 - f1: 0.1537 - val_loss: 35.1641 - val_f1: 0.2220\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 35.0109 - f1: 0.2287 - val_loss: 33.3152 - val_f1: 0.2634\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 33.2220 - f1: 0.2597 - val_loss: 31.3310 - val_f1: 0.3026\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 31.0211 - f1: 0.2944 - val_loss: 30.3340 - val_f1: 0.3144\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 29.5557 - f1: 0.3111 - val_loss: 28.9412 - val_f1: 0.3074\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 28.1745 - f1: 0.3152 - val_loss: 27.2178 - val_f1: 0.3475\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 26.9590 - f1: 0.3323 - val_loss: 25.5989 - val_f1: 0.3811\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 25.4226 - f1: 0.3785 - val_loss: 24.9984 - val_f1: 0.3800\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 24.1670 - f1: 0.4236 - val_loss: 23.5531 - val_f1: 0.4179\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 22.9618 - f1: 0.4461 - val_loss: 23.0482 - val_f1: 0.3938\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 22.2625 - f1: 0.4523 - val_loss: 21.9370 - val_f1: 0.4153\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 21.4776 - f1: 0.4603 - val_loss: 21.3962 - val_f1: 0.4238\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 20.6897 - f1: 0.4709 - val_loss: 20.5411 - val_f1: 0.4007\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 19.9308 - f1: 0.4630 - val_loss: 19.7801 - val_f1: 0.4405\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 19.4318 - f1: 0.4563 - val_loss: 19.6711 - val_f1: 0.4192\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 18.9267 - f1: 0.4657 - val_loss: 18.5862 - val_f1: 0.4594\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 18.1857 - f1: 0.4714 - val_loss: 18.1373 - val_f1: 0.4675\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 17.5094 - f1: 0.5186 - val_loss: 18.0601 - val_f1: 0.4299\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 17.2481 - f1: 0.4946 - val_loss: 17.3964 - val_f1: 0.4602\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 16.7935 - f1: 0.5193 - val_loss: 16.8638 - val_f1: 0.4611\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 16.2118 - f1: 0.5388 - val_loss: 16.4679 - val_f1: 0.4645\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 15.7444 - f1: 0.5537 - val_loss: 15.8360 - val_f1: 0.4939\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 15.3452 - f1: 0.5642 - val_loss: 15.4674 - val_f1: 0.5066\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 14.9797 - f1: 0.5519 - val_loss: 15.1429 - val_f1: 0.5028\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 14.6716 - f1: 0.5440 - val_loss: 15.0599 - val_f1: 0.4915\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 14.3441 - f1: 0.5517 - val_loss: 14.5118 - val_f1: 0.4978\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 13.9826 - f1: 0.5837 - val_loss: 14.3709 - val_f1: 0.4772\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 13.6518 - f1: 0.5916 - val_loss: 13.9067 - val_f1: 0.5270\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 13.3904 - f1: 0.5685 - val_loss: 13.7074 - val_f1: 0.5154\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 13.1848 - f1: 0.5858 - val_loss: 13.5620 - val_f1: 0.5290\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 13.0013 - f1: 0.5762 - val_loss: 13.1444 - val_f1: 0.5202\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 12.5045 - f1: 0.5977 - val_loss: 12.8151 - val_f1: 0.5455\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 12.5058 - f1: 0.5995 - val_loss: 12.7048 - val_f1: 0.5361\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 12.1198 - f1: 0.6193 - val_loss: 12.5382 - val_f1: 0.5357\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 11.9047 - f1: 0.6298 - val_loss: 12.5075 - val_f1: 0.4845\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 327us/sample - loss: 11.7148 - f1: 0.6161 - val_loss: 11.9911 - val_f1: 0.5425\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 369us/sample - loss: 11.3778 - f1: 0.6210 - val_loss: 11.9502 - val_f1: 0.5172\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 11.2899 - f1: 0.6336 - val_loss: 11.6964 - val_f1: 0.5372\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 10.9925 - f1: 0.6331 - val_loss: 11.3650 - val_f1: 0.5647\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 10.7024 - f1: 0.6611 - val_loss: 11.2464 - val_f1: 0.5520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 10.7090 - f1: 0.6369 - val_loss: 11.0958 - val_f1: 0.5528\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 10.5019 - f1: 0.6591 - val_loss: 10.9066 - val_f1: 0.5664\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 10.2488 - f1: 0.6459 - val_loss: 10.6717 - val_f1: 0.5744\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 10.1671 - f1: 0.6677 - val_loss: 10.7484 - val_f1: 0.5419\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 9.9540 - f1: 0.6471 - val_loss: 10.3963 - val_f1: 0.5553\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 9.8303 - f1: 0.6484 - val_loss: 10.1713 - val_f1: 0.5710\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 9.7881 - f1: 0.6401 - val_loss: 10.2182 - val_f1: 0.5468\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 9.5234 - f1: 0.6609 - val_loss: 9.9478 - val_f1: 0.5798\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 298us/sample - loss: 9.4569 - f1: 0.6531 - val_loss: 9.8509 - val_f1: 0.5803\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 9.2261 - f1: 0.6610 - val_loss: 9.7319 - val_f1: 0.5816\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 9.3357 - f1: 0.6367 - val_loss: 10.0135 - val_f1: 0.5304\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 9.0306 - f1: 0.6849 - val_loss: 9.4820 - val_f1: 0.5804\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 8.8640 - f1: 0.6795 - val_loss: 9.3014 - val_f1: 0.5996\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 292us/sample - loss: 8.6882 - f1: 0.6762 - val_loss: 9.2340 - val_f1: 0.5865\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 8.5612 - f1: 0.6969 - val_loss: 9.3206 - val_f1: 0.5448\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 8.4572 - f1: 0.6912 - val_loss: 8.9946 - val_f1: 0.5786\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 296us/sample - loss: 8.3089 - f1: 0.6997 - val_loss: 8.9549 - val_f1: 0.5693\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 8.2261 - f1: 0.6939 - val_loss: 8.6536 - val_f1: 0.6193\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 8.0105 - f1: 0.7340 - val_loss: 8.6388 - val_f1: 0.5943\n",
      "Epoch 63/2000\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 7.9670 - f1: 0.7307 - val_loss: 8.5651 - val_f1: 0.5977\n",
      "Epoch 64/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 7.8643 - f1: 0.7262 - val_loss: 8.4493 - val_f1: 0.6088\n",
      "Epoch 65/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 7.7093 - f1: 0.7266 - val_loss: 8.3383 - val_f1: 0.5971\n",
      "Epoch 66/2000\n",
      "1000/1000 [==============================] - 0s 292us/sample - loss: 7.6319 - f1: 0.7221 - val_loss: 8.2165 - val_f1: 0.6048\n",
      "Epoch 67/2000\n",
      "1000/1000 [==============================] - 0s 295us/sample - loss: 7.5128 - f1: 0.7274 - val_loss: 8.1234 - val_f1: 0.5972\n",
      "Epoch 68/2000\n",
      "1000/1000 [==============================] - 0s 294us/sample - loss: 7.3949 - f1: 0.7448 - val_loss: 7.9748 - val_f1: 0.6161\n",
      "Epoch 69/2000\n",
      "1000/1000 [==============================] - 0s 329us/sample - loss: 7.3926 - f1: 0.7161 - val_loss: 7.9582 - val_f1: 0.5882\n",
      "Epoch 70/2000\n",
      "1000/1000 [==============================] - 0s 354us/sample - loss: 7.2965 - f1: 0.7153 - val_loss: 7.8195 - val_f1: 0.6141\n",
      "Epoch 71/2000\n",
      "1000/1000 [==============================] - 0s 309us/sample - loss: 7.1789 - f1: 0.7218 - val_loss: 7.8172 - val_f1: 0.5993\n",
      "Running through fold 4\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 49.7574 - f1: 0.0370 - val_loss: 45.9009 - val_f1: 0.0571\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 45.7097 - f1: 0.0659 - val_loss: 42.4219 - val_f1: 0.1384\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 41.9948 - f1: 0.1107 - val_loss: 39.1536 - val_f1: 0.1776\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 39.1042 - f1: 0.1335 - val_loss: 36.8503 - val_f1: 0.1934\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 36.4249 - f1: 0.1963 - val_loss: 35.0872 - val_f1: 0.2015\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 34.4724 - f1: 0.2039 - val_loss: 33.2594 - val_f1: 0.2076\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 32.3367 - f1: 0.2235 - val_loss: 30.8703 - val_f1: 0.2274\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 30.1960 - f1: 0.2320 - val_loss: 28.5316 - val_f1: 0.2700\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 28.6495 - f1: 0.2923 - val_loss: 27.4945 - val_f1: 0.2753\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 27.0776 - f1: 0.2901 - val_loss: 25.5061 - val_f1: 0.3382\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 25.3520 - f1: 0.3397 - val_loss: 24.8344 - val_f1: 0.3175\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 24.2472 - f1: 0.3622 - val_loss: 23.5305 - val_f1: 0.3346\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 22.9898 - f1: 0.3757 - val_loss: 22.2305 - val_f1: 0.3749\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 21.8548 - f1: 0.3968 - val_loss: 21.4549 - val_f1: 0.3703\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 21.0730 - f1: 0.3763 - val_loss: 21.1595 - val_f1: 0.3320\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 20.2593 - f1: 0.3974 - val_loss: 19.9649 - val_f1: 0.3953\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 19.5322 - f1: 0.4095 - val_loss: 19.2395 - val_f1: 0.3693\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 18.6477 - f1: 0.4440 - val_loss: 18.5073 - val_f1: 0.3976\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 17.9344 - f1: 0.4631 - val_loss: 17.7317 - val_f1: 0.4315\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 17.3368 - f1: 0.4786 - val_loss: 17.1746 - val_f1: 0.4408\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 16.7127 - f1: 0.4867 - val_loss: 16.8033 - val_f1: 0.4174\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 16.4361 - f1: 0.4753 - val_loss: 16.6654 - val_f1: 0.3855\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 15.9452 - f1: 0.4975 - val_loss: 16.0016 - val_f1: 0.4230\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 15.5572 - f1: 0.4892 - val_loss: 15.4579 - val_f1: 0.4508\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 15.0645 - f1: 0.5036 - val_loss: 15.0250 - val_f1: 0.4710\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 14.7683 - f1: 0.5050 - val_loss: 14.6377 - val_f1: 0.4727\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 14.1278 - f1: 0.5603 - val_loss: 14.3242 - val_f1: 0.4974\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 297us/sample - loss: 13.9126 - f1: 0.5280 - val_loss: 14.2044 - val_f1: 0.4699\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 13.6342 - f1: 0.5326 - val_loss: 13.9670 - val_f1: 0.4718\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 13.2570 - f1: 0.5583 - val_loss: 13.3944 - val_f1: 0.5031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 13.0202 - f1: 0.5399 - val_loss: 13.2533 - val_f1: 0.4886\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 12.8341 - f1: 0.5452 - val_loss: 13.0124 - val_f1: 0.4917\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 12.4501 - f1: 0.5771 - val_loss: 12.5766 - val_f1: 0.5272\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 12.2080 - f1: 0.5654 - val_loss: 12.4177 - val_f1: 0.5095\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 11.8967 - f1: 0.5990 - val_loss: 12.1505 - val_f1: 0.5167\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 11.6403 - f1: 0.6002 - val_loss: 11.9850 - val_f1: 0.5148\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 11.5140 - f1: 0.6031 - val_loss: 11.7459 - val_f1: 0.5243\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 363us/sample - loss: 11.1357 - f1: 0.6236 - val_loss: 11.4470 - val_f1: 0.5484\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 357us/sample - loss: 10.9296 - f1: 0.6247 - val_loss: 11.3542 - val_f1: 0.5337\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 10.8648 - f1: 0.6018 - val_loss: 11.1732 - val_f1: 0.5348\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 10.8521 - f1: 0.5989 - val_loss: 11.0429 - val_f1: 0.5288\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 299us/sample - loss: 10.5803 - f1: 0.6086 - val_loss: 10.8800 - val_f1: 0.5353\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 10.3034 - f1: 0.6222 - val_loss: 10.6383 - val_f1: 0.5345\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 10.0357 - f1: 0.6582 - val_loss: 10.4679 - val_f1: 0.5409\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 9.9857 - f1: 0.6129 - val_loss: 10.4444 - val_f1: 0.5239\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 9.9810 - f1: 0.6029 - val_loss: 10.3549 - val_f1: 0.5101\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 9.6501 - f1: 0.6327 - val_loss: 10.0086 - val_f1: 0.5530\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 9.4844 - f1: 0.6417 - val_loss: 9.9122 - val_f1: 0.5350\n",
      "\n",
      "\n",
      "Running through training size 5000\n",
      "Running through fold 0\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 310us/sample - loss: 42.3924 - f1: 0.1010 - val_loss: 34.1649 - val_f1: 0.1916\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 30.7172 - f1: 0.2504 - val_loss: 26.7463 - val_f1: 0.3111\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 24.0456 - f1: 0.3312 - val_loss: 21.3761 - val_f1: 0.3906\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 19.8541 - f1: 0.3681 - val_loss: 17.7908 - val_f1: 0.3839\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 17.0088 - f1: 0.3983 - val_loss: 15.7796 - val_f1: 0.4375\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 15.0856 - f1: 0.4122 - val_loss: 14.0281 - val_f1: 0.4442\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 13.4720 - f1: 0.4366 - val_loss: 12.6200 - val_f1: 0.4712\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 12.3118 - f1: 0.4598 - val_loss: 11.6141 - val_f1: 0.4747\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 11.2066 - f1: 0.4800 - val_loss: 10.4232 - val_f1: 0.5457\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 10.3025 - f1: 0.4936 - val_loss: 9.7332 - val_f1: 0.5229\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 9.5670 - f1: 0.5108 - val_loss: 9.0133 - val_f1: 0.5643\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 8.8560 - f1: 0.5274 - val_loss: 8.3532 - val_f1: 0.5793\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 8.2917 - f1: 0.5363 - val_loss: 7.8781 - val_f1: 0.5751\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 7.7990 - f1: 0.5437 - val_loss: 7.4206 - val_f1: 0.5833\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 7.3597 - f1: 0.5520 - val_loss: 7.0507 - val_f1: 0.5898\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 6.9015 - f1: 0.5772 - val_loss: 6.5458 - val_f1: 0.6156\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 6.5050 - f1: 0.5845 - val_loss: 6.1525 - val_f1: 0.6245\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 6.1443 - f1: 0.5939 - val_loss: 5.9744 - val_f1: 0.5947\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 5.8505 - f1: 0.5990 - val_loss: 5.5590 - val_f1: 0.6364\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 5.4983 - f1: 0.6221 - val_loss: 5.3599 - val_f1: 0.6346\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 5.2642 - f1: 0.6106 - val_loss: 5.1133 - val_f1: 0.6365\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 4.9975 - f1: 0.6212 - val_loss: 4.8935 - val_f1: 0.6293\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 4.8047 - f1: 0.6187 - val_loss: 4.6427 - val_f1: 0.6456\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 4.5864 - f1: 0.6386 - val_loss: 4.4341 - val_f1: 0.6575\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 4.3743 - f1: 0.6362 - val_loss: 4.2162 - val_f1: 0.6737\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 4.2128 - f1: 0.6401 - val_loss: 4.0755 - val_f1: 0.6679\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 4.0341 - f1: 0.6509 - val_loss: 4.0040 - val_f1: 0.6452\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.9152 - f1: 0.6557 - val_loss: 3.8218 - val_f1: 0.6618\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.7649 - f1: 0.6479 - val_loss: 3.7381 - val_f1: 0.6491\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 180us/sample - loss: 3.5819 - f1: 0.6737 - val_loss: 3.4863 - val_f1: 0.6929\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.4557 - f1: 0.6784 - val_loss: 3.3333 - val_f1: 0.7029\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.4019 - f1: 0.6713 - val_loss: 3.3242 - val_f1: 0.6826\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 3.2725 - f1: 0.6767 - val_loss: 3.1788 - val_f1: 0.6987\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.1200 - f1: 0.6962 - val_loss: 3.0319 - val_f1: 0.7270\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.0027 - f1: 0.6997 - val_loss: 3.0270 - val_f1: 0.6878\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.9477 - f1: 0.6826 - val_loss: 3.0250 - val_f1: 0.6826\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.8637 - f1: 0.6912 - val_loss: 2.7719 - val_f1: 0.7265\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.7774 - f1: 0.7025 - val_loss: 2.7383 - val_f1: 0.7171\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.7822 - f1: 0.6793 - val_loss: 2.7570 - val_f1: 0.6892\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.6547 - f1: 0.7004 - val_loss: 2.5338 - val_f1: 0.7415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.5151 - f1: 0.7181 - val_loss: 2.5990 - val_f1: 0.6945\n",
      "Epoch 42/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.4907 - f1: 0.7142 - val_loss: 2.3886 - val_f1: 0.7386\n",
      "Epoch 43/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.4192 - f1: 0.7170 - val_loss: 2.3767 - val_f1: 0.7354\n",
      "Epoch 44/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.3423 - f1: 0.7267 - val_loss: 2.3431 - val_f1: 0.7355\n",
      "Epoch 45/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.3581 - f1: 0.7087 - val_loss: 2.2704 - val_f1: 0.7454\n",
      "Epoch 46/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.3122 - f1: 0.7079 - val_loss: 2.2551 - val_f1: 0.7254\n",
      "Epoch 47/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.2803 - f1: 0.7050 - val_loss: 2.2443 - val_f1: 0.7352\n",
      "Epoch 48/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.1413 - f1: 0.7349 - val_loss: 2.1509 - val_f1: 0.7427\n",
      "Epoch 49/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.1509 - f1: 0.7209 - val_loss: 2.1592 - val_f1: 0.7297\n",
      "Epoch 50/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.0517 - f1: 0.7472 - val_loss: 2.0799 - val_f1: 0.7383\n",
      "Running through fold 1\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 321us/sample - loss: 41.9398 - f1: 0.1251 - val_loss: 34.2987 - val_f1: 0.2782\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 30.4548 - f1: 0.2673 - val_loss: 25.6462 - val_f1: 0.3831\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 175us/sample - loss: 23.5900 - f1: 0.3626 - val_loss: 20.7261 - val_f1: 0.4426\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 19.5270 - f1: 0.4092 - val_loss: 17.8658 - val_f1: 0.4645\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 16.8797 - f1: 0.4337 - val_loss: 15.5840 - val_f1: 0.4914\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 169us/sample - loss: 14.9218 - f1: 0.4460 - val_loss: 14.1500 - val_f1: 0.4766\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 174us/sample - loss: 13.3644 - f1: 0.4855 - val_loss: 12.6845 - val_f1: 0.4976\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 12.1045 - f1: 0.5037 - val_loss: 11.5646 - val_f1: 0.5300\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 168us/sample - loss: 11.1168 - f1: 0.5132 - val_loss: 10.5179 - val_f1: 0.5622\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 10.2807 - f1: 0.5161 - val_loss: 9.7574 - val_f1: 0.5723\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 9.4933 - f1: 0.5329 - val_loss: 9.0403 - val_f1: 0.5726\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 8.8927 - f1: 0.5443 - val_loss: 8.4544 - val_f1: 0.5691\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 8.2467 - f1: 0.5618 - val_loss: 7.8049 - val_f1: 0.6209\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 7.7423 - f1: 0.5814 - val_loss: 7.4878 - val_f1: 0.5967\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 7.3056 - f1: 0.5801 - val_loss: 6.9907 - val_f1: 0.6101\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 6.8268 - f1: 0.6079 - val_loss: 6.5559 - val_f1: 0.6344\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 6.4927 - f1: 0.6055 - val_loss: 6.2956 - val_f1: 0.6091\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 6.1470 - f1: 0.6065 - val_loss: 5.9952 - val_f1: 0.6253\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 5.8911 - f1: 0.6017 - val_loss: 5.6410 - val_f1: 0.6337\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 5.5787 - f1: 0.6119 - val_loss: 5.3762 - val_f1: 0.6289\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 5.2998 - f1: 0.6221 - val_loss: 5.0753 - val_f1: 0.6681\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 5.0480 - f1: 0.6368 - val_loss: 4.9047 - val_f1: 0.6569\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 4.8542 - f1: 0.6311 - val_loss: 4.6966 - val_f1: 0.6592\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 4.5905 - f1: 0.6526 - val_loss: 4.4650 - val_f1: 0.6790\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 4.3892 - f1: 0.6546 - val_loss: 4.2259 - val_f1: 0.6962\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 4.2370 - f1: 0.6526 - val_loss: 4.2462 - val_f1: 0.6501\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 4.0858 - f1: 0.6561 - val_loss: 4.0695 - val_f1: 0.6469\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.9208 - f1: 0.6579 - val_loss: 3.8795 - val_f1: 0.6656\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.7467 - f1: 0.6740 - val_loss: 3.8386 - val_f1: 0.6639\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.6120 - f1: 0.6737 - val_loss: 3.5712 - val_f1: 0.6908\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.5198 - f1: 0.6793 - val_loss: 3.3959 - val_f1: 0.7008\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.3948 - f1: 0.6827 - val_loss: 3.2472 - val_f1: 0.7228\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.2652 - f1: 0.6856 - val_loss: 3.1943 - val_f1: 0.7135\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.1432 - f1: 0.6971 - val_loss: 3.1340 - val_f1: 0.7064\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.0841 - f1: 0.6864 - val_loss: 2.9695 - val_f1: 0.7231\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.9697 - f1: 0.7002 - val_loss: 3.0368 - val_f1: 0.6881\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.8752 - f1: 0.7062 - val_loss: 2.8120 - val_f1: 0.7244\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.7740 - f1: 0.7116 - val_loss: 2.8043 - val_f1: 0.7149\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.6945 - f1: 0.7192 - val_loss: 2.6676 - val_f1: 0.7238\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.6112 - f1: 0.7203 - val_loss: 2.5511 - val_f1: 0.7424\n",
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.5658 - f1: 0.7173 - val_loss: 2.5456 - val_f1: 0.7285\n",
      "Epoch 42/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.5388 - f1: 0.7066 - val_loss: 2.4440 - val_f1: 0.7478\n",
      "Epoch 43/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.4325 - f1: 0.7243 - val_loss: 2.4496 - val_f1: 0.7152\n",
      "Epoch 44/2000\n",
      "5000/5000 [==============================] - 1s 171us/sample - loss: 2.3756 - f1: 0.7302 - val_loss: 2.3808 - val_f1: 0.7221\n",
      "Epoch 45/2000\n",
      "5000/5000 [==============================] - 1s 172us/sample - loss: 2.3081 - f1: 0.7256 - val_loss: 2.2475 - val_f1: 0.7519\n",
      "Epoch 46/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.2845 - f1: 0.7283 - val_loss: 2.3724 - val_f1: 0.7173\n",
      "Epoch 47/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.2308 - f1: 0.7247 - val_loss: 2.1522 - val_f1: 0.7561\n",
      "Epoch 48/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.1579 - f1: 0.7342 - val_loss: 2.1942 - val_f1: 0.7374\n",
      "Epoch 49/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.1500 - f1: 0.7262 - val_loss: 2.2143 - val_f1: 0.7246\n",
      "Epoch 50/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.0820 - f1: 0.7455 - val_loss: 2.1223 - val_f1: 0.7339\n",
      "Epoch 51/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.0668 - f1: 0.7301 - val_loss: 2.0535 - val_f1: 0.7465\n",
      "Epoch 52/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.0298 - f1: 0.7305 - val_loss: 1.9562 - val_f1: 0.7719\n",
      "Epoch 53/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 1.9507 - f1: 0.7498 - val_loss: 2.0226 - val_f1: 0.7377\n",
      "Epoch 54/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.9384 - f1: 0.7426 - val_loss: 2.0344 - val_f1: 0.7441\n",
      "Epoch 55/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.8807 - f1: 0.7498 - val_loss: 1.9228 - val_f1: 0.7349\n",
      "Epoch 56/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 1.8505 - f1: 0.7587 - val_loss: 1.8882 - val_f1: 0.7443\n",
      "Epoch 57/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.8597 - f1: 0.7467 - val_loss: 1.8144 - val_f1: 0.7667\n",
      "Epoch 58/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.7837 - f1: 0.7573 - val_loss: 1.9041 - val_f1: 0.7344\n",
      "Epoch 59/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.7491 - f1: 0.7608 - val_loss: 1.8655 - val_f1: 0.7275\n",
      "Epoch 60/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.6920 - f1: 0.7739 - val_loss: 1.7403 - val_f1: 0.7618\n",
      "Epoch 61/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.6813 - f1: 0.7694 - val_loss: 1.7103 - val_f1: 0.7673\n",
      "Epoch 62/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 1.6559 - f1: 0.7654 - val_loss: 1.7765 - val_f1: 0.7534\n",
      "Running through fold 2\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 325us/sample - loss: 41.7461 - f1: 0.1217 - val_loss: 33.6121 - val_f1: 0.2420\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 30.0030 - f1: 0.2643 - val_loss: 25.6626 - val_f1: 0.3725\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 23.6381 - f1: 0.3508 - val_loss: 20.6473 - val_f1: 0.4214\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 19.4485 - f1: 0.3944 - val_loss: 17.5602 - val_f1: 0.4360\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 16.7782 - f1: 0.4169 - val_loss: 15.4196 - val_f1: 0.4680\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 180us/sample - loss: 14.8138 - f1: 0.4317 - val_loss: 13.9316 - val_f1: 0.4679\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 13.3363 - f1: 0.4545 - val_loss: 12.4201 - val_f1: 0.5108\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 12.1015 - f1: 0.4756 - val_loss: 11.3186 - val_f1: 0.5223\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 11.0671 - f1: 0.4913 - val_loss: 10.4512 - val_f1: 0.5301\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 169us/sample - loss: 10.2294 - f1: 0.5037 - val_loss: 9.7036 - val_f1: 0.5511\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 173us/sample - loss: 9.4918 - f1: 0.5244 - val_loss: 9.0253 - val_f1: 0.5580\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 8.8207 - f1: 0.5352 - val_loss: 8.3966 - val_f1: 0.5740\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 8.2775 - f1: 0.5425 - val_loss: 7.7850 - val_f1: 0.5797\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 7.7922 - f1: 0.5406 - val_loss: 7.3780 - val_f1: 0.5714\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 7.3007 - f1: 0.5665 - val_loss: 6.9158 - val_f1: 0.6162\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 6.8697 - f1: 0.5698 - val_loss: 6.7468 - val_f1: 0.5530\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 6.5723 - f1: 0.5649 - val_loss: 6.2104 - val_f1: 0.6086\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 6.1222 - f1: 0.5992 - val_loss: 5.9499 - val_f1: 0.6124\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 5.8766 - f1: 0.6058 - val_loss: 5.6202 - val_f1: 0.6139\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 5.5869 - f1: 0.6037 - val_loss: 5.3889 - val_f1: 0.6078\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 5.3487 - f1: 0.6131 - val_loss: 5.1211 - val_f1: 0.6276\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 5.0897 - f1: 0.6145 - val_loss: 4.8342 - val_f1: 0.6466\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 4.8097 - f1: 0.6366 - val_loss: 4.5853 - val_f1: 0.6726\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 4.6203 - f1: 0.6338 - val_loss: 4.5085 - val_f1: 0.6357\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 4.4651 - f1: 0.6378 - val_loss: 4.2086 - val_f1: 0.6741\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 4.2658 - f1: 0.6521 - val_loss: 4.1409 - val_f1: 0.6587\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 4.0865 - f1: 0.6493 - val_loss: 4.0561 - val_f1: 0.6288\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.9308 - f1: 0.6626 - val_loss: 3.8161 - val_f1: 0.6691\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.8328 - f1: 0.6478 - val_loss: 3.6922 - val_f1: 0.6704\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.6670 - f1: 0.6575 - val_loss: 3.5489 - val_f1: 0.6783\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.5269 - f1: 0.6688 - val_loss: 3.3958 - val_f1: 0.7049\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.4054 - f1: 0.6721 - val_loss: 3.4141 - val_f1: 0.6682\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.3421 - f1: 0.6611 - val_loss: 3.2714 - val_f1: 0.6814\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.2091 - f1: 0.6761 - val_loss: 3.0424 - val_f1: 0.7122\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.0607 - f1: 0.6926 - val_loss: 2.9700 - val_f1: 0.7108\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.9850 - f1: 0.6930 - val_loss: 3.0020 - val_f1: 0.6665\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.9296 - f1: 0.6879 - val_loss: 2.8947 - val_f1: 0.7008\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.7735 - f1: 0.7122 - val_loss: 2.7686 - val_f1: 0.7037\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.7891 - f1: 0.6874 - val_loss: 2.7584 - val_f1: 0.6950\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.7027 - f1: 0.6872 - val_loss: 2.8233 - val_f1: 0.6510\n",
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 2.6101 - f1: 0.7057 - val_loss: 2.5736 - val_f1: 0.7061\n",
      "Running through fold 3\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 348us/sample - loss: 42.4411 - f1: 0.0896 - val_loss: 33.6983 - val_f1: 0.2423\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 182us/sample - loss: 30.4193 - f1: 0.2425 - val_loss: 25.8764 - val_f1: 0.3493\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 169us/sample - loss: 23.8409 - f1: 0.3339 - val_loss: 20.9306 - val_f1: 0.4211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 19.5757 - f1: 0.4028 - val_loss: 17.5398 - val_f1: 0.4865\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 16.8197 - f1: 0.4286 - val_loss: 15.4275 - val_f1: 0.4832\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 14.8549 - f1: 0.4566 - val_loss: 13.8839 - val_f1: 0.4975\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 13.3757 - f1: 0.4762 - val_loss: 12.4590 - val_f1: 0.5436\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 12.1678 - f1: 0.4795 - val_loss: 11.3651 - val_f1: 0.5357\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 11.0896 - f1: 0.5124 - val_loss: 10.5369 - val_f1: 0.5550\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 10.2671 - f1: 0.5186 - val_loss: 9.6951 - val_f1: 0.5777\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 9.5023 - f1: 0.5475 - val_loss: 9.1923 - val_f1: 0.5380\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 8.8498 - f1: 0.5540 - val_loss: 8.5990 - val_f1: 0.5661\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 176us/sample - loss: 8.3344 - f1: 0.5619 - val_loss: 7.9186 - val_f1: 0.5845\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 7.7665 - f1: 0.5790 - val_loss: 7.4399 - val_f1: 0.5961\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 7.2993 - f1: 0.5739 - val_loss: 7.0378 - val_f1: 0.5937\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 6.8340 - f1: 0.6026 - val_loss: 6.5355 - val_f1: 0.6229\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 6.5583 - f1: 0.5868 - val_loss: 6.2441 - val_f1: 0.6251\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 6.1516 - f1: 0.6065 - val_loss: 5.9548 - val_f1: 0.6329\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 5.8349 - f1: 0.6115 - val_loss: 5.6428 - val_f1: 0.6296\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 5.5369 - f1: 0.6297 - val_loss: 5.3486 - val_f1: 0.6387\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 5.3113 - f1: 0.6196 - val_loss: 5.1702 - val_f1: 0.6436\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 5.0445 - f1: 0.6324 - val_loss: 4.8178 - val_f1: 0.6535\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 4.8198 - f1: 0.6398 - val_loss: 4.7666 - val_f1: 0.6459\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 4.5881 - f1: 0.6547 - val_loss: 4.6270 - val_f1: 0.6237\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 4.3891 - f1: 0.6581 - val_loss: 4.2342 - val_f1: 0.6697\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 4.1617 - f1: 0.6698 - val_loss: 4.0515 - val_f1: 0.6786\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.9922 - f1: 0.6742 - val_loss: 3.9047 - val_f1: 0.6960\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 3.8725 - f1: 0.6763 - val_loss: 3.8413 - val_f1: 0.6773\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 3.7735 - f1: 0.6637 - val_loss: 3.6536 - val_f1: 0.6826\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 3.5865 - f1: 0.6807 - val_loss: 3.5043 - val_f1: 0.6937\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 3.4923 - f1: 0.6775 - val_loss: 3.4396 - val_f1: 0.6846\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.3943 - f1: 0.6794 - val_loss: 3.3605 - val_f1: 0.6811\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 186us/sample - loss: 3.2393 - f1: 0.6912 - val_loss: 3.2225 - val_f1: 0.6960\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.1820 - f1: 0.6808 - val_loss: 3.0874 - val_f1: 0.7045\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 3.0646 - f1: 0.6937 - val_loss: 3.0176 - val_f1: 0.7029\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.9591 - f1: 0.6930 - val_loss: 2.8736 - val_f1: 0.7154\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.8648 - f1: 0.7038 - val_loss: 2.7642 - val_f1: 0.7270\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.7740 - f1: 0.7086 - val_loss: 2.7111 - val_f1: 0.7351\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.7045 - f1: 0.7097 - val_loss: 2.7671 - val_f1: 0.6997\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.6459 - f1: 0.7098 - val_loss: 2.5259 - val_f1: 0.7468\n",
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.5333 - f1: 0.7260 - val_loss: 2.5752 - val_f1: 0.7200\n",
      "Epoch 42/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.4986 - f1: 0.7116 - val_loss: 2.5671 - val_f1: 0.7050\n",
      "Epoch 43/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.4470 - f1: 0.7147 - val_loss: 2.5528 - val_f1: 0.6964\n",
      "Epoch 44/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.3881 - f1: 0.7214 - val_loss: 2.2968 - val_f1: 0.7481\n",
      "Epoch 45/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.3331 - f1: 0.7254 - val_loss: 2.3911 - val_f1: 0.7140\n",
      "Epoch 46/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 2.3193 - f1: 0.7150 - val_loss: 2.3073 - val_f1: 0.7308\n",
      "Epoch 47/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.1930 - f1: 0.7350 - val_loss: 2.2113 - val_f1: 0.7415\n",
      "Epoch 48/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.1887 - f1: 0.7284 - val_loss: 2.2246 - val_f1: 0.7090\n",
      "Epoch 49/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 2.1487 - f1: 0.7207 - val_loss: 2.1945 - val_f1: 0.7195\n",
      "Epoch 50/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 2.0776 - f1: 0.7361 - val_loss: 2.1331 - val_f1: 0.7175\n",
      "Running through fold 4\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 344us/sample - loss: 42.5559 - f1: 0.1011 - val_loss: 34.2657 - val_f1: 0.2096\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 30.4797 - f1: 0.2396 - val_loss: 25.7741 - val_f1: 0.3552\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 23.6933 - f1: 0.3433 - val_loss: 21.0242 - val_f1: 0.4085\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 19.6870 - f1: 0.3848 - val_loss: 17.8369 - val_f1: 0.4334\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 16.8947 - f1: 0.4178 - val_loss: 15.7918 - val_f1: 0.4519\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 15.0662 - f1: 0.4330 - val_loss: 14.2220 - val_f1: 0.4503\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 13.5692 - f1: 0.4572 - val_loss: 12.5313 - val_f1: 0.5087\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 12.2040 - f1: 0.4949 - val_loss: 11.7578 - val_f1: 0.4915\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 11.2213 - f1: 0.4921 - val_loss: 10.5784 - val_f1: 0.5220\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 10.2772 - f1: 0.5333 - val_loss: 9.8050 - val_f1: 0.5429\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 9.5549 - f1: 0.5334 - val_loss: 9.2210 - val_f1: 0.5514\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 8.9272 - f1: 0.5411 - val_loss: 8.5178 - val_f1: 0.5741\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 170us/sample - loss: 8.4114 - f1: 0.5405 - val_loss: 7.9937 - val_f1: 0.5802\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 7.8594 - f1: 0.5562 - val_loss: 7.4545 - val_f1: 0.6080\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 7.3453 - f1: 0.5821 - val_loss: 7.0782 - val_f1: 0.6080\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 6.9087 - f1: 0.5974 - val_loss: 6.6431 - val_f1: 0.6168\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 6.5398 - f1: 0.6030 - val_loss: 6.3211 - val_f1: 0.6117\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 167us/sample - loss: 6.2441 - f1: 0.5933 - val_loss: 5.9697 - val_f1: 0.6213\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 5.8746 - f1: 0.6149 - val_loss: 5.7075 - val_f1: 0.6396\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 5.5663 - f1: 0.6221 - val_loss: 5.4783 - val_f1: 0.6249\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 5.3556 - f1: 0.6290 - val_loss: 5.1723 - val_f1: 0.6463\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 5.0934 - f1: 0.6266 - val_loss: 4.9406 - val_f1: 0.6492\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 4.9210 - f1: 0.6231 - val_loss: 4.7387 - val_f1: 0.6496\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 4.7074 - f1: 0.6310 - val_loss: 4.5728 - val_f1: 0.6462\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 4.4709 - f1: 0.6502 - val_loss: 4.3654 - val_f1: 0.6528\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 4.2509 - f1: 0.6589 - val_loss: 4.2009 - val_f1: 0.6553\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 4.0744 - f1: 0.6679 - val_loss: 4.0233 - val_f1: 0.6725\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 3.9734 - f1: 0.6626 - val_loss: 3.8626 - val_f1: 0.6602\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 3.8019 - f1: 0.6699 - val_loss: 3.6869 - val_f1: 0.6893\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 167us/sample - loss: 3.6460 - f1: 0.6727 - val_loss: 3.5483 - val_f1: 0.6969\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 3.5698 - f1: 0.6729 - val_loss: 3.4259 - val_f1: 0.7004\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 3.3579 - f1: 0.6975 - val_loss: 3.3255 - val_f1: 0.7014\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 3.2874 - f1: 0.6857 - val_loss: 3.2693 - val_f1: 0.6881\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 3.1939 - f1: 0.6800 - val_loss: 3.1394 - val_f1: 0.7013\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 3.0772 - f1: 0.6995 - val_loss: 3.0708 - val_f1: 0.6883\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.0545 - f1: 0.6800 - val_loss: 2.9009 - val_f1: 0.7219\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.9056 - f1: 0.7002 - val_loss: 2.8241 - val_f1: 0.7200\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.8232 - f1: 0.7022 - val_loss: 2.8383 - val_f1: 0.6891\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 185us/sample - loss: 2.7724 - f1: 0.6992 - val_loss: 2.6825 - val_f1: 0.7093\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.6855 - f1: 0.7004 - val_loss: 2.7796 - val_f1: 0.6734\n",
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.6314 - f1: 0.6957 - val_loss: 2.5940 - val_f1: 0.6992\n",
      "Epoch 42/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.5272 - f1: 0.7116 - val_loss: 2.5275 - val_f1: 0.7082\n",
      "Epoch 43/2000\n",
      "5000/5000 [==============================] - 1s 185us/sample - loss: 2.4884 - f1: 0.7066 - val_loss: 2.4353 - val_f1: 0.7353\n",
      "Epoch 44/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.3762 - f1: 0.7259 - val_loss: 2.4140 - val_f1: 0.7091\n",
      "Epoch 45/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.3587 - f1: 0.7142 - val_loss: 2.3959 - val_f1: 0.7018\n",
      "Epoch 46/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.2948 - f1: 0.7259 - val_loss: 2.3930 - val_f1: 0.7119\n",
      "Epoch 47/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 2.2819 - f1: 0.7139 - val_loss: 2.2255 - val_f1: 0.7243\n",
      "Epoch 48/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.1636 - f1: 0.7334 - val_loss: 2.2360 - val_f1: 0.7183\n",
      "Epoch 49/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.1461 - f1: 0.7324 - val_loss: 2.1261 - val_f1: 0.7289\n",
      "Epoch 50/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.1472 - f1: 0.7211 - val_loss: 2.1326 - val_f1: 0.7242\n",
      "Epoch 51/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.0616 - f1: 0.7288 - val_loss: 2.1212 - val_f1: 0.7236\n",
      "Epoch 52/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.0246 - f1: 0.7379 - val_loss: 2.0874 - val_f1: 0.7226\n",
      "Epoch 53/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.9934 - f1: 0.7318 - val_loss: 1.9699 - val_f1: 0.7430\n",
      "\n",
      "\n",
      "Running through training size 10000\n",
      "Running through fold 0\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 2s 233us/sample - loss: 36.5905 - f1: 0.1556 - val_loss: 25.3443 - val_f1: 0.3548\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 21.5638 - f1: 0.3431 - val_loss: 17.4769 - val_f1: 0.4304\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 15.6962 - f1: 0.4085 - val_loss: 13.5098 - val_f1: 0.4852\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 12.6067 - f1: 0.4484 - val_loss: 11.1838 - val_f1: 0.5139\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 10.5612 - f1: 0.4840 - val_loss: 9.6057 - val_f1: 0.5398\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 9.0647 - f1: 0.5168 - val_loss: 8.1642 - val_f1: 0.5749\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 7.9183 - f1: 0.5397 - val_loss: 7.1598 - val_f1: 0.6048\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 7.0610 - f1: 0.5461 - val_loss: 6.4886 - val_f1: 0.5828\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 6.2907 - f1: 0.5754 - val_loss: 5.8122 - val_f1: 0.6070\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 5.6692 - f1: 0.5908 - val_loss: 5.2121 - val_f1: 0.6396\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 5.1601 - f1: 0.6111 - val_loss: 4.8160 - val_f1: 0.6545\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 4.7306 - f1: 0.6212 - val_loss: 4.4809 - val_f1: 0.6453\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 4.3368 - f1: 0.6328 - val_loss: 4.0476 - val_f1: 0.6640\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 4.0139 - f1: 0.6352 - val_loss: 3.9325 - val_f1: 0.6407\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 3.7479 - f1: 0.6476 - val_loss: 3.4324 - val_f1: 0.7001\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 3.4852 - f1: 0.6565 - val_loss: 3.2345 - val_f1: 0.7013\n",
      "Epoch 17/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 143us/sample - loss: 3.2416 - f1: 0.6717 - val_loss: 2.9934 - val_f1: 0.7138\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 3.0334 - f1: 0.6817 - val_loss: 2.8723 - val_f1: 0.7145\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.8960 - f1: 0.6784 - val_loss: 2.6772 - val_f1: 0.7427\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.7331 - f1: 0.6874 - val_loss: 2.6218 - val_f1: 0.7047\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.5920 - f1: 0.6955 - val_loss: 2.4049 - val_f1: 0.7435\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.4509 - f1: 0.7079 - val_loss: 2.2691 - val_f1: 0.7662\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.3557 - f1: 0.7090 - val_loss: 2.2422 - val_f1: 0.7306\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.2457 - f1: 0.7148 - val_loss: 2.1982 - val_f1: 0.7190\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.1876 - f1: 0.7095 - val_loss: 1.9817 - val_f1: 0.7688\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.0760 - f1: 0.7201 - val_loss: 2.1149 - val_f1: 0.7070\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.9873 - f1: 0.7291 - val_loss: 1.9178 - val_f1: 0.7561\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 1.9396 - f1: 0.7330 - val_loss: 1.7994 - val_f1: 0.7705\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.8604 - f1: 0.7344 - val_loss: 1.8091 - val_f1: 0.7543\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 1.7815 - f1: 0.7463 - val_loss: 1.6972 - val_f1: 0.7724\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.7793 - f1: 0.7339 - val_loss: 1.6233 - val_f1: 0.7843\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.7101 - f1: 0.7444 - val_loss: 1.6480 - val_f1: 0.7667\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.6520 - f1: 0.7501 - val_loss: 1.6106 - val_f1: 0.7673\n",
      "Epoch 34/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.6133 - f1: 0.7476 - val_loss: 1.6484 - val_f1: 0.7374\n",
      "Epoch 35/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.6051 - f1: 0.7411 - val_loss: 1.4984 - val_f1: 0.7759\n",
      "Epoch 36/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.5354 - f1: 0.7559 - val_loss: 1.4469 - val_f1: 0.7873\n",
      "Epoch 37/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.5510 - f1: 0.7432 - val_loss: 1.4208 - val_f1: 0.7868\n",
      "Epoch 38/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.4868 - f1: 0.7562 - val_loss: 1.3934 - val_f1: 0.7873\n",
      "Epoch 39/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.4280 - f1: 0.7663 - val_loss: 1.3678 - val_f1: 0.7924\n",
      "Epoch 40/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.4299 - f1: 0.7636 - val_loss: 1.3544 - val_f1: 0.7924\n",
      "Epoch 41/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3767 - f1: 0.7679 - val_loss: 1.3536 - val_f1: 0.7867\n",
      "Running through fold 1\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 2s 236us/sample - loss: 36.7939 - f1: 0.1656 - val_loss: 25.4150 - val_f1: 0.3641\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 21.5106 - f1: 0.3646 - val_loss: 17.6028 - val_f1: 0.4506\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 15.7509 - f1: 0.4337 - val_loss: 13.6520 - val_f1: 0.5088\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 12.6211 - f1: 0.4739 - val_loss: 11.3118 - val_f1: 0.4876\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 10.5545 - f1: 0.5060 - val_loss: 9.4929 - val_f1: 0.5532\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 8.9998 - f1: 0.5398 - val_loss: 8.1814 - val_f1: 0.5836\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 7.9196 - f1: 0.5508 - val_loss: 7.3553 - val_f1: 0.5837\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 6.9978 - f1: 0.5736 - val_loss: 6.4288 - val_f1: 0.6283\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 6.2654 - f1: 0.5879 - val_loss: 5.7834 - val_f1: 0.6361\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 5.6496 - f1: 0.6075 - val_loss: 5.2566 - val_f1: 0.6358\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 5.1544 - f1: 0.6155 - val_loss: 4.8782 - val_f1: 0.6428\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 4.6717 - f1: 0.6330 - val_loss: 4.3810 - val_f1: 0.6655\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 4.3136 - f1: 0.6443 - val_loss: 4.0436 - val_f1: 0.6727\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 3.9978 - f1: 0.6495 - val_loss: 3.7477 - val_f1: 0.6795\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 3.6831 - f1: 0.6691 - val_loss: 3.5985 - val_f1: 0.6632\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 3.4805 - f1: 0.6634 - val_loss: 3.3150 - val_f1: 0.6936\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 3.2391 - f1: 0.6796 - val_loss: 3.0472 - val_f1: 0.7126\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 3.0062 - f1: 0.6940 - val_loss: 2.8588 - val_f1: 0.7205\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.8831 - f1: 0.6876 - val_loss: 2.7548 - val_f1: 0.7099\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.7150 - f1: 0.7012 - val_loss: 2.5513 - val_f1: 0.7320\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.5820 - f1: 0.6995 - val_loss: 2.4727 - val_f1: 0.7046\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.4566 - f1: 0.7045 - val_loss: 2.2928 - val_f1: 0.7366\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.3688 - f1: 0.7086 - val_loss: 2.2543 - val_f1: 0.7342\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 2.2213 - f1: 0.7253 - val_loss: 2.1329 - val_f1: 0.7427\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 147us/sample - loss: 2.1571 - f1: 0.7232 - val_loss: 2.1340 - val_f1: 0.7163\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 2.0558 - f1: 0.7329 - val_loss: 2.2872 - val_f1: 0.6862\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 1.9916 - f1: 0.7339 - val_loss: 1.9179 - val_f1: 0.7398\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 1.8723 - f1: 0.7490 - val_loss: 1.9151 - val_f1: 0.7377\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 1.8496 - f1: 0.7397 - val_loss: 1.8318 - val_f1: 0.7456\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 1.7980 - f1: 0.7400 - val_loss: 1.7645 - val_f1: 0.7447\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 1.7216 - f1: 0.7526 - val_loss: 1.6713 - val_f1: 0.7623\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 1.6986 - f1: 0.7472 - val_loss: 1.6465 - val_f1: 0.7618\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 1.6405 - f1: 0.7496 - val_loss: 1.5863 - val_f1: 0.7610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 1.5777 - f1: 0.7622 - val_loss: 1.5254 - val_f1: 0.7656\n",
      "Epoch 35/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.5386 - f1: 0.7637 - val_loss: 1.5826 - val_f1: 0.7516\n",
      "Epoch 36/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.4928 - f1: 0.7659 - val_loss: 1.4937 - val_f1: 0.7637\n",
      "Epoch 37/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.4840 - f1: 0.7628 - val_loss: 1.4111 - val_f1: 0.7826\n",
      "Epoch 38/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.4787 - f1: 0.7577 - val_loss: 1.4048 - val_f1: 0.7804\n",
      "Epoch 39/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.4112 - f1: 0.7748 - val_loss: 1.3631 - val_f1: 0.7837\n",
      "Epoch 40/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3762 - f1: 0.7757 - val_loss: 1.3608 - val_f1: 0.7807\n",
      "Epoch 41/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.3615 - f1: 0.7738 - val_loss: 1.3588 - val_f1: 0.7760\n",
      "Epoch 42/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3432 - f1: 0.7756 - val_loss: 1.3790 - val_f1: 0.7643\n",
      "Epoch 43/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.3237 - f1: 0.7725 - val_loss: 1.2899 - val_f1: 0.7868\n",
      "Epoch 44/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3016 - f1: 0.7786 - val_loss: 1.3443 - val_f1: 0.7531\n",
      "Epoch 45/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.2564 - f1: 0.7858 - val_loss: 1.2284 - val_f1: 0.7975\n",
      "Epoch 46/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.2154 - f1: 0.7949 - val_loss: 1.2751 - val_f1: 0.7649\n",
      "Epoch 47/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2537 - f1: 0.7821 - val_loss: 1.3050 - val_f1: 0.7614\n",
      "Epoch 48/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.1972 - f1: 0.7956 - val_loss: 1.2107 - val_f1: 0.7811\n",
      "Epoch 49/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2093 - f1: 0.7845 - val_loss: 1.1456 - val_f1: 0.7993\n",
      "Epoch 50/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.1757 - f1: 0.7906 - val_loss: 1.1818 - val_f1: 0.7934\n",
      "Epoch 51/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.1640 - f1: 0.7929 - val_loss: 1.1322 - val_f1: 0.8025\n",
      "Epoch 52/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.1270 - f1: 0.8000 - val_loss: 1.1690 - val_f1: 0.7900\n",
      "Epoch 53/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.1395 - f1: 0.7912 - val_loss: 1.1615 - val_f1: 0.7914\n",
      "Epoch 54/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.1075 - f1: 0.8012 - val_loss: 1.1139 - val_f1: 0.7994\n",
      "Epoch 55/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.0802 - f1: 0.8055 - val_loss: 1.1492 - val_f1: 0.7928\n",
      "Running through fold 2\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 2s 238us/sample - loss: 36.3574 - f1: 0.1685 - val_loss: 25.7000 - val_f1: 0.3650\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 21.2340 - f1: 0.3420 - val_loss: 17.2459 - val_f1: 0.4525\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 15.4889 - f1: 0.4129 - val_loss: 13.3641 - val_f1: 0.4730\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 12.4039 - f1: 0.4388 - val_loss: 11.0648 - val_f1: 0.4887\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 10.3546 - f1: 0.4755 - val_loss: 9.3431 - val_f1: 0.5307\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 8.8917 - f1: 0.5041 - val_loss: 8.0753 - val_f1: 0.5715\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 7.7622 - f1: 0.5262 - val_loss: 7.0378 - val_f1: 0.6009\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 6.8470 - f1: 0.5493 - val_loss: 6.2200 - val_f1: 0.6268\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 6.1161 - f1: 0.5702 - val_loss: 5.5692 - val_f1: 0.6380\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 5.4960 - f1: 0.5863 - val_loss: 5.0713 - val_f1: 0.6422\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 5.0150 - f1: 0.5967 - val_loss: 4.5909 - val_f1: 0.6567\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 4.6066 - f1: 0.6081 - val_loss: 4.2145 - val_f1: 0.6666\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 4.2003 - f1: 0.6240 - val_loss: 3.9957 - val_f1: 0.6483\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 3.8691 - f1: 0.6374 - val_loss: 3.6307 - val_f1: 0.6816\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 3.6435 - f1: 0.6322 - val_loss: 3.3872 - val_f1: 0.6817\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 3.3345 - f1: 0.6568 - val_loss: 3.1622 - val_f1: 0.6861\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 3.1644 - f1: 0.6532 - val_loss: 3.0293 - val_f1: 0.6719\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.9702 - f1: 0.6654 - val_loss: 2.7556 - val_f1: 0.7161\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.7722 - f1: 0.6786 - val_loss: 2.5909 - val_f1: 0.7220\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 2.6364 - f1: 0.6809 - val_loss: 2.5464 - val_f1: 0.7090\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.5037 - f1: 0.6842 - val_loss: 2.3377 - val_f1: 0.7387\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.3758 - f1: 0.6959 - val_loss: 2.2154 - val_f1: 0.7296\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.2585 - f1: 0.7036 - val_loss: 2.1106 - val_f1: 0.7430\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.1812 - f1: 0.7073 - val_loss: 2.1029 - val_f1: 0.7172\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.1004 - f1: 0.7068 - val_loss: 2.0094 - val_f1: 0.7225\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.0390 - f1: 0.6994 - val_loss: 1.9636 - val_f1: 0.7293\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.9431 - f1: 0.7192 - val_loss: 1.8063 - val_f1: 0.7643\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.8527 - f1: 0.7288 - val_loss: 1.7321 - val_f1: 0.7692\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.8231 - f1: 0.7212 - val_loss: 1.7342 - val_f1: 0.7512\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.7488 - f1: 0.7321 - val_loss: 1.6484 - val_f1: 0.7677\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.6880 - f1: 0.7376 - val_loss: 1.5830 - val_f1: 0.7755\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.6640 - f1: 0.7295 - val_loss: 1.5209 - val_f1: 0.7837\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.6060 - f1: 0.7428 - val_loss: 1.5549 - val_f1: 0.7497\n",
      "Epoch 34/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.5410 - f1: 0.7522 - val_loss: 1.4455 - val_f1: 0.7902\n",
      "Epoch 35/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.5474 - f1: 0.7430 - val_loss: 1.4458 - val_f1: 0.7803\n",
      "Epoch 36/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.5170 - f1: 0.7407 - val_loss: 1.5346 - val_f1: 0.7449\n",
      "Epoch 37/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.4508 - f1: 0.7520 - val_loss: 1.3900 - val_f1: 0.7707\n",
      "Epoch 38/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.4153 - f1: 0.7595 - val_loss: 1.3970 - val_f1: 0.7648\n",
      "Epoch 39/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.4224 - f1: 0.7493 - val_loss: 1.4257 - val_f1: 0.7589\n",
      "Epoch 40/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3722 - f1: 0.7634 - val_loss: 1.3176 - val_f1: 0.7844\n",
      "Epoch 41/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3263 - f1: 0.7682 - val_loss: 1.3143 - val_f1: 0.7761\n",
      "Epoch 42/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.3060 - f1: 0.7701 - val_loss: 1.3177 - val_f1: 0.7725\n",
      "Epoch 43/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2837 - f1: 0.7716 - val_loss: 1.3613 - val_f1: 0.7550\n",
      "Epoch 44/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3141 - f1: 0.7517 - val_loss: 1.1877 - val_f1: 0.7981\n",
      "Running through fold 3\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 3s 257us/sample - loss: 36.9668 - f1: 0.1556 - val_loss: 25.6205 - val_f1: 0.3426\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 21.5986 - f1: 0.3477 - val_loss: 17.4076 - val_f1: 0.4394\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 15.7235 - f1: 0.4063 - val_loss: 13.7668 - val_f1: 0.4779\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 12.6092 - f1: 0.4437 - val_loss: 11.1822 - val_f1: 0.5082\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 10.5878 - f1: 0.4656 - val_loss: 9.4657 - val_f1: 0.5455\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 9.0754 - f1: 0.4979 - val_loss: 8.2482 - val_f1: 0.5786\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 7.9963 - f1: 0.5179 - val_loss: 7.2588 - val_f1: 0.5961\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 7.0683 - f1: 0.5485 - val_loss: 6.4300 - val_f1: 0.6264\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 6.2940 - f1: 0.5713 - val_loss: 5.9355 - val_f1: 0.5917\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 5.6965 - f1: 0.5907 - val_loss: 5.3275 - val_f1: 0.6276\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 5.1878 - f1: 0.6008 - val_loss: 4.7960 - val_f1: 0.6456\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 4.7326 - f1: 0.6179 - val_loss: 4.3761 - val_f1: 0.6493\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 4.3478 - f1: 0.6218 - val_loss: 4.0201 - val_f1: 0.6766\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 4.0057 - f1: 0.6376 - val_loss: 3.7633 - val_f1: 0.6851\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 3.7547 - f1: 0.6412 - val_loss: 3.5406 - val_f1: 0.6605\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 3.5040 - f1: 0.6454 - val_loss: 3.1951 - val_f1: 0.7168\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 3.2668 - f1: 0.6562 - val_loss: 3.0869 - val_f1: 0.6997\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 3.0770 - f1: 0.6600 - val_loss: 2.8613 - val_f1: 0.6957\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.8925 - f1: 0.6729 - val_loss: 2.7391 - val_f1: 0.6925\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.7267 - f1: 0.6821 - val_loss: 2.6219 - val_f1: 0.7101\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 2.6225 - f1: 0.6775 - val_loss: 2.4215 - val_f1: 0.7220\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.4795 - f1: 0.6897 - val_loss: 2.3666 - val_f1: 0.7114\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.3516 - f1: 0.6991 - val_loss: 2.2519 - val_f1: 0.7145\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.2419 - f1: 0.7031 - val_loss: 2.1542 - val_f1: 0.7266\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.1725 - f1: 0.7038 - val_loss: 2.0956 - val_f1: 0.7177\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.0552 - f1: 0.7166 - val_loss: 1.9800 - val_f1: 0.7316\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.9883 - f1: 0.7123 - val_loss: 1.8423 - val_f1: 0.7624\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.9389 - f1: 0.7197 - val_loss: 1.8921 - val_f1: 0.7108\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.8645 - f1: 0.7225 - val_loss: 1.9156 - val_f1: 0.7087\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.8039 - f1: 0.7264 - val_loss: 1.7232 - val_f1: 0.7479\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.7715 - f1: 0.7228 - val_loss: 1.6814 - val_f1: 0.7523\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.7133 - f1: 0.7263 - val_loss: 1.6279 - val_f1: 0.7585\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.6441 - f1: 0.7433 - val_loss: 1.5319 - val_f1: 0.7737\n",
      "Epoch 34/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.6388 - f1: 0.7344 - val_loss: 1.5088 - val_f1: 0.7782\n",
      "Epoch 35/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.5668 - f1: 0.7485 - val_loss: 1.6718 - val_f1: 0.7201\n",
      "Epoch 36/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.5370 - f1: 0.7445 - val_loss: 1.5177 - val_f1: 0.7523\n",
      "Epoch 37/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.4815 - f1: 0.7561 - val_loss: 1.3994 - val_f1: 0.7828\n",
      "Epoch 38/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.5030 - f1: 0.7434 - val_loss: 1.4036 - val_f1: 0.7620\n",
      "Epoch 39/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.4436 - f1: 0.7573 - val_loss: 1.3876 - val_f1: 0.7812\n",
      "Epoch 40/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.4086 - f1: 0.7610 - val_loss: 1.3969 - val_f1: 0.7707\n",
      "Epoch 41/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3797 - f1: 0.7623 - val_loss: 1.3186 - val_f1: 0.7790\n",
      "Epoch 42/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.3536 - f1: 0.7655 - val_loss: 1.2859 - val_f1: 0.7915\n",
      "Epoch 43/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.3131 - f1: 0.7712 - val_loss: 1.3079 - val_f1: 0.7835\n",
      "Epoch 44/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.3045 - f1: 0.7655 - val_loss: 1.3346 - val_f1: 0.7653\n",
      "Epoch 45/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.2696 - f1: 0.7674 - val_loss: 1.2741 - val_f1: 0.7877\n",
      "Epoch 46/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.2855 - f1: 0.7615 - val_loss: 1.3107 - val_f1: 0.7605\n",
      "Epoch 47/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2471 - f1: 0.7740 - val_loss: 1.2997 - val_f1: 0.7543\n",
      "Epoch 48/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.2409 - f1: 0.7688 - val_loss: 1.1930 - val_f1: 0.7901\n",
      "Epoch 49/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.2001 - f1: 0.7795 - val_loss: 1.1286 - val_f1: 0.8051\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.2038 - f1: 0.7727 - val_loss: 1.1704 - val_f1: 0.7923\n",
      "Epoch 51/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.1725 - f1: 0.7843 - val_loss: 1.0931 - val_f1: 0.8116\n",
      "Epoch 52/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.1659 - f1: 0.7802 - val_loss: 1.1477 - val_f1: 0.7840\n",
      "Epoch 53/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 1.1777 - f1: 0.7769 - val_loss: 1.1146 - val_f1: 0.7939\n",
      "Epoch 54/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.1519 - f1: 0.7767 - val_loss: 1.1724 - val_f1: 0.7737\n",
      "Epoch 55/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.1234 - f1: 0.7854 - val_loss: 1.0784 - val_f1: 0.8030\n",
      "Epoch 56/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.0928 - f1: 0.7914 - val_loss: 1.0779 - val_f1: 0.8063\n",
      "Epoch 57/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.0973 - f1: 0.7915 - val_loss: 1.1231 - val_f1: 0.7901\n",
      "Epoch 58/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 1.1038 - f1: 0.7885 - val_loss: 1.0560 - val_f1: 0.8109\n",
      "Epoch 59/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.0827 - f1: 0.7894 - val_loss: 1.0357 - val_f1: 0.8083\n",
      "Running through fold 4\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 2s 243us/sample - loss: 36.3580 - f1: 0.1873 - val_loss: 26.6727 - val_f1: 0.3496\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 22.1612 - f1: 0.3486 - val_loss: 18.1188 - val_f1: 0.4563\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 16.3869 - f1: 0.4282 - val_loss: 14.0889 - val_f1: 0.5269\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 13.1737 - f1: 0.4639 - val_loss: 11.8189 - val_f1: 0.5240\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 11.0475 - f1: 0.4962 - val_loss: 9.9063 - val_f1: 0.5592\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 9.4846 - f1: 0.5188 - val_loss: 8.5207 - val_f1: 0.5904\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 8.2725 - f1: 0.5434 - val_loss: 7.6871 - val_f1: 0.5659\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 7.3143 - f1: 0.5650 - val_loss: 6.8109 - val_f1: 0.6112\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 6.5289 - f1: 0.5789 - val_loss: 6.0243 - val_f1: 0.6324\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 5.8666 - f1: 0.5954 - val_loss: 5.4531 - val_f1: 0.6483\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 5.3232 - f1: 0.6103 - val_loss: 5.0055 - val_f1: 0.6225\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 4.8804 - f1: 0.6148 - val_loss: 4.5868 - val_f1: 0.6443\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 4.4477 - f1: 0.6328 - val_loss: 4.0980 - val_f1: 0.6840\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 4.1294 - f1: 0.6372 - val_loss: 3.8642 - val_f1: 0.6647\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 3.8578 - f1: 0.6363 - val_loss: 3.5636 - val_f1: 0.6839\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 3.5395 - f1: 0.6609 - val_loss: 3.3136 - val_f1: 0.6942\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 3.3132 - f1: 0.6686 - val_loss: 3.1743 - val_f1: 0.6892\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 3.1051 - f1: 0.6764 - val_loss: 3.0029 - val_f1: 0.6866\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.9224 - f1: 0.6807 - val_loss: 2.7821 - val_f1: 0.7119\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.7794 - f1: 0.6890 - val_loss: 2.6173 - val_f1: 0.7318\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.6095 - f1: 0.7015 - val_loss: 2.5249 - val_f1: 0.7191\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.5185 - f1: 0.6924 - val_loss: 2.3813 - val_f1: 0.7260\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.3850 - f1: 0.7043 - val_loss: 2.2835 - val_f1: 0.7294\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.2570 - f1: 0.7149 - val_loss: 2.1134 - val_f1: 0.7502\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.1965 - f1: 0.7097 - val_loss: 2.1251 - val_f1: 0.7219\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.1053 - f1: 0.7114 - val_loss: 2.0419 - val_f1: 0.7309\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 2.0195 - f1: 0.7220 - val_loss: 1.9143 - val_f1: 0.7498\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.9186 - f1: 0.7375 - val_loss: 1.8365 - val_f1: 0.7514\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.8537 - f1: 0.7357 - val_loss: 1.8399 - val_f1: 0.7356\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.8080 - f1: 0.7348 - val_loss: 1.7696 - val_f1: 0.7337\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.7352 - f1: 0.7462 - val_loss: 1.7222 - val_f1: 0.7401\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 1.7270 - f1: 0.7365 - val_loss: 1.6459 - val_f1: 0.7613\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.6554 - f1: 0.7475 - val_loss: 1.5549 - val_f1: 0.7678\n",
      "Epoch 34/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.6312 - f1: 0.7467 - val_loss: 1.6119 - val_f1: 0.7480\n",
      "Epoch 35/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.5760 - f1: 0.7543 - val_loss: 1.5130 - val_f1: 0.7735\n",
      "Epoch 36/2000\n",
      "10000/10000 [==============================] - 1s 147us/sample - loss: 1.5157 - f1: 0.7599 - val_loss: 1.4445 - val_f1: 0.7806\n",
      "Epoch 37/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.5085 - f1: 0.7549 - val_loss: 1.6266 - val_f1: 0.7367\n",
      "Epoch 38/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.4943 - f1: 0.7539 - val_loss: 1.4281 - val_f1: 0.7797\n",
      "Epoch 39/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.4357 - f1: 0.7637 - val_loss: 1.4700 - val_f1: 0.7600\n",
      "Epoch 40/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.4333 - f1: 0.7596 - val_loss: 1.4009 - val_f1: 0.7701\n",
      "Epoch 41/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3829 - f1: 0.7663 - val_loss: 1.3332 - val_f1: 0.7852\n",
      "Epoch 42/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.3289 - f1: 0.7760 - val_loss: 1.2701 - val_f1: 0.7960\n",
      "Epoch 43/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3374 - f1: 0.7723 - val_loss: 1.3822 - val_f1: 0.7679\n",
      "Epoch 44/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 1.3068 - f1: 0.7751 - val_loss: 1.2821 - val_f1: 0.7762\n",
      "Epoch 45/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2934 - f1: 0.7745 - val_loss: 1.2247 - val_f1: 0.7963\n",
      "Epoch 46/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2633 - f1: 0.7780 - val_loss: 1.2267 - val_f1: 0.7942\n",
      "Epoch 47/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2380 - f1: 0.7832 - val_loss: 1.1602 - val_f1: 0.8133\n",
      "Epoch 48/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2164 - f1: 0.7861 - val_loss: 1.2228 - val_f1: 0.7838\n",
      "Epoch 49/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2233 - f1: 0.7815 - val_loss: 1.1588 - val_f1: 0.7967\n",
      "Epoch 50/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.2124 - f1: 0.7811 - val_loss: 1.1749 - val_f1: 0.8003\n",
      "Epoch 51/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2096 - f1: 0.7782 - val_loss: 1.1804 - val_f1: 0.7893\n",
      "Epoch 52/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.2197 - f1: 0.7757 - val_loss: 1.1682 - val_f1: 0.7949\n",
      "Epoch 53/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.1671 - f1: 0.7842 - val_loss: 1.0717 - val_f1: 0.8248\n",
      "Epoch 54/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.1505 - f1: 0.7873 - val_loss: 1.2102 - val_f1: 0.7856\n",
      "Epoch 55/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.1030 - f1: 0.8023 - val_loss: 1.0340 - val_f1: 0.8233\n",
      "Epoch 56/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.0988 - f1: 0.7989 - val_loss: 1.1172 - val_f1: 0.7994\n",
      "Epoch 57/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.1127 - f1: 0.7900 - val_loss: 1.1993 - val_f1: 0.7611\n",
      "Epoch 58/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.1142 - f1: 0.7881 - val_loss: 1.0302 - val_f1: 0.8205\n",
      "Epoch 59/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.0746 - f1: 0.7995 - val_loss: 1.1262 - val_f1: 0.7912\n",
      "Epoch 60/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.0793 - f1: 0.7938 - val_loss: 1.1704 - val_f1: 0.7882\n",
      "Epoch 61/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.0826 - f1: 0.7894 - val_loss: 1.0114 - val_f1: 0.8289\n",
      "Epoch 62/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 1.0320 - f1: 0.8093 - val_loss: 1.0270 - val_f1: 0.8091\n",
      "Epoch 63/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 1.0746 - f1: 0.7890 - val_loss: 1.0597 - val_f1: 0.7914\n",
      "\n",
      "\n",
      "Running through training size 15000\n",
      "Running through fold 0\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 211us/sample - loss: 32.2240 - f1: 0.2253 - val_loss: 20.9691 - val_f1: 0.3868\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 17.2317 - f1: 0.3924 - val_loss: 13.9027 - val_f1: 0.4828\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 143us/sample - loss: 12.2747 - f1: 0.4583 - val_loss: 10.4960 - val_f1: 0.5495\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 144us/sample - loss: 9.5774 - f1: 0.5067 - val_loss: 8.4731 - val_f1: 0.5545\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 7.7988 - f1: 0.5396 - val_loss: 6.9620 - val_f1: 0.5929\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 6.5345 - f1: 0.5677 - val_loss: 6.0654 - val_f1: 0.5844\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 5.5768 - f1: 0.5877 - val_loss: 5.1683 - val_f1: 0.5954\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 4.8188 - f1: 0.6118 - val_loss: 4.3269 - val_f1: 0.6747\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 4.2404 - f1: 0.6291 - val_loss: 3.8962 - val_f1: 0.6671\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.7612 - f1: 0.6401 - val_loss: 3.4447 - val_f1: 0.6766\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.3729 - f1: 0.6540 - val_loss: 3.0910 - val_f1: 0.6889\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 3.0553 - f1: 0.6637 - val_loss: 2.9278 - val_f1: 0.6682\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 2.7969 - f1: 0.6718 - val_loss: 2.6141 - val_f1: 0.7017\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 2.5806 - f1: 0.6797 - val_loss: 2.3942 - val_f1: 0.7151\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 2.4026 - f1: 0.6874 - val_loss: 2.3249 - val_f1: 0.7089\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.2524 - f1: 0.6940 - val_loss: 2.1128 - val_f1: 0.7325\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.0925 - f1: 0.7071 - val_loss: 1.9787 - val_f1: 0.7338\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.0059 - f1: 0.7039 - val_loss: 1.8747 - val_f1: 0.7546\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.8954 - f1: 0.7193 - val_loss: 1.7207 - val_f1: 0.7656\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.8039 - f1: 0.7276 - val_loss: 1.8199 - val_f1: 0.7121\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.7256 - f1: 0.7293 - val_loss: 1.6113 - val_f1: 0.7596\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.6655 - f1: 0.7362 - val_loss: 1.5499 - val_f1: 0.7736\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.5988 - f1: 0.7391 - val_loss: 1.6621 - val_f1: 0.7184\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.5744 - f1: 0.7312 - val_loss: 1.5344 - val_f1: 0.7611\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.5012 - f1: 0.7472 - val_loss: 1.4166 - val_f1: 0.7711\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4377 - f1: 0.7517 - val_loss: 1.3940 - val_f1: 0.7685\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4043 - f1: 0.7581 - val_loss: 1.4618 - val_f1: 0.7499\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.3725 - f1: 0.7558 - val_loss: 1.2560 - val_f1: 0.8001\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.3391 - f1: 0.7591 - val_loss: 1.2291 - val_f1: 0.7936\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2870 - f1: 0.7675 - val_loss: 1.2584 - val_f1: 0.7807\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.2632 - f1: 0.7744 - val_loss: 1.2107 - val_f1: 0.7971\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2848 - f1: 0.7635 - val_loss: 1.2369 - val_f1: 0.7866\n",
      "Epoch 33/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.2350 - f1: 0.7659 - val_loss: 1.1144 - val_f1: 0.8117\n",
      "Epoch 34/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.2126 - f1: 0.7747 - val_loss: 1.2294 - val_f1: 0.7684\n",
      "Epoch 35/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.1913 - f1: 0.7771 - val_loss: 1.1435 - val_f1: 0.7841\n",
      "Epoch 36/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1450 - f1: 0.7880 - val_loss: 1.0993 - val_f1: 0.7999\n",
      "Epoch 37/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1369 - f1: 0.7852 - val_loss: 1.1291 - val_f1: 0.7830\n",
      "Epoch 38/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1131 - f1: 0.7883 - val_loss: 1.0813 - val_f1: 0.8028\n",
      "Epoch 39/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1351 - f1: 0.7765 - val_loss: 1.0681 - val_f1: 0.7932\n",
      "Epoch 40/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.0893 - f1: 0.7877 - val_loss: 1.0686 - val_f1: 0.8024\n",
      "Epoch 41/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.0992 - f1: 0.7825 - val_loss: 1.0356 - val_f1: 0.8105\n",
      "Epoch 42/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.0721 - f1: 0.7896 - val_loss: 0.9722 - val_f1: 0.8293\n",
      "Epoch 43/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 1.0355 - f1: 0.7943 - val_loss: 0.9835 - val_f1: 0.8166\n",
      "Epoch 44/2000\n",
      "15000/15000 [==============================] - 2s 143us/sample - loss: 1.0360 - f1: 0.7941 - val_loss: 1.0211 - val_f1: 0.7945\n",
      "Epoch 45/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 1.0168 - f1: 0.8034 - val_loss: 0.9411 - val_f1: 0.8247\n",
      "Epoch 46/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 1.0261 - f1: 0.7963 - val_loss: 0.9902 - val_f1: 0.8126\n",
      "Epoch 47/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.9987 - f1: 0.8015 - val_loss: 0.9747 - val_f1: 0.8072\n",
      "Epoch 48/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 0.9926 - f1: 0.8006 - val_loss: 0.9712 - val_f1: 0.8182\n",
      "Epoch 49/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.9899 - f1: 0.8028 - val_loss: 0.8925 - val_f1: 0.8373\n",
      "Epoch 50/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.9641 - f1: 0.8067 - val_loss: 0.9160 - val_f1: 0.8203\n",
      "Epoch 51/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.9736 - f1: 0.8058 - val_loss: 0.9173 - val_f1: 0.8278\n",
      "Epoch 52/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.9432 - f1: 0.8118 - val_loss: 0.9231 - val_f1: 0.8252\n",
      "Running through fold 1\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 213us/sample - loss: 31.7129 - f1: 0.2283 - val_loss: 20.4646 - val_f1: 0.3914\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 16.7008 - f1: 0.3886 - val_loss: 13.5620 - val_f1: 0.4487\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 11.8379 - f1: 0.4580 - val_loss: 10.0044 - val_f1: 0.5300\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 9.2038 - f1: 0.4993 - val_loss: 8.0097 - val_f1: 0.5748\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 7.4546 - f1: 0.5417 - val_loss: 6.7302 - val_f1: 0.5740\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 6.2631 - f1: 0.5648 - val_loss: 5.5604 - val_f1: 0.6243\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 5.3475 - f1: 0.5863 - val_loss: 4.7619 - val_f1: 0.6512\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 4.6474 - f1: 0.6062 - val_loss: 4.1454 - val_f1: 0.6847\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 4.0717 - f1: 0.6333 - val_loss: 3.6918 - val_f1: 0.6854\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.6196 - f1: 0.6462 - val_loss: 3.2809 - val_f1: 0.7082\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.2421 - f1: 0.6596 - val_loss: 2.9428 - val_f1: 0.7128\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.9639 - f1: 0.6663 - val_loss: 2.7095 - val_f1: 0.7110\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.7203 - f1: 0.6722 - val_loss: 2.4886 - val_f1: 0.7213\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.5252 - f1: 0.6856 - val_loss: 2.3695 - val_f1: 0.7228\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.3185 - f1: 0.7008 - val_loss: 2.1301 - val_f1: 0.7388\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.1909 - f1: 0.7022 - val_loss: 2.0686 - val_f1: 0.7296\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.0691 - f1: 0.7084 - val_loss: 1.9583 - val_f1: 0.7360\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.9384 - f1: 0.7227 - val_loss: 1.9036 - val_f1: 0.7311\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.8488 - f1: 0.7222 - val_loss: 1.7706 - val_f1: 0.7459\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.7608 - f1: 0.7271 - val_loss: 1.6282 - val_f1: 0.7749\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.6834 - f1: 0.7388 - val_loss: 1.6600 - val_f1: 0.7477\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.5983 - f1: 0.7465 - val_loss: 1.4650 - val_f1: 0.7931\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.5638 - f1: 0.7402 - val_loss: 1.4190 - val_f1: 0.7930\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4755 - f1: 0.7560 - val_loss: 1.4326 - val_f1: 0.7775\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4499 - f1: 0.7525 - val_loss: 1.4186 - val_f1: 0.7536\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.4048 - f1: 0.7551 - val_loss: 1.3710 - val_f1: 0.7721\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.3630 - f1: 0.7572 - val_loss: 1.2958 - val_f1: 0.7892\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 1.3222 - f1: 0.7637 - val_loss: 1.3461 - val_f1: 0.7590\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.3310 - f1: 0.7566 - val_loss: 1.2167 - val_f1: 0.7937\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.2636 - f1: 0.7702 - val_loss: 1.1740 - val_f1: 0.8012\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 146us/sample - loss: 1.2322 - f1: 0.7754 - val_loss: 1.1949 - val_f1: 0.7911\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.1974 - f1: 0.7817 - val_loss: 1.1538 - val_f1: 0.7962\n",
      "Running through fold 2\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 214us/sample - loss: 32.2881 - f1: 0.2353 - val_loss: 20.7086 - val_f1: 0.4323\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 17.3418 - f1: 0.3872 - val_loss: 13.9444 - val_f1: 0.4695\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 12.3737 - f1: 0.4517 - val_loss: 10.5485 - val_f1: 0.5334\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 9.6516 - f1: 0.5050 - val_loss: 8.4400 - val_f1: 0.5852\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 7.9101 - f1: 0.5397 - val_loss: 7.0922 - val_f1: 0.5746\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 6.6377 - f1: 0.5651 - val_loss: 6.0215 - val_f1: 0.6119\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 5.6679 - f1: 0.5931 - val_loss: 5.0958 - val_f1: 0.6480\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 4.9328 - f1: 0.6122 - val_loss: 4.5758 - val_f1: 0.6187\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 4.3473 - f1: 0.6265 - val_loss: 3.9768 - val_f1: 0.6689\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.8866 - f1: 0.6435 - val_loss: 3.4418 - val_f1: 0.7145\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.4712 - f1: 0.6545 - val_loss: 3.1930 - val_f1: 0.6913\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.1415 - f1: 0.6704 - val_loss: 2.9514 - val_f1: 0.6926\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.8961 - f1: 0.6709 - val_loss: 2.7170 - val_f1: 0.7100\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.6658 - f1: 0.6816 - val_loss: 2.4844 - val_f1: 0.7149\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.4436 - f1: 0.7029 - val_loss: 2.3826 - val_f1: 0.7212\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.3119 - f1: 0.7045 - val_loss: 2.1681 - val_f1: 0.7279\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.1643 - f1: 0.7109 - val_loss: 2.0737 - val_f1: 0.7242\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.0445 - f1: 0.7133 - val_loss: 1.9504 - val_f1: 0.7413\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.9559 - f1: 0.7141 - val_loss: 1.9069 - val_f1: 0.7249\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.8426 - f1: 0.7275 - val_loss: 1.7599 - val_f1: 0.7464\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.7746 - f1: 0.7304 - val_loss: 1.6624 - val_f1: 0.7595\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.7174 - f1: 0.7261 - val_loss: 1.6072 - val_f1: 0.7543\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.6442 - f1: 0.7331 - val_loss: 1.5685 - val_f1: 0.7618\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.5875 - f1: 0.7381 - val_loss: 1.5632 - val_f1: 0.7489\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.5310 - f1: 0.7442 - val_loss: 1.4853 - val_f1: 0.7600\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4685 - f1: 0.7531 - val_loss: 1.4479 - val_f1: 0.7639\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4240 - f1: 0.7586 - val_loss: 1.3036 - val_f1: 0.7911\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.3736 - f1: 0.7626 - val_loss: 1.3537 - val_f1: 0.7579\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.3605 - f1: 0.7596 - val_loss: 1.3706 - val_f1: 0.7573\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.3416 - f1: 0.7553 - val_loss: 1.2551 - val_f1: 0.7838\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2765 - f1: 0.7732 - val_loss: 1.4714 - val_f1: 0.7267\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2827 - f1: 0.7651 - val_loss: 1.2075 - val_f1: 0.7889\n",
      "Epoch 33/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2443 - f1: 0.7705 - val_loss: 1.1524 - val_f1: 0.8004\n",
      "Epoch 34/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2268 - f1: 0.7701 - val_loss: 1.0891 - val_f1: 0.8123\n",
      "Epoch 35/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.1735 - f1: 0.7830 - val_loss: 1.0501 - val_f1: 0.8193\n",
      "Epoch 36/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.1829 - f1: 0.7774 - val_loss: 1.1359 - val_f1: 0.7905\n",
      "Epoch 37/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1530 - f1: 0.7793 - val_loss: 1.0842 - val_f1: 0.8022\n",
      "Epoch 38/2000\n",
      "15000/15000 [==============================] - 2s 145us/sample - loss: 1.1230 - f1: 0.7850 - val_loss: 1.0449 - val_f1: 0.8104\n",
      "Epoch 39/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.1006 - f1: 0.7891 - val_loss: 1.1896 - val_f1: 0.7661\n",
      "Epoch 40/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.1215 - f1: 0.7771 - val_loss: 1.0414 - val_f1: 0.7988\n",
      "Epoch 41/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 1.0721 - f1: 0.7932 - val_loss: 1.0443 - val_f1: 0.7986\n",
      "Epoch 42/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 1.0594 - f1: 0.7929 - val_loss: 1.0125 - val_f1: 0.8107\n",
      "Epoch 43/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 1.0787 - f1: 0.7866 - val_loss: 1.0464 - val_f1: 0.8004\n",
      "Epoch 44/2000\n",
      "15000/15000 [==============================] - 2s 143us/sample - loss: 1.0500 - f1: 0.7944 - val_loss: 1.1322 - val_f1: 0.7731\n",
      "Running through fold 3\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 217us/sample - loss: 32.0636 - f1: 0.2263 - val_loss: 21.0429 - val_f1: 0.3984\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 16.9479 - f1: 0.4098 - val_loss: 13.7168 - val_f1: 0.4973\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 12.0985 - f1: 0.4670 - val_loss: 10.4602 - val_f1: 0.5209\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 9.4277 - f1: 0.5134 - val_loss: 8.2515 - val_f1: 0.5818\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 7.6684 - f1: 0.5464 - val_loss: 6.9058 - val_f1: 0.5871\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 6.4418 - f1: 0.5726 - val_loss: 5.7873 - val_f1: 0.6134\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 5.5095 - f1: 0.5901 - val_loss: 4.9882 - val_f1: 0.6524\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 4.7822 - f1: 0.6128 - val_loss: 4.3068 - val_f1: 0.6704\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 4.1908 - f1: 0.6335 - val_loss: 3.8183 - val_f1: 0.6683\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.7378 - f1: 0.6428 - val_loss: 3.3629 - val_f1: 0.7099\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.3634 - f1: 0.6568 - val_loss: 3.1092 - val_f1: 0.6887\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 3.0506 - f1: 0.6722 - val_loss: 2.9402 - val_f1: 0.6759\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.8164 - f1: 0.6727 - val_loss: 2.6849 - val_f1: 0.6884\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.5764 - f1: 0.6888 - val_loss: 2.3106 - val_f1: 0.7542\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.3952 - f1: 0.6931 - val_loss: 2.2100 - val_f1: 0.7423\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.2251 - f1: 0.7039 - val_loss: 2.0511 - val_f1: 0.7500\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.1129 - f1: 0.7102 - val_loss: 1.9962 - val_f1: 0.7278\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.0094 - f1: 0.7111 - val_loss: 1.8728 - val_f1: 0.7331\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.8530 - f1: 0.7339 - val_loss: 1.7544 - val_f1: 0.7606\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.7986 - f1: 0.7279 - val_loss: 1.7167 - val_f1: 0.7306\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.7153 - f1: 0.7303 - val_loss: 1.6358 - val_f1: 0.7521\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.6503 - f1: 0.7376 - val_loss: 1.5528 - val_f1: 0.7670\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.6006 - f1: 0.7383 - val_loss: 1.4624 - val_f1: 0.7695\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.5281 - f1: 0.7475 - val_loss: 1.5175 - val_f1: 0.7407\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4846 - f1: 0.7503 - val_loss: 1.4229 - val_f1: 0.7618\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.4236 - f1: 0.7592 - val_loss: 1.3430 - val_f1: 0.7863\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 145us/sample - loss: 1.4000 - f1: 0.7568 - val_loss: 1.3719 - val_f1: 0.7632\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.3424 - f1: 0.7663 - val_loss: 1.2672 - val_f1: 0.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.3366 - f1: 0.7646 - val_loss: 1.2385 - val_f1: 0.7890\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2852 - f1: 0.7684 - val_loss: 1.2293 - val_f1: 0.7906\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.2545 - f1: 0.7735 - val_loss: 1.2096 - val_f1: 0.7889\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.2243 - f1: 0.7756 - val_loss: 1.1516 - val_f1: 0.8049\n",
      "Epoch 33/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.2099 - f1: 0.7784 - val_loss: 1.1174 - val_f1: 0.8051\n",
      "Epoch 34/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1798 - f1: 0.7778 - val_loss: 1.2046 - val_f1: 0.7759\n",
      "Epoch 35/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1381 - f1: 0.7890 - val_loss: 1.1471 - val_f1: 0.7856\n",
      "Epoch 36/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1351 - f1: 0.7885 - val_loss: 1.0603 - val_f1: 0.8083\n",
      "Epoch 37/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1236 - f1: 0.7858 - val_loss: 1.0449 - val_f1: 0.8087\n",
      "Epoch 38/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1092 - f1: 0.7906 - val_loss: 1.0352 - val_f1: 0.8047\n",
      "Epoch 39/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.1000 - f1: 0.7854 - val_loss: 0.9548 - val_f1: 0.8422\n",
      "Epoch 40/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.0709 - f1: 0.7929 - val_loss: 1.0581 - val_f1: 0.7928\n",
      "Epoch 41/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0439 - f1: 0.7947 - val_loss: 0.9720 - val_f1: 0.8182\n",
      "Epoch 42/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0175 - f1: 0.8036 - val_loss: 1.0311 - val_f1: 0.8019\n",
      "Epoch 43/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0693 - f1: 0.7852 - val_loss: 1.0951 - val_f1: 0.7635\n",
      "Epoch 44/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0343 - f1: 0.7954 - val_loss: 0.9281 - val_f1: 0.8353\n",
      "Epoch 45/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0078 - f1: 0.8052 - val_loss: 1.0090 - val_f1: 0.8020\n",
      "Epoch 46/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0153 - f1: 0.7963 - val_loss: 0.9032 - val_f1: 0.8338\n",
      "Epoch 47/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.9700 - f1: 0.8108 - val_loss: 0.9790 - val_f1: 0.8188\n",
      "Epoch 48/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.9854 - f1: 0.8034 - val_loss: 0.9102 - val_f1: 0.8321\n",
      "Epoch 49/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.9698 - f1: 0.8042 - val_loss: 0.9240 - val_f1: 0.8242\n",
      "Running through fold 4\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 217us/sample - loss: 32.3995 - f1: 0.2356 - val_loss: 21.6207 - val_f1: 0.3854\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 17.4787 - f1: 0.4103 - val_loss: 14.0933 - val_f1: 0.4623\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 12.4048 - f1: 0.4596 - val_loss: 10.7436 - val_f1: 0.5091\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 9.6544 - f1: 0.5013 - val_loss: 8.5594 - val_f1: 0.5419\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 7.8436 - f1: 0.5379 - val_loss: 7.0207 - val_f1: 0.5898\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 6.5452 - f1: 0.5702 - val_loss: 5.8405 - val_f1: 0.6238\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 5.6066 - f1: 0.5825 - val_loss: 5.0288 - val_f1: 0.6542\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 4.8454 - f1: 0.6163 - val_loss: 4.3605 - val_f1: 0.6763\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 4.2687 - f1: 0.6274 - val_loss: 3.8923 - val_f1: 0.6859\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 143us/sample - loss: 3.7855 - f1: 0.6472 - val_loss: 3.5102 - val_f1: 0.6932\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 145us/sample - loss: 3.4329 - f1: 0.6484 - val_loss: 3.2720 - val_f1: 0.6707\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 3.1219 - f1: 0.6656 - val_loss: 2.8751 - val_f1: 0.7013\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.8583 - f1: 0.6759 - val_loss: 2.8782 - val_f1: 0.6714\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.6549 - f1: 0.6812 - val_loss: 2.6397 - val_f1: 0.6813\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 2.4765 - f1: 0.6910 - val_loss: 2.2239 - val_f1: 0.7408\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.2927 - f1: 0.7057 - val_loss: 2.1974 - val_f1: 0.7211\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.1828 - f1: 0.7094 - val_loss: 2.1424 - val_f1: 0.7112\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 2.0743 - f1: 0.7101 - val_loss: 1.9843 - val_f1: 0.7254\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.9526 - f1: 0.7210 - val_loss: 1.7675 - val_f1: 0.7761\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.8516 - f1: 0.7298 - val_loss: 1.8416 - val_f1: 0.7251\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 1.7812 - f1: 0.7301 - val_loss: 1.6967 - val_f1: 0.7584\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.7144 - f1: 0.7358 - val_loss: 1.7468 - val_f1: 0.7354\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.6548 - f1: 0.7435 - val_loss: 1.5541 - val_f1: 0.7720\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.6167 - f1: 0.7419 - val_loss: 1.5308 - val_f1: 0.7707\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.5261 - f1: 0.7525 - val_loss: 1.4225 - val_f1: 0.7804\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.5021 - f1: 0.7514 - val_loss: 1.4253 - val_f1: 0.7769\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.4383 - f1: 0.7599 - val_loss: 1.5681 - val_f1: 0.7332\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.3969 - f1: 0.7629 - val_loss: 1.3988 - val_f1: 0.7658\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.3768 - f1: 0.7625 - val_loss: 1.2984 - val_f1: 0.7928\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.3465 - f1: 0.7627 - val_loss: 1.2741 - val_f1: 0.7756\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.3106 - f1: 0.7678 - val_loss: 1.2594 - val_f1: 0.7827\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.2708 - f1: 0.7723 - val_loss: 1.2446 - val_f1: 0.7778\n",
      "Epoch 33/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.2495 - f1: 0.7748 - val_loss: 1.2563 - val_f1: 0.7796\n",
      "Epoch 34/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.2369 - f1: 0.7759 - val_loss: 1.2143 - val_f1: 0.7870\n",
      "Epoch 35/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.2064 - f1: 0.7771 - val_loss: 1.2378 - val_f1: 0.7712\n",
      "Epoch 36/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.1560 - f1: 0.7883 - val_loss: 1.1452 - val_f1: 0.7937\n",
      "Epoch 37/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.1642 - f1: 0.7819 - val_loss: 1.1299 - val_f1: 0.7947\n",
      "Epoch 38/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.1677 - f1: 0.7789 - val_loss: 1.0522 - val_f1: 0.8080\n",
      "Epoch 39/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.1285 - f1: 0.7854 - val_loss: 1.0492 - val_f1: 0.8030\n",
      "Epoch 40/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0860 - f1: 0.7969 - val_loss: 1.1092 - val_f1: 0.7917\n",
      "Epoch 41/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.1006 - f1: 0.7872 - val_loss: 1.0963 - val_f1: 0.7935\n",
      "Epoch 42/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.0813 - f1: 0.7916 - val_loss: 1.0232 - val_f1: 0.8138\n",
      "Epoch 43/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 1.0557 - f1: 0.7986 - val_loss: 1.1564 - val_f1: 0.7752\n",
      "Epoch 44/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0596 - f1: 0.7924 - val_loss: 1.0101 - val_f1: 0.8152\n",
      "Epoch 45/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0171 - f1: 0.8060 - val_loss: 0.9995 - val_f1: 0.8085\n",
      "Epoch 46/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0157 - f1: 0.8031 - val_loss: 1.1124 - val_f1: 0.7704\n",
      "Epoch 47/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0119 - f1: 0.7986 - val_loss: 0.9839 - val_f1: 0.8144\n",
      "Epoch 48/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 1.0032 - f1: 0.8018 - val_loss: 0.9862 - val_f1: 0.8122\n",
      "\n",
      "\n",
      "Running through training size 20000\n",
      "Running through fold 0\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 199us/sample - loss: 28.6922 - f1: 0.2583 - val_loss: 17.3257 - val_f1: 0.3862\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 13.9962 - f1: 0.4120 - val_loss: 10.8989 - val_f1: 0.5346\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 9.6478 - f1: 0.4874 - val_loss: 8.0499 - val_f1: 0.5641\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 7.3300 - f1: 0.5338 - val_loss: 6.3101 - val_f1: 0.6017\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 5.8572 - f1: 0.5700 - val_loss: 5.3396 - val_f1: 0.5988\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 4.8364 - f1: 0.5988 - val_loss: 4.2703 - val_f1: 0.6612\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 4.1187 - f1: 0.6194 - val_loss: 3.7280 - val_f1: 0.6563\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 3.5494 - f1: 0.6394 - val_loss: 3.1723 - val_f1: 0.6974\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 3.1224 - f1: 0.6564 - val_loss: 2.8559 - val_f1: 0.6858\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 2.7808 - f1: 0.6708 - val_loss: 2.5405 - val_f1: 0.7103\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 2.5100 - f1: 0.6871 - val_loss: 2.3627 - val_f1: 0.7061\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 2.2944 - f1: 0.6916 - val_loss: 2.1538 - val_f1: 0.7242\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 2.0998 - f1: 0.7092 - val_loss: 1.8572 - val_f1: 0.7685\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.9506 - f1: 0.7165 - val_loss: 1.7835 - val_f1: 0.7619\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.8418 - f1: 0.7190 - val_loss: 1.7305 - val_f1: 0.7447\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.7365 - f1: 0.7292 - val_loss: 1.6656 - val_f1: 0.7404\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.6539 - f1: 0.7304 - val_loss: 1.4568 - val_f1: 0.7880\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.5619 - f1: 0.7446 - val_loss: 1.4660 - val_f1: 0.7581\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.5008 - f1: 0.7457 - val_loss: 1.4305 - val_f1: 0.7575\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.4530 - f1: 0.7455 - val_loss: 1.4405 - val_f1: 0.7550\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.3771 - f1: 0.7589 - val_loss: 1.2700 - val_f1: 0.8016\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.3541 - f1: 0.7572 - val_loss: 1.4657 - val_f1: 0.7183\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.3197 - f1: 0.7554 - val_loss: 1.1841 - val_f1: 0.8015\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.2580 - f1: 0.7690 - val_loss: 1.1768 - val_f1: 0.7961\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.2094 - f1: 0.7781 - val_loss: 1.1133 - val_f1: 0.8103\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.2328 - f1: 0.7619 - val_loss: 1.2329 - val_f1: 0.7514\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.1840 - f1: 0.7721 - val_loss: 1.1545 - val_f1: 0.7868\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.1420 - f1: 0.7790 - val_loss: 1.0859 - val_f1: 0.7908\n",
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.1138 - f1: 0.7813 - val_loss: 1.0540 - val_f1: 0.8060\n",
      "Epoch 30/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.1025 - f1: 0.7811 - val_loss: 1.0067 - val_f1: 0.8144\n",
      "Epoch 31/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0860 - f1: 0.7858 - val_loss: 1.1235 - val_f1: 0.7774\n",
      "Epoch 32/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0641 - f1: 0.7850 - val_loss: 0.9431 - val_f1: 0.8257\n",
      "Epoch 33/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0473 - f1: 0.7898 - val_loss: 0.9758 - val_f1: 0.8142\n",
      "Epoch 34/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0314 - f1: 0.7895 - val_loss: 0.9464 - val_f1: 0.8265\n",
      "Epoch 35/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0112 - f1: 0.7973 - val_loss: 0.9041 - val_f1: 0.8273\n",
      "Epoch 36/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0005 - f1: 0.7972 - val_loss: 0.9880 - val_f1: 0.8004\n",
      "Epoch 37/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9870 - f1: 0.7975 - val_loss: 0.9093 - val_f1: 0.8222\n",
      "Epoch 38/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.9778 - f1: 0.7991 - val_loss: 1.0208 - val_f1: 0.7942\n",
      "Epoch 39/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9515 - f1: 0.8067 - val_loss: 0.8765 - val_f1: 0.8304\n",
      "Epoch 40/2000\n",
      "20000/20000 [==============================] - 3s 142us/sample - loss: 0.9635 - f1: 0.7990 - val_loss: 0.8402 - val_f1: 0.8487\n",
      "Epoch 41/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.9409 - f1: 0.8088 - val_loss: 0.9266 - val_f1: 0.8164\n",
      "Epoch 42/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.9454 - f1: 0.8048 - val_loss: 0.8430 - val_f1: 0.8492\n",
      "Epoch 43/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9245 - f1: 0.8089 - val_loss: 0.8673 - val_f1: 0.8216\n",
      "Epoch 44/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.9108 - f1: 0.8095 - val_loss: 0.7967 - val_f1: 0.8490\n",
      "Epoch 45/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.9259 - f1: 0.8095 - val_loss: 0.8573 - val_f1: 0.8294\n",
      "Epoch 46/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.9112 - f1: 0.8065 - val_loss: 0.8604 - val_f1: 0.8311\n",
      "Epoch 47/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.8925 - f1: 0.8172 - val_loss: 0.8820 - val_f1: 0.8152\n",
      "Epoch 48/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.8694 - f1: 0.8184 - val_loss: 0.8517 - val_f1: 0.8233\n",
      "Epoch 49/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.8820 - f1: 0.8176 - val_loss: 0.8527 - val_f1: 0.8283\n",
      "Epoch 50/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8858 - f1: 0.8182 - val_loss: 0.8543 - val_f1: 0.8233\n",
      "Running through fold 1\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 199us/sample - loss: 29.2043 - f1: 0.2760 - val_loss: 17.9611 - val_f1: 0.4336\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 14.4016 - f1: 0.4394 - val_loss: 11.3821 - val_f1: 0.5395\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 9.9646 - f1: 0.4996 - val_loss: 8.2818 - val_f1: 0.5879\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 7.5340 - f1: 0.5445 - val_loss: 6.4918 - val_f1: 0.6044\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 6.0120 - f1: 0.5769 - val_loss: 5.1792 - val_f1: 0.6537\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 4.9381 - f1: 0.6097 - val_loss: 4.2919 - val_f1: 0.6868\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 4.1785 - f1: 0.6242 - val_loss: 3.6917 - val_f1: 0.6860\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 3.5883 - f1: 0.6440 - val_loss: 3.2410 - val_f1: 0.6793\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 3.1600 - f1: 0.6613 - val_loss: 2.8363 - val_f1: 0.7094\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 2.7831 - f1: 0.6821 - val_loss: 2.6668 - val_f1: 0.6871\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 2.5414 - f1: 0.6821 - val_loss: 2.2699 - val_f1: 0.7427\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 2.3083 - f1: 0.6955 - val_loss: 2.1300 - val_f1: 0.7340\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 2.1136 - f1: 0.7094 - val_loss: 1.9281 - val_f1: 0.7487\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.9612 - f1: 0.7209 - val_loss: 1.8215 - val_f1: 0.7485\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.8527 - f1: 0.7200 - val_loss: 1.6862 - val_f1: 0.7617\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.7291 - f1: 0.7322 - val_loss: 1.6139 - val_f1: 0.7605\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.6397 - f1: 0.7390 - val_loss: 1.5066 - val_f1: 0.7800\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.5732 - f1: 0.7392 - val_loss: 1.4759 - val_f1: 0.7638\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.5078 - f1: 0.7465 - val_loss: 1.3924 - val_f1: 0.7805\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.4234 - f1: 0.7588 - val_loss: 1.4412 - val_f1: 0.7487\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.3832 - f1: 0.7586 - val_loss: 1.2879 - val_f1: 0.7886\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.3552 - f1: 0.7550 - val_loss: 1.4250 - val_f1: 0.7410\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.2737 - f1: 0.7706 - val_loss: 1.3087 - val_f1: 0.7668\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.2613 - f1: 0.7717 - val_loss: 1.2560 - val_f1: 0.7691\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 143us/sample - loss: 1.2155 - f1: 0.7772 - val_loss: 1.1572 - val_f1: 0.7971\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.2021 - f1: 0.7699 - val_loss: 1.1201 - val_f1: 0.8044\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.1617 - f1: 0.7785 - val_loss: 1.1870 - val_f1: 0.7700\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.1496 - f1: 0.7800 - val_loss: 1.0893 - val_f1: 0.7903\n",
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.1111 - f1: 0.7875 - val_loss: 1.0337 - val_f1: 0.8016\n",
      "Epoch 30/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0897 - f1: 0.7885 - val_loss: 1.0015 - val_f1: 0.8116\n",
      "Epoch 31/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0718 - f1: 0.7861 - val_loss: 0.9826 - val_f1: 0.8205\n",
      "Epoch 32/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0543 - f1: 0.7927 - val_loss: 0.9828 - val_f1: 0.8162\n",
      "Epoch 33/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0323 - f1: 0.7986 - val_loss: 1.1509 - val_f1: 0.7578\n",
      "Epoch 34/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0208 - f1: 0.8010 - val_loss: 0.9692 - val_f1: 0.8168\n",
      "Epoch 35/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0123 - f1: 0.7963 - val_loss: 1.1246 - val_f1: 0.7773\n",
      "Epoch 36/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9985 - f1: 0.8001 - val_loss: 1.0139 - val_f1: 0.7907\n",
      "Epoch 37/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9781 - f1: 0.8018 - val_loss: 0.8968 - val_f1: 0.8237\n",
      "Epoch 38/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9651 - f1: 0.8080 - val_loss: 0.8736 - val_f1: 0.8352\n",
      "Epoch 39/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9419 - f1: 0.8122 - val_loss: 0.9354 - val_f1: 0.8234\n",
      "Epoch 40/2000\n",
      "20000/20000 [==============================] - 3s 144us/sample - loss: 0.9126 - f1: 0.8181 - val_loss: 0.9472 - val_f1: 0.8042\n",
      "Epoch 41/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.9485 - f1: 0.8057 - val_loss: 0.8366 - val_f1: 0.8400\n",
      "Epoch 42/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9138 - f1: 0.8167 - val_loss: 0.9014 - val_f1: 0.8122\n",
      "Epoch 43/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9188 - f1: 0.8119 - val_loss: 0.9815 - val_f1: 0.7881\n",
      "Epoch 44/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.8892 - f1: 0.8177 - val_loss: 0.8918 - val_f1: 0.8143\n",
      "Epoch 45/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.9000 - f1: 0.8158 - val_loss: 0.8922 - val_f1: 0.8236\n",
      "Epoch 46/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.8943 - f1: 0.8170 - val_loss: 0.8459 - val_f1: 0.8309\n",
      "Epoch 47/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.8668 - f1: 0.8215 - val_loss: 0.7678 - val_f1: 0.8578\n",
      "Epoch 48/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8806 - f1: 0.8175 - val_loss: 0.8375 - val_f1: 0.8378\n",
      "Epoch 49/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8900 - f1: 0.8154 - val_loss: 0.8170 - val_f1: 0.8365\n",
      "Epoch 50/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8503 - f1: 0.8270 - val_loss: 0.7982 - val_f1: 0.8429\n",
      "Epoch 51/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8890 - f1: 0.8136 - val_loss: 0.7891 - val_f1: 0.8537\n",
      "Epoch 52/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.8455 - f1: 0.8304 - val_loss: 0.8075 - val_f1: 0.8422\n",
      "Epoch 53/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.8327 - f1: 0.8299 - val_loss: 0.8224 - val_f1: 0.8344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.8328 - f1: 0.8298 - val_loss: 0.8001 - val_f1: 0.8389\n",
      "Epoch 55/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8375 - f1: 0.8269 - val_loss: 0.8020 - val_f1: 0.8362\n",
      "Epoch 56/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8379 - f1: 0.8261 - val_loss: 0.7667 - val_f1: 0.8467\n",
      "Epoch 57/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8139 - f1: 0.8336 - val_loss: 0.8745 - val_f1: 0.8247\n",
      "Running through fold 2\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 201us/sample - loss: 29.0458 - f1: 0.2627 - val_loss: 17.4453 - val_f1: 0.4747\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 14.2454 - f1: 0.4297 - val_loss: 11.3231 - val_f1: 0.5020\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 9.8031 - f1: 0.4954 - val_loss: 8.2260 - val_f1: 0.5538\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 7.4318 - f1: 0.5410 - val_loss: 6.5159 - val_f1: 0.5750\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 5.9084 - f1: 0.5764 - val_loss: 5.0985 - val_f1: 0.6538\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 4.8712 - f1: 0.5989 - val_loss: 4.4026 - val_f1: 0.6229\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 4.1054 - f1: 0.6250 - val_loss: 3.6497 - val_f1: 0.6775\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 3.5425 - f1: 0.6446 - val_loss: 3.1038 - val_f1: 0.7122\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 3.1008 - f1: 0.6626 - val_loss: 2.7167 - val_f1: 0.7388\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 2.7526 - f1: 0.6751 - val_loss: 2.5279 - val_f1: 0.7050\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 2.4724 - f1: 0.6903 - val_loss: 2.1869 - val_f1: 0.7558\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 2.2770 - f1: 0.6952 - val_loss: 2.0063 - val_f1: 0.7598\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 2.0779 - f1: 0.7095 - val_loss: 2.0046 - val_f1: 0.7081\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.9386 - f1: 0.7164 - val_loss: 1.8426 - val_f1: 0.7272\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.8051 - f1: 0.7248 - val_loss: 1.6824 - val_f1: 0.7513\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.7060 - f1: 0.7285 - val_loss: 1.6238 - val_f1: 0.7544\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.6149 - f1: 0.7378 - val_loss: 1.4854 - val_f1: 0.7778\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.5355 - f1: 0.7454 - val_loss: 1.4768 - val_f1: 0.7557\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.4943 - f1: 0.7412 - val_loss: 1.3685 - val_f1: 0.7751\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.4139 - f1: 0.7528 - val_loss: 1.4097 - val_f1: 0.7431\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.3580 - f1: 0.7554 - val_loss: 1.2986 - val_f1: 0.7772\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.3146 - f1: 0.7624 - val_loss: 1.2259 - val_f1: 0.7901\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.2722 - f1: 0.7650 - val_loss: 1.2227 - val_f1: 0.7857\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.2292 - f1: 0.7731 - val_loss: 1.2538 - val_f1: 0.7585\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.2137 - f1: 0.7692 - val_loss: 1.0667 - val_f1: 0.8184\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.1729 - f1: 0.7771 - val_loss: 1.1615 - val_f1: 0.7765\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 3s 144us/sample - loss: 1.1452 - f1: 0.7795 - val_loss: 1.0742 - val_f1: 0.8030\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.1295 - f1: 0.7774 - val_loss: 1.1742 - val_f1: 0.7528\n",
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0957 - f1: 0.7875 - val_loss: 1.0128 - val_f1: 0.8165\n",
      "Epoch 30/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0797 - f1: 0.7865 - val_loss: 1.1143 - val_f1: 0.7860\n",
      "Epoch 31/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0381 - f1: 0.7960 - val_loss: 0.9796 - val_f1: 0.8168\n",
      "Epoch 32/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0642 - f1: 0.7852 - val_loss: 1.0528 - val_f1: 0.7878\n",
      "Epoch 33/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0300 - f1: 0.7929 - val_loss: 0.9167 - val_f1: 0.8333\n",
      "Epoch 34/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9859 - f1: 0.8082 - val_loss: 1.0073 - val_f1: 0.7986\n",
      "Epoch 35/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9739 - f1: 0.8043 - val_loss: 0.9537 - val_f1: 0.8114\n",
      "Epoch 36/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9886 - f1: 0.7997 - val_loss: 0.9858 - val_f1: 0.8036\n",
      "Epoch 37/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9744 - f1: 0.8022 - val_loss: 0.8625 - val_f1: 0.8362\n",
      "Epoch 38/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 0.9558 - f1: 0.8068 - val_loss: 0.8663 - val_f1: 0.8283\n",
      "Epoch 39/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9345 - f1: 0.8099 - val_loss: 0.8217 - val_f1: 0.8540\n",
      "Epoch 40/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9446 - f1: 0.8075 - val_loss: 0.8690 - val_f1: 0.8343\n",
      "Epoch 41/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8938 - f1: 0.8221 - val_loss: 0.8868 - val_f1: 0.8291\n",
      "Epoch 42/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9261 - f1: 0.8121 - val_loss: 0.8702 - val_f1: 0.8284\n",
      "Epoch 43/2000\n",
      "20000/20000 [==============================] - 3s 144us/sample - loss: 0.9024 - f1: 0.8164 - val_loss: 0.8193 - val_f1: 0.8465\n",
      "Epoch 44/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8952 - f1: 0.8161 - val_loss: 0.8036 - val_f1: 0.8471\n",
      "Epoch 45/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8898 - f1: 0.8162 - val_loss: 0.8274 - val_f1: 0.8366\n",
      "Epoch 46/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8879 - f1: 0.8172 - val_loss: 0.9642 - val_f1: 0.7854\n",
      "Epoch 47/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.8841 - f1: 0.8223 - val_loss: 0.8667 - val_f1: 0.8264\n",
      "Epoch 48/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.8827 - f1: 0.8173 - val_loss: 0.8177 - val_f1: 0.8445\n",
      "Epoch 49/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.8440 - f1: 0.8283 - val_loss: 0.8933 - val_f1: 0.8196\n",
      "Running through fold 3\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 202us/sample - loss: 28.9724 - f1: 0.2594 - val_loss: 17.8972 - val_f1: 0.3992\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 14.3006 - f1: 0.4269 - val_loss: 11.2832 - val_f1: 0.5366\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 9.8833 - f1: 0.5013 - val_loss: 8.4731 - val_f1: 0.5405\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 7.5089 - f1: 0.5402 - val_loss: 6.4070 - val_f1: 0.6135\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 5.9717 - f1: 0.5769 - val_loss: 5.2202 - val_f1: 0.6410\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 4.9293 - f1: 0.6037 - val_loss: 4.3205 - val_f1: 0.6677\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 4.1447 - f1: 0.6246 - val_loss: 3.6697 - val_f1: 0.6810\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 3.6056 - f1: 0.6387 - val_loss: 3.2133 - val_f1: 0.6861\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 3.1307 - f1: 0.6641 - val_loss: 2.8605 - val_f1: 0.6843\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 141us/sample - loss: 2.7960 - f1: 0.6740 - val_loss: 2.5425 - val_f1: 0.7251\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 2.5175 - f1: 0.6862 - val_loss: 2.2452 - val_f1: 0.7299\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 2.2826 - f1: 0.6971 - val_loss: 2.1178 - val_f1: 0.7275\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 2.1125 - f1: 0.7027 - val_loss: 1.9570 - val_f1: 0.7389\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.9770 - f1: 0.7092 - val_loss: 1.7593 - val_f1: 0.7679\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.8424 - f1: 0.7182 - val_loss: 1.6609 - val_f1: 0.7718\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.7259 - f1: 0.7288 - val_loss: 1.5554 - val_f1: 0.7773\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.6464 - f1: 0.7334 - val_loss: 1.5397 - val_f1: 0.7574\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.5583 - f1: 0.7409 - val_loss: 1.4736 - val_f1: 0.7696\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 141us/sample - loss: 1.4892 - f1: 0.7459 - val_loss: 1.4478 - val_f1: 0.7627\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.4285 - f1: 0.7494 - val_loss: 1.3391 - val_f1: 0.7689\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.3988 - f1: 0.7482 - val_loss: 1.3177 - val_f1: 0.7776\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.3210 - f1: 0.7623 - val_loss: 1.2744 - val_f1: 0.7682\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.3160 - f1: 0.7576 - val_loss: 1.2072 - val_f1: 0.7887\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.2356 - f1: 0.7682 - val_loss: 1.1038 - val_f1: 0.8158\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.2114 - f1: 0.7695 - val_loss: 1.1724 - val_f1: 0.7971\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 141us/sample - loss: 1.1937 - f1: 0.7694 - val_loss: 1.1722 - val_f1: 0.7776\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.1660 - f1: 0.7750 - val_loss: 1.0845 - val_f1: 0.8025\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.0960 - f1: 0.7925 - val_loss: 1.0531 - val_f1: 0.8091\n",
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.1355 - f1: 0.7762 - val_loss: 1.0534 - val_f1: 0.8097\n",
      "Epoch 30/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0802 - f1: 0.7906 - val_loss: 1.0853 - val_f1: 0.7871\n",
      "Epoch 31/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 1.0780 - f1: 0.7863 - val_loss: 1.0109 - val_f1: 0.8136\n",
      "Epoch 32/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 1.0456 - f1: 0.7922 - val_loss: 1.0027 - val_f1: 0.8083\n",
      "Epoch 33/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 1.0339 - f1: 0.7952 - val_loss: 0.9530 - val_f1: 0.8216\n",
      "Epoch 34/2000\n",
      "20000/20000 [==============================] - 3s 146us/sample - loss: 1.0239 - f1: 0.7944 - val_loss: 0.9640 - val_f1: 0.8106\n",
      "Running through fold 4\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 209us/sample - loss: 29.2462 - f1: 0.2600 - val_loss: 17.7391 - val_f1: 0.4424\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 14.4455 - f1: 0.4260 - val_loss: 11.4991 - val_f1: 0.4936\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 10.0582 - f1: 0.4843 - val_loss: 8.5426 - val_f1: 0.5480\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 7.6369 - f1: 0.5375 - val_loss: 6.5338 - val_f1: 0.6126\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 6.0856 - f1: 0.5716 - val_loss: 5.3343 - val_f1: 0.6151\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 4.9916 - f1: 0.6037 - val_loss: 4.3821 - val_f1: 0.6591\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 141us/sample - loss: 4.2105 - f1: 0.6229 - val_loss: 3.7499 - val_f1: 0.6789\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 3.6187 - f1: 0.6369 - val_loss: 3.2096 - val_f1: 0.7004\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 3.1679 - f1: 0.6594 - val_loss: 2.8513 - val_f1: 0.6929\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 2.7982 - f1: 0.6727 - val_loss: 2.5279 - val_f1: 0.7113\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 2.5433 - f1: 0.6822 - val_loss: 2.3218 - val_f1: 0.7088\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 2.3152 - f1: 0.6885 - val_loss: 2.1290 - val_f1: 0.7283\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 2.1218 - f1: 0.6991 - val_loss: 2.0097 - val_f1: 0.7140\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.9718 - f1: 0.7083 - val_loss: 1.8358 - val_f1: 0.7385\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.8465 - f1: 0.7180 - val_loss: 1.7681 - val_f1: 0.7435\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.7458 - f1: 0.7253 - val_loss: 1.6093 - val_f1: 0.7598\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 141us/sample - loss: 1.6735 - f1: 0.7254 - val_loss: 1.5692 - val_f1: 0.7502\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.5817 - f1: 0.7370 - val_loss: 1.5005 - val_f1: 0.7596\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.5275 - f1: 0.7369 - val_loss: 1.4365 - val_f1: 0.7546\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.4438 - f1: 0.7452 - val_loss: 1.3717 - val_f1: 0.7574\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.3919 - f1: 0.7507 - val_loss: 1.2815 - val_f1: 0.7724\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.3351 - f1: 0.7564 - val_loss: 1.3029 - val_f1: 0.7511\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.2970 - f1: 0.7585 - val_loss: 1.2207 - val_f1: 0.7887\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.2399 - f1: 0.7730 - val_loss: 1.1339 - val_f1: 0.8091\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.2228 - f1: 0.7661 - val_loss: 1.1639 - val_f1: 0.7859\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.1889 - f1: 0.7695 - val_loss: 1.1532 - val_f1: 0.7833\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.1601 - f1: 0.7785 - val_loss: 1.0417 - val_f1: 0.8155\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.1198 - f1: 0.7847 - val_loss: 1.1057 - val_f1: 0.7931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.1085 - f1: 0.7813 - val_loss: 1.0838 - val_f1: 0.7836\n",
      "Epoch 30/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.0768 - f1: 0.7891 - val_loss: 0.9642 - val_f1: 0.8247\n",
      "Epoch 31/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.0816 - f1: 0.7874 - val_loss: 1.1787 - val_f1: 0.7724\n",
      "Epoch 32/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.0725 - f1: 0.7829 - val_loss: 0.9459 - val_f1: 0.8292\n",
      "Epoch 33/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 1.0236 - f1: 0.7980 - val_loss: 0.9318 - val_f1: 0.8274\n",
      "Epoch 34/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0168 - f1: 0.7968 - val_loss: 0.9926 - val_f1: 0.8080\n",
      "Epoch 35/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 1.0098 - f1: 0.7955 - val_loss: 0.9311 - val_f1: 0.8173\n",
      "Epoch 36/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9917 - f1: 0.7987 - val_loss: 0.8576 - val_f1: 0.8473\n",
      "Epoch 37/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.9692 - f1: 0.8031 - val_loss: 0.9541 - val_f1: 0.8051\n",
      "Epoch 38/2000\n",
      "20000/20000 [==============================] - 3s 143us/sample - loss: 0.9630 - f1: 0.8016 - val_loss: 0.8615 - val_f1: 0.8455\n",
      "Epoch 39/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9659 - f1: 0.8006 - val_loss: 0.9511 - val_f1: 0.8026\n",
      "Epoch 40/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.9573 - f1: 0.8042 - val_loss: 1.0057 - val_f1: 0.7809\n",
      "Epoch 41/2000\n",
      "20000/20000 [==============================] - 3s 138us/sample - loss: 0.9276 - f1: 0.8136 - val_loss: 0.8606 - val_f1: 0.8340\n",
      "Epoch 42/2000\n",
      "20000/20000 [==============================] - 3s 142us/sample - loss: 0.9338 - f1: 0.8063 - val_loss: 0.8605 - val_f1: 0.8388\n",
      "Epoch 43/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.9056 - f1: 0.8141 - val_loss: 0.8530 - val_f1: 0.8331\n",
      "Epoch 44/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.9131 - f1: 0.8113 - val_loss: 0.9374 - val_f1: 0.8084\n",
      "Epoch 45/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.9137 - f1: 0.8078 - val_loss: 0.8075 - val_f1: 0.8449\n",
      "Epoch 46/2000\n",
      "20000/20000 [==============================] - 3s 137us/sample - loss: 0.8983 - f1: 0.8156 - val_loss: 0.8639 - val_f1: 0.8223\n"
     ]
    }
   ],
   "source": [
    "mlb=LabelBinarizer()\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    print('\\n\\nRunning through training size '+str(train_size))\n",
    "    k_folds_errors = []\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, train_size=train_size)\n",
    "    k = 0\n",
    "    for train_index, _ in sss.split(training_spectra, training_keys):\n",
    "        print('Running through fold '+str(k))\n",
    "        training_keys_binarized = mlb.fit_transform(training_keys.reshape([training_keys.shape[0],1]))\n",
    "        testing_keys_binarized = mlb.transform(testing_keys)\n",
    "        \n",
    "        model = compile_model(\n",
    "            build_dnn_model,\n",
    "            model_features)\n",
    "        model_weights = model.get_weights()\n",
    "        model_weights_updated = model_weights[:]\n",
    "        model_weights_updated[0:2] = dae_model.get_weights()[0:2]\n",
    "        model.set_weights(model_weights_updated)\n",
    "\n",
    "        csv_logger = tf.keras.callbacks.CSVLogger('./final-models-keras/'+model_id_save_as+'_trainsize'+str(train_size)+'_fold'+str(k)+'.log')\n",
    "\n",
    "        output = model.fit(\n",
    "            x=training_spectra_scaled[train_index],\n",
    "            y=training_keys_binarized[train_index],\n",
    "            epochs=num_epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(testing_spectra_scaled,\n",
    "                             testing_keys_binarized),\n",
    "            shuffle=True,\n",
    "            callbacks=[earlystop_callback, csv_logger],\n",
    "        )\n",
    "        model.save('./final-models-keras/'+model_id_save_as+'_trainsize'+str(train_size)+'_fold'+str(k)+'.hdf5')\n",
    "        k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36_update)",
   "language": "python",
   "name": "conda_tensorflow_p36_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

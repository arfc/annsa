{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from annsa.template_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model, training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annsa.model_classes import (dnn_model_features,\n",
    "                                 DNN,\n",
    "                                 save_model,\n",
    "                                 train_earlystop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_dataset = pd.read_csv('../../source-interdiction/training_testing_data/background_template_dataset.csv')\n",
    "source_dataset = pd.read_csv('../../source-interdiction/training_testing_data/shielded_templates_200kev_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourcedist: [50.0, 112.5, 175.0, 237.5, 300.0]\n",
      "sourceheight: [50.0, 75.0, 100.0, 125.0, 150.0]\n",
      "alum shieldingdensity: [1.82, 4.18, 7.49, 13.16]\n",
      "iron shieldingdensity: [1.53, 3.5, 6.28, 11.02]\n",
      "lead shieldingdensity: [0.22, 0.51, 0.92, 1.61]\n",
      "fwhm: [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0]\n"
     ]
    }
   ],
   "source": [
    "print('sourcedist: ' + str(sorted(set(source_dataset['sourcedist']))))\n",
    "print('sourceheight: ' + str(sorted(set(source_dataset['sourceheight']))))\n",
    "print('alum shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='alum']['shieldingdensity']))))\n",
    "print('iron shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='iron']['shieldingdensity']))))\n",
    "print('lead shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='lead']['shieldingdensity']))))\n",
    "print('fwhm: ' + str(sorted(set(source_dataset['fwhm']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = source_dataset[(source_dataset['fwhm']==7.0) | \n",
    "                                (source_dataset['fwhm']==7.5) |\n",
    "                                (source_dataset['fwhm']==8.0)]\n",
    "\n",
    "source_dataset = source_dataset[(source_dataset['sourcedist']==50.5) | \n",
    "                                (source_dataset['sourcedist']==175.0) | \n",
    "                                (source_dataset['sourcedist']==300.0)]\n",
    "\n",
    "source_dataset = source_dataset[(source_dataset['sourceheight']==50.0) |\n",
    "                                (source_dataset['sourceheight']==100.0) |\n",
    "                                (source_dataset['sourceheight']==150.0)]\n",
    "\n",
    "# remove 80% shielding\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=13.16]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=11.02]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=1.61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove empty spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicies dropped: [552 553 554 555 556 557 564 565 566 582 583 584 585 586 587 594 595 596\n",
      " 612 613 614 615 616 617 624 625 626 642 643 644 645 646 647 654 655 656\n",
      " 672 673 674 675 676 677 684 685 686 702 703 704 705 706 707 714 715 716]\n"
     ]
    }
   ],
   "source": [
    "zero_count_indicies = np.argwhere(np.sum(source_dataset.values[:,6:],axis=1) == 0).flatten()\n",
    "\n",
    "print('indicies dropped: ' +str(zero_count_indicies))\n",
    "\n",
    "source_dataset.drop(source_dataset.index[zero_count_indicies], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add empty spectra for background "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_spectra = []\n",
    "for fwhm in set(source_dataset['fwhm']):\n",
    "    num_examples = source_dataset[(source_dataset['fwhm']==fwhm) &\n",
    "                                  (source_dataset['isotope']==source_dataset['isotope'].iloc()[0])].shape[0]\n",
    "    for k in range(num_examples):\n",
    "        blank_spectra_tmp = [0]*1200\n",
    "        blank_spectra_tmp[5] = fwhm\n",
    "        blank_spectra_tmp[0] = 'background'\n",
    "        blank_spectra_tmp[3] = 'background'\n",
    "        blank_spectra.append(blank_spectra_tmp)\n",
    "\n",
    "source_dataset = source_dataset.append(pd.DataFrame(blank_spectra,\n",
    "                                                    columns=source_dataset.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset from spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_dataset = source_dataset.values[:,5:].astype('float64')\n",
    "all_keys = source_dataset['isotope'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='DNN-kfoldsfull_20_dae_features'\n",
    "with open('../hyperparameter_search/hyperparameter-search-results/'+model_id,\"rb\" ) as f:\n",
    "        model_features=pickle.load(f)\n",
    "        \n",
    "model_class = DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define online data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration_time():\n",
    "    return 10**np.random.uniform(np.log10(10),np.log10(3600))\n",
    "\n",
    "def background_cps():\n",
    "    return np.random.poisson(200)\n",
    "\n",
    "def signal_to_background():\n",
    "    return np.random.uniform(0.1,2)\n",
    "\n",
    "def calibration():\n",
    "    return [np.random.uniform(0,10),\n",
    "            np.random.uniform(2500/3000,3500/3000),\n",
    "            0]\n",
    "\n",
    "online_data_augmentation = online_data_augmentation_vanilla(background_dataset,\n",
    "                                background_cps,\n",
    "                                integration_time,\n",
    "                                signal_to_background,\n",
    "                                calibration,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create temporary testing dataset based on training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_isotope=10\n",
    "\n",
    "testing_spectra = []\n",
    "testing_keys = []\n",
    "\n",
    "for key in set(all_keys):\n",
    "    for _ in range(examples_per_isotope):\n",
    "        testing_spectra_tmp = source_dataset[source_dataset['isotope']==key].sample().values[:,5:].astype('float64')\n",
    "        testing_spectra_tmp = online_data_augmentation(testing_spectra_tmp).numpy()[0]\n",
    "        testing_keys.append(key)\n",
    "        testing_spectra.append(testing_spectra_tmp)\n",
    "\n",
    "testing_spectra = np.array(testing_spectra)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='DNN_onlinedataaugfull_log10integrationtime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: CostFunc loss: 0.00 0.90, EarlyStop loss: 0.00 0.15\n",
      "Epoch 20: CostFunc loss: 0.00 0.92, EarlyStop loss: 0.00 0.17\n",
      "Epoch 30: CostFunc loss: 0.00 0.93, EarlyStop loss: 0.00 0.17\n",
      "Epoch 40: CostFunc loss: 0.00 0.86, EarlyStop loss: 0.00 0.16\n",
      "Epoch 50: CostFunc loss: 0.00 0.89, EarlyStop loss: 0.00 0.16\n",
      "Epoch 60: CostFunc loss: 0.00 0.89, EarlyStop loss: 0.00 0.13\n",
      "Epoch 70: CostFunc loss: 0.00 0.86, EarlyStop loss: 0.00 0.13\n",
      "Epoch 80: CostFunc loss: 0.00 0.94, EarlyStop loss: 0.00 0.16\n",
      "Epoch 90: CostFunc loss: 0.00 0.88, EarlyStop loss: 0.00 0.16\n",
      "Epoch 100: CostFunc loss: 0.00 0.86, EarlyStop loss: 0.00 0.15\n",
      "Epoch 110: CostFunc loss: 0.00 0.94, EarlyStop loss: 0.00 0.17\n",
      "Epoch 120: CostFunc loss: 0.00 0.92, EarlyStop loss: 0.00 0.16\n",
      "Epoch 130: CostFunc loss: 0.00 0.89, EarlyStop loss: 0.00 0.16\n",
      "Epoch 140: CostFunc loss: 0.00 0.86, EarlyStop loss: 0.00 0.16\n",
      "Epoch 150: CostFunc loss: 0.00 0.91, EarlyStop loss: 0.00 0.17\n",
      "Epoch 160: CostFunc loss: 0.00 0.91, EarlyStop loss: 0.00 0.17\n",
      "Epoch 170: CostFunc loss: 0.00 0.87, EarlyStop loss: 0.00 0.16\n",
      "Epoch 180: CostFunc loss: 0.00 0.84, EarlyStop loss: 0.00 0.15\n",
      "Epoch 190: CostFunc loss: 0.00 0.81, EarlyStop loss: 0.00 0.13\n",
      "Epoch 200: CostFunc loss: 0.00 0.85, EarlyStop loss: 0.00 0.14\n",
      "Epoch 210: CostFunc loss: 0.00 0.93, EarlyStop loss: 0.00 0.20\n",
      "Epoch 220: CostFunc loss: 0.00 0.84, EarlyStop loss: 0.00 0.16\n",
      "Epoch 230: CostFunc loss: 0.00 0.83, EarlyStop loss: 0.00 0.15\n",
      "Epoch 240: CostFunc loss: 0.00 0.82, EarlyStop loss: 0.00 0.14\n",
      "Epoch 250: CostFunc loss: 0.00 0.84, EarlyStop loss: 0.00 0.14\n",
      "Epoch 260: CostFunc loss: 0.00 0.87, EarlyStop loss: 0.00 0.19\n",
      "Epoch 270: CostFunc loss: 0.00 0.80, EarlyStop loss: 0.00 0.13\n",
      "Epoch 280: CostFunc loss: 0.00 0.84, EarlyStop loss: 0.00 0.15\n",
      "Epoch 290: CostFunc loss: 0.00 0.91, EarlyStop loss: 0.00 0.19\n",
      "Epoch 300: CostFunc loss: 0.00 0.83, EarlyStop loss: 0.00 0.17\n"
     ]
    }
   ],
   "source": [
    "mlb=LabelBinarizer()\n",
    "\n",
    "training_errors = []\n",
    "total_networks = 10\n",
    "for network_id in range(total_networks):\n",
    "    # reset model on each iteration\n",
    "    # model = model_class(model_features)\n",
    "    # optimizer = tf.train.AdamOptimizer(model_features.learining_rate)\n",
    "    all_keys_binarized = mlb.fit_transform(all_keys.reshape([all_keys.shape[0],1]))\n",
    "    testing_keys_binarized = mlb.transform(testing_keys)\n",
    "    _, f1_error = model.fit_batch(\n",
    "        (spectra_dataset, all_keys_binarized),\n",
    "        (testing_spectra, testing_keys_binarized),\n",
    "        optimizer=optimizer,\n",
    "        num_epochs=1000,\n",
    "        verbose=10,\n",
    "        obj_cost=model.cross_entropy,\n",
    "        earlystop_cost_fn=model.f1_error,\n",
    "        earlystop_patience=1000,\n",
    "        data_augmentation=online_data_augmentation,\n",
    "        augment_testing_data=False,\n",
    "        print_errors=True,\n",
    "        record_train_errors=False)\n",
    "\n",
    "    training_errors.append(f1_error['test'])\n",
    "    np.save('./final-models/final_test_errors_'+model_id, training_errors)\n",
    "    model.save_weights('./final-models/'+model_id+'_checkpoint_'+str(network_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

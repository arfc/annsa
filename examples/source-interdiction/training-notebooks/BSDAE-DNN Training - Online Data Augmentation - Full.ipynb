{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from glob import glob\n",
    "\n",
    "from annsa.template_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model, training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annsa.model_classes import (dnn_model_features,\n",
    "                                 DNN,\n",
    "                                 dae_model_features,\n",
    "                                 DAE,\n",
    "                                 save_model,\n",
    "                                 train_earlystop)\n",
    "\n",
    "from annsa.load_pretrained_network import load_pretrained_dae_into_dnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_dataset = pd.read_csv('../../source-interdiction/training_testing_data/background_template_dataset.csv')\n",
    "source_dataset = pd.read_csv('../../source-interdiction/training_testing_data/shielded_templates_200kev_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourcedist: [50.0, 112.5, 175.0, 237.5, 300.0]\n",
      "sourceheight: [50.0, 75.0, 100.0, 125.0, 150.0]\n",
      "alum shieldingdensity: [1.82, 4.18, 7.49, 13.16]\n",
      "iron shieldingdensity: [1.53, 3.5, 6.28, 11.02]\n",
      "lead shieldingdensity: [0.22, 0.51, 0.92, 1.61]\n",
      "fwhm: [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0]\n"
     ]
    }
   ],
   "source": [
    "print('sourcedist: ' + str(sorted(set(source_dataset['sourcedist']))))\n",
    "print('sourceheight: ' + str(sorted(set(source_dataset['sourceheight']))))\n",
    "print('alum shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='alum']['shieldingdensity']))))\n",
    "print('iron shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='iron']['shieldingdensity']))))\n",
    "print('lead shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='lead']['shieldingdensity']))))\n",
    "print('fwhm: ' + str(sorted(set(source_dataset['fwhm']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = source_dataset[(source_dataset['fwhm']==7.0) | \n",
    "                                (source_dataset['fwhm']==7.5) |\n",
    "                                (source_dataset['fwhm']==8.0)]\n",
    "\n",
    "source_dataset = source_dataset[(source_dataset['sourcedist']==50.5) | \n",
    "                                (source_dataset['sourcedist']==175.0) | \n",
    "                                (source_dataset['sourcedist']==300.0)]\n",
    "\n",
    "source_dataset = source_dataset[(source_dataset['sourceheight']==50.0) |\n",
    "                                (source_dataset['sourceheight']==100.0) |\n",
    "                                (source_dataset['sourceheight']==150.0)]\n",
    "\n",
    "# remove 80% shielding\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=13.16]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=11.02]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=1.61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove empty spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicies dropped: [552 553 554 555 556 557 564 565 566 582 583 584 585 586 587 594 595 596\n",
      " 612 613 614 615 616 617 624 625 626 642 643 644 645 646 647 654 655 656\n",
      " 672 673 674 675 676 677 684 685 686 702 703 704 705 706 707 714 715 716]\n"
     ]
    }
   ],
   "source": [
    "zero_count_indicies = np.argwhere(np.sum(source_dataset.values[:,6:],axis=1) == 0).flatten()\n",
    "\n",
    "print('indicies dropped: ' +str(zero_count_indicies))\n",
    "\n",
    "source_dataset.drop(source_dataset.index[zero_count_indicies], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add empty spectra for background "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_spectra = []\n",
    "for fwhm in set(source_dataset['fwhm']):\n",
    "    num_examples = source_dataset[(source_dataset['fwhm']==fwhm) &\n",
    "                                  (source_dataset['isotope']==source_dataset['isotope'].iloc()[0])].shape[0]\n",
    "    for k in range(num_examples):\n",
    "        blank_spectra_tmp = [0]*1200\n",
    "        blank_spectra_tmp[5] = fwhm\n",
    "        blank_spectra_tmp[0] = 'background'\n",
    "        blank_spectra_tmp[3] = 'background'\n",
    "        blank_spectra.append(blank_spectra_tmp)\n",
    "\n",
    "source_dataset = source_dataset.append(pd.DataFrame(blank_spectra,\n",
    "                                                    columns=source_dataset.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset from spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_dataset = source_dataset.values[:,5:].astype('float64')\n",
    "all_keys = source_dataset['isotope'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define online data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration_time():\n",
    "    return np.random.uniform(np.log10(10),np.log10(3600))\n",
    "\n",
    "def background_cps():\n",
    "    return np.random.poisson(200)\n",
    "\n",
    "def signal_to_background():\n",
    "    return np.random.uniform(0.1,2)\n",
    "\n",
    "def calibration():\n",
    "    return [np.random.uniform(0,10),\n",
    "            np.random.uniform(2500/3000,3500/3000),\n",
    "            0]\n",
    "\n",
    "online_data_augmentation = online_data_augmentation_vanilla(background_dataset,\n",
    "                                background_cps,\n",
    "                                integration_time,\n",
    "                                signal_to_background,\n",
    "                                calibration,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create temporary testing dataset based on training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_per_isotope=10\n",
    "\n",
    "testing_spectra = []\n",
    "testing_keys = []\n",
    "\n",
    "for key in np.unique(all_keys):\n",
    "    for _ in range(examples_per_isotope):\n",
    "        testing_spectra_tmp = source_dataset[source_dataset['isotope']==key].sample().values[:,5:].astype('float64')\n",
    "        testing_spectra_tmp = online_data_augmentation(testing_spectra_tmp).numpy()[0]\n",
    "        testing_keys.append(key)\n",
    "        testing_spectra.append(testing_spectra_tmp)\n",
    "\n",
    "testing_spectra = np.array(testing_spectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id='DSCAE-CNN_onlinedataaugfull_update'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dae_models = all_dae_models = [x.split('checkpoint_')[1][:-7] for x in\n",
    "                                   glob('../hyperparameter_search/hyperparameter-search-results/BSDAE_full_tmp_checkpoint_[0-9]*index')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['128_64',\n",
       " '1024_1024_128',\n",
       " '1024_1024_32',\n",
       " '64',\n",
       " '1024_1024_64',\n",
       " '256_128',\n",
       " '512_512_128',\n",
       " '256_64',\n",
       " '256',\n",
       " '128']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dae_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../hyperparameter_search/hyperparameter-search-results/BSCAE_full_fullnetwork_128_64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e5ed3b68f98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         model, model_features = load_pretrained_dae_into_dnn(dae_features_filename = '../hyperparameter_search/hyperparameter-search-results/BSCAE_full_fullnetwork_'+model_id,\n\u001b[1;32m     13\u001b[0m                                                     \u001b[0mdae_weights_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../hyperparameter_search/hyperparameter-search-results/BSCAE_full_fullnetwork_checkpoint_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                                                     dnn_dense_nodes = [128],)\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearining_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/annsa-0.1.dev0-py3.6.egg/annsa/load_pretrained_network.py\u001b[0m in \u001b[0;36mload_pretrained_dae_into_dnn\u001b[0;34m(dae_features_filename, dae_weights_filename, dnn_dense_nodes, learining_rate, batch_size, output_size, activation_function, l2_regularization_scale, dropout_probability)\u001b[0m\n\u001b[1;32m    199\u001b[0m     '''\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m     \u001b[0mdae_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdae_features_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0mDAE_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDAE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdae_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdae_weights_filename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/annsa-0.1.dev0-py3.6.egg/annsa/load_pretrained_network.py\u001b[0m in \u001b[0;36mload_features\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     43\u001b[0m     '''\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mmodel_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../hyperparameter_search/hyperparameter-search-results/BSCAE_full_fullnetwork_128_64'"
     ]
    }
   ],
   "source": [
    "mlb=LabelBinarizer()\n",
    "\n",
    "training_errors = []\n",
    "total_networks = 10\n",
    "for network_id in range(total_networks):\n",
    "\n",
    "    # reset model on each iteration\n",
    "    all_keys_binarized = mlb.fit_transform(all_keys.reshape([all_keys.shape[0],1]))\n",
    "    testing_keys_binarized = mlb.transform(testing_keys)\n",
    "    for model_id in all_dae_models:\n",
    "\n",
    "        model, model_features = load_pretrained_dae_into_dnn(dae_features_filename = '../hyperparameter_search/hyperparameter-search-results/BSCAE_full_fullnetwork_'+model_id,\n",
    "                                                    dae_weights_filename = '../hyperparameter_search/hyperparameter-search-results/BSCAE_full_fullnetwork_checkpoint_'+model_id+'_',\n",
    "                                                    dnn_dense_nodes = [128],)\n",
    "        optimizer = tf.train.AdamOptimizer(model_features.learining_rate)\n",
    "\n",
    "        _, f1_error = model.fit_batch(\n",
    "            (spectra_dataset, all_keys_binarized),\n",
    "            (testing_spectra, testing_keys_binarized),\n",
    "            optimizer=optimizer,\n",
    "            num_epochs=1500,\n",
    "            verbose=1,\n",
    "            obj_cost=model.cross_entropy,\n",
    "            earlystop_cost_fn=model.f1_error,\n",
    "            earlystop_patience=1000,\n",
    "            data_augmentation=online_data_augmentation,\n",
    "            augment_testing_data=False,\n",
    "            print_errors=True,\n",
    "            record_train_errors=False)\n",
    "\n",
    "        # training_errors.append(f1_error['test'])\n",
    "        #np.save('./final-models/final_test_errors_'+model_id, training_errors)\n",
    "        # model.save_weights('./final-models/'+model_id+'_checkpoint_'+str(network_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_device_id = str(3)\n",
    "model_id_save_as = 'learningcurve-dnn-easy-final'\n",
    "architecture_id = 'final-models/learningcurve-dnn-easy-final-features'\n",
    "model_class_id = 'DNN'\n",
    "testing_dataset_id = '../../source-interdiction/dataset_generation/validation_dataset_200keV_log10time_100.npy'\n",
    "training_dataset_id = '../../source-interdiction/dataset_generation/training_dataset_200keV_log10time_10000.npy'\n",
    "difficulty_setting = 'easy'\n",
    "\n",
    "train_sizes = [50, 100, 500, 1000, 5000, 10000, 15000, 20000,]\n",
    "earlystop_patience = 10\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_device_id\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(5)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model, training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from annsa.model_classes import build_dnn_model, compile_model, f1\n",
    "from annsa.load_dataset import load_easy, load_full, dataset_to_spectrakeys\n",
    "from annsa.load_pretrained_network import load_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.load(training_dataset_id)\n",
    "training_spectra, training_keys = dataset_to_spectrakeys(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = np.load(testing_dataset_id)\n",
    "testing_spectra, testing_keys = dataset_to_spectrakeys(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = load_features(architecture_id)\n",
    "model_features.input_dim = 1024\n",
    "model_features.dropout_rate = model_features.dropout_probability\n",
    "model_features.loss = tf.keras.losses.categorical_crossentropy\n",
    "model_features.optimizer = tf.keras.optimizers.Adam\n",
    "model_features.output_function = tf.nn.softmax\n",
    "model_features.metrics = [f1]\n",
    "model_features.learning_rate = model_features.learining_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36_update/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36_update/lib/python3.6/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = compile_model(\n",
    "            build_dnn_model,\n",
    "            model_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape (Reshape)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                65600     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                1950      \n",
      "=================================================================\n",
      "Total params: 67,550\n",
      "Trainable params: 67,550\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_spectra_scaled = model_features.scaler.transform(training_spectra)\n",
    "testing_spectra_scaled = model_features.scaler.transform(testing_spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_f1',\n",
    "    patience=earlystop_patience,\n",
    "    mode='max',\n",
    "    min_delta=0.01,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running through training size 50\n",
      "Running through fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36_update/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 2s 40ms/sample - loss: 29.9896 - f1: 0.0590 - val_loss: 28.5813 - val_f1: 0.0323\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 28.5834 - f1: 0.0479 - val_loss: 26.6575 - val_f1: 0.0098\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 26.9324 - f1: 0.0172 - val_loss: 25.7215 - val_f1: 0.0205\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 25.6597 - f1: 0.0800 - val_loss: 24.6484 - val_f1: 0.0277\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 24.5101 - f1: 0.0900 - val_loss: 23.1004 - val_f1: 0.0344\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 23.1695 - f1: 0.1140 - val_loss: 22.1958 - val_f1: 0.0517\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 21.9344 - f1: 0.0697 - val_loss: 20.7921 - val_f1: 0.0366\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 20.8729 - f1: 0.0613 - val_loss: 19.6744 - val_f1: 0.0515\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 19.3902 - f1: 0.0879 - val_loss: 18.7306 - val_f1: 0.0295\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 18.2259 - f1: 0.0871 - val_loss: 17.9233 - val_f1: 0.0141\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 17.3988 - f1: 0.0217 - val_loss: 17.0979 - val_f1: 0.0159\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 16.3657 - f1: 0.0233 - val_loss: 15.7754 - val_f1: 0.0177\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 15.1624 - f1: 0.0500 - val_loss: 14.6168 - val_f1: 0.0142\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 14.1342 - f1: 0.0541 - val_loss: 13.7784 - val_f1: 0.0069\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 13.1363 - f1: 0.1067 - val_loss: 13.0598 - val_f1: 0.0019\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 2s 38ms/sample - loss: 12.5538 - f1: 0.0000e+00 - val_loss: 12.6413 - val_f1: 0.0000e+00\n",
      "Running through fold 1\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 2s 41ms/sample - loss: 31.0975 - f1: 0.0312 - val_loss: 29.4181 - val_f1: 0.0334\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 28.5937 - f1: 0.0286 - val_loss: 27.7043 - val_f1: 0.0114\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 26.8060 - f1: 0.0333 - val_loss: 25.4504 - val_f1: 0.0375\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 24.6984 - f1: 0.0492 - val_loss: 24.0586 - val_f1: 0.0286\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 23.2579 - f1: 0.0498 - val_loss: 22.5215 - val_f1: 0.0265\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 21.7529 - f1: 0.0000e+00 - val_loss: 21.4597 - val_f1: 0.0396\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 20.7517 - f1: 0.0886 - val_loss: 20.0172 - val_f1: 0.0247\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 19.3189 - f1: 0.0345 - val_loss: 18.7940 - val_f1: 0.0460\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 17.9751 - f1: 0.0222 - val_loss: 17.9087 - val_f1: 0.0644\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 17.0591 - f1: 0.1037 - val_loss: 16.9032 - val_f1: 0.0566\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 15.9863 - f1: 0.1051 - val_loss: 15.5980 - val_f1: 0.0457\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 14.5057 - f1: 0.1140 - val_loss: 14.7868 - val_f1: 0.0541\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 14.0270 - f1: 0.0976 - val_loss: 14.1474 - val_f1: 0.0429\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 13.2627 - f1: 0.1017 - val_loss: 13.7040 - val_f1: 0.0406\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 12.8126 - f1: 0.0541 - val_loss: 12.9890 - val_f1: 0.0407\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 12.1553 - f1: 0.0571 - val_loss: 12.2756 - val_f1: 0.0351\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 11.5827 - f1: 0.0571 - val_loss: 11.7371 - val_f1: 0.0253\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 11.0910 - f1: 0.0829 - val_loss: 11.3692 - val_f1: 0.0222\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 2s 39ms/sample - loss: 10.6946 - f1: 0.0588 - val_loss: 11.1150 - val_f1: 0.0330\n",
      "Running through fold 2\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 2s 46ms/sample - loss: 28.6088 - f1: 0.0172 - val_loss: 27.1037 - val_f1: 0.0492\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 26.1617 - f1: 0.0351 - val_loss: 25.2933 - val_f1: 0.0827\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 25.3570 - f1: 0.1221 - val_loss: 25.4097 - val_f1: 0.0816\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 25.0366 - f1: 0.0990 - val_loss: 24.4877 - val_f1: 0.0761\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 23.6230 - f1: 0.1413 - val_loss: 23.6280 - val_f1: 0.1095\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 22.7684 - f1: 0.1499 - val_loss: 23.2035 - val_f1: 0.1215\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 22.0274 - f1: 0.1455 - val_loss: 22.5012 - val_f1: 0.1085\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 21.6685 - f1: 0.1294 - val_loss: 21.3478 - val_f1: 0.1106\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 20.6116 - f1: 0.2130 - val_loss: 20.4170 - val_f1: 0.1314\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 19.6432 - f1: 0.2284 - val_loss: 19.2797 - val_f1: 0.1261\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 18.3699 - f1: 0.1834 - val_loss: 18.2158 - val_f1: 0.0646\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 17.3746 - f1: 0.0345 - val_loss: 17.8302 - val_f1: 0.0377\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 16.6742 - f1: 0.0408 - val_loss: 16.9153 - val_f1: 0.0676\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 15.6602 - f1: 0.2083 - val_loss: 16.2967 - val_f1: 0.1118\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 15.2611 - f1: 0.1610 - val_loss: 15.5263 - val_f1: 0.1178\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 2s 44ms/sample - loss: 14.2087 - f1: 0.1458 - val_loss: 14.9535 - val_f1: 0.1157\n",
      "Running through fold 3\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 2s 44ms/sample - loss: 29.6849 - f1: 0.0333 - val_loss: 27.4044 - val_f1: 0.0241\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 27.7893 - f1: 0.0185 - val_loss: 26.3417 - val_f1: 0.0544\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 26.4786 - f1: 0.0519 - val_loss: 25.2953 - val_f1: 0.0272\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 25.2589 - f1: 0.0370 - val_loss: 24.5006 - val_f1: 0.0287\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 24.6711 - f1: 0.0667 - val_loss: 23.4074 - val_f1: 0.0390\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 23.7873 - f1: 0.0377 - val_loss: 21.9459 - val_f1: 0.0519\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 21.8766 - f1: 0.1576 - val_loss: 20.7451 - val_f1: 0.0467\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 20.5621 - f1: 0.1084 - val_loss: 19.5785 - val_f1: 0.0533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 19.5339 - f1: 0.1413 - val_loss: 18.6420 - val_f1: 0.0410\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 18.0855 - f1: 0.0625 - val_loss: 17.4152 - val_f1: 0.0345\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 16.7097 - f1: 0.0893 - val_loss: 16.0043 - val_f1: 0.0315\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 2s 43ms/sample - loss: 15.2789 - f1: 0.0698 - val_loss: 14.8204 - val_f1: 0.0253\n",
      "Running through fold 4\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 2s 45ms/sample - loss: 29.5934 - f1: 0.0159 - val_loss: 28.0202 - val_f1: 0.0151\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 27.7364 - f1: 0.0333 - val_loss: 27.0807 - val_f1: 0.0220\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 26.7684 - f1: 0.0467 - val_loss: 25.8206 - val_f1: 0.0102\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 25.9490 - f1: 0.0588 - val_loss: 25.1066 - val_f1: 0.0342\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 24.9699 - f1: 0.0690 - val_loss: 24.0736 - val_f1: 0.0422\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 24.1379 - f1: 0.0874 - val_loss: 22.9085 - val_f1: 0.0529\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 23.0142 - f1: 0.1211 - val_loss: 22.1383 - val_f1: 0.0617\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 22.7842 - f1: 0.1023 - val_loss: 21.3048 - val_f1: 0.0546\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 21.2561 - f1: 0.1000 - val_loss: 20.2304 - val_f1: 0.0263\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 19.8759 - f1: 0.0890 - val_loss: 19.2473 - val_f1: 0.0652\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 18.8821 - f1: 0.0638 - val_loss: 18.6457 - val_f1: 0.0489\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 18.1678 - f1: 0.1557 - val_loss: 17.9010 - val_f1: 0.0331\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 16.8901 - f1: 0.1603 - val_loss: 17.1209 - val_f1: 0.0299\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 15.9490 - f1: 0.0417 - val_loss: 16.1511 - val_f1: 0.0150\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 15.3466 - f1: 0.0945 - val_loss: 15.4937 - val_f1: 0.0272\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 14.7960 - f1: 0.0871 - val_loss: 14.9580 - val_f1: 0.0199\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 14.1118 - f1: 0.1082 - val_loss: 14.3463 - val_f1: 0.0400\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 12.9784 - f1: 0.2469 - val_loss: 13.6145 - val_f1: 0.0647\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 4ms/sample - loss: 12.2432 - f1: 0.1933 - val_loss: 12.9055 - val_f1: 0.0623\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 2s 43ms/sample - loss: 11.4860 - f1: 0.1845 - val_loss: 12.4640 - val_f1: 0.0555\n",
      "\n",
      "\n",
      "Running through training size 100\n",
      "Running through fold 0\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 2s 23ms/sample - loss: 29.3436 - f1: 0.1044 - val_loss: 27.2066 - val_f1: 0.0788\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.4401 - f1: 0.0687 - val_loss: 25.3889 - val_f1: 0.0405\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 25.0280 - f1: 0.0521 - val_loss: 22.7421 - val_f1: 0.0707\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.8814 - f1: 0.0797 - val_loss: 20.2398 - val_f1: 0.0920\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.7921 - f1: 0.0923 - val_loss: 18.4928 - val_f1: 0.0209\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.3441 - f1: 0.0278 - val_loss: 16.5085 - val_f1: 0.0078\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.9944 - f1: 0.0467 - val_loss: 14.6010 - val_f1: 0.0339\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.1276 - f1: 0.0507 - val_loss: 13.1827 - val_f1: 0.0394\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 12.6628 - f1: 0.0548 - val_loss: 11.8845 - val_f1: 0.0144\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.5220 - f1: 0.1303 - val_loss: 11.2638 - val_f1: 0.0171\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.0234 - f1: 0.0417 - val_loss: 10.7895 - val_f1: 0.0204\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 10.5717 - f1: 0.0455 - val_loss: 10.3548 - val_f1: 0.0192\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 10.1508 - f1: 0.0446 - val_loss: 9.9708 - val_f1: 0.0237\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 2s 23ms/sample - loss: 9.7698 - f1: 0.0446 - val_loss: 9.6257 - val_f1: 0.0185\n",
      "Running through fold 1\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 2s 23ms/sample - loss: 29.4099 - f1: 0.0334 - val_loss: 25.9832 - val_f1: 0.0547\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.1804 - f1: 0.0264 - val_loss: 23.6239 - val_f1: 0.0952\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.7233 - f1: 0.0632 - val_loss: 21.5783 - val_f1: 0.1042\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.2932 - f1: 0.1977 - val_loss: 19.4788 - val_f1: 0.0883\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.3474 - f1: 0.0870 - val_loss: 18.1613 - val_f1: 0.0904\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.6509 - f1: 0.0554 - val_loss: 16.2584 - val_f1: 0.0570\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.7522 - f1: 0.1478 - val_loss: 14.8454 - val_f1: 0.0737\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.2461 - f1: 0.0542 - val_loss: 13.7111 - val_f1: 0.0387\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 13.1265 - f1: 0.0410 - val_loss: 12.7218 - val_f1: 0.0216\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 12.2835 - f1: 0.0152 - val_loss: 12.1529 - val_f1: 0.0191\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.7885 - f1: 0.0299 - val_loss: 11.6924 - val_f1: 0.0159\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 2s 23ms/sample - loss: 11.2708 - f1: 0.0152 - val_loss: 11.3537 - val_f1: 0.0289\n",
      "Running through fold 2\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 2s 24ms/sample - loss: 30.0397 - f1: 0.0242 - val_loss: 26.8745 - val_f1: 0.0507\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.4349 - f1: 0.0706 - val_loss: 23.4441 - val_f1: 0.0309\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.8631 - f1: 0.0394 - val_loss: 20.7144 - val_f1: 0.0574\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.3152 - f1: 0.0546 - val_loss: 18.7426 - val_f1: 0.0201\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.6842 - f1: 0.0308 - val_loss: 17.7120 - val_f1: 0.0477\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.3044 - f1: 0.1349 - val_loss: 15.9469 - val_f1: 0.0657\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.6840 - f1: 0.0628 - val_loss: 14.8298 - val_f1: 0.0857\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.6949 - f1: 0.1142 - val_loss: 14.0663 - val_f1: 0.0791\n",
      "Epoch 9/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 2ms/sample - loss: 13.7332 - f1: 0.1118 - val_loss: 13.3613 - val_f1: 0.0846\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 12.8439 - f1: 0.0603 - val_loss: 12.6919 - val_f1: 0.0670\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 12.6003 - f1: 0.1524 - val_loss: 11.8818 - val_f1: 0.0737\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.2916 - f1: 0.1276 - val_loss: 11.1069 - val_f1: 0.0581\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 10.4322 - f1: 0.0563 - val_loss: 10.2200 - val_f1: 0.0223\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 9.8032 - f1: 0.0147 - val_loss: 9.9712 - val_f1: 0.0069\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 9.5608 - f1: 0.0000e+00 - val_loss: 9.5320 - val_f1: 0.0086\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 9.0850 - f1: 0.0000e+00 - val_loss: 9.1992 - val_f1: 0.0196\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 2s 23ms/sample - loss: 8.8522 - f1: 0.0000e+00 - val_loss: 8.8619 - val_f1: 0.0239\n",
      "Running through fold 3\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 2s 24ms/sample - loss: 28.2848 - f1: 0.0085 - val_loss: 26.4368 - val_f1: 0.0802\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 25.6435 - f1: 0.0608 - val_loss: 23.8770 - val_f1: 0.0158\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.0635 - f1: 0.0089 - val_loss: 23.1832 - val_f1: 0.0521\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.5041 - f1: 0.1715 - val_loss: 21.4022 - val_f1: 0.0340\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.8964 - f1: 0.1031 - val_loss: 19.5979 - val_f1: 0.0525\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.2530 - f1: 0.0619 - val_loss: 19.0050 - val_f1: 0.0553\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.1228 - f1: 0.0622 - val_loss: 17.3444 - val_f1: 0.0628\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.1659 - f1: 0.1682 - val_loss: 15.6405 - val_f1: 0.0819\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.9305 - f1: 0.1490 - val_loss: 14.3721 - val_f1: 0.1220\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.0049 - f1: 0.1230 - val_loss: 13.6347 - val_f1: 0.0718\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 13.0488 - f1: 0.1248 - val_loss: 13.0555 - val_f1: 0.0891\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 12.8590 - f1: 0.1043 - val_loss: 12.2836 - val_f1: 0.0930\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.8061 - f1: 0.2384 - val_loss: 12.0881 - val_f1: 0.0670\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.4142 - f1: 0.1235 - val_loss: 11.8335 - val_f1: 0.1136\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.0884 - f1: 0.1255 - val_loss: 11.1879 - val_f1: 0.1096\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 10.3850 - f1: 0.1516 - val_loss: 10.6553 - val_f1: 0.0816\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 9.8055 - f1: 0.1576 - val_loss: 10.2740 - val_f1: 0.0828\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 9.5312 - f1: 0.1202 - val_loss: 9.9734 - val_f1: 0.0750\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 2s 24ms/sample - loss: 9.1311 - f1: 0.1388 - val_loss: 9.5297 - val_f1: 0.1028\n",
      "Running through fold 4\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 3s 25ms/sample - loss: 29.9386 - f1: 0.0156 - val_loss: 26.3573 - val_f1: 0.0742\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.0156 - f1: 0.0518 - val_loss: 25.4862 - val_f1: 0.0519\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.4895 - f1: 0.0428 - val_loss: 23.5795 - val_f1: 0.0568\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.4571 - f1: 0.0285 - val_loss: 21.5616 - val_f1: 0.0728\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.1670 - f1: 0.0652 - val_loss: 19.9395 - val_f1: 0.1000\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.0619 - f1: 0.1102 - val_loss: 18.4429 - val_f1: 0.1076\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.5551 - f1: 0.1213 - val_loss: 17.0257 - val_f1: 0.1362\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.6372 - f1: 0.1289 - val_loss: 16.0926 - val_f1: 0.1114\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.7711 - f1: 0.1251 - val_loss: 16.0551 - val_f1: 0.1162\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.0986 - f1: 0.1961 - val_loss: 14.3348 - val_f1: 0.1090\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 13.4834 - f1: 0.1265 - val_loss: 13.3812 - val_f1: 0.0742\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 12.7413 - f1: 0.1224 - val_loss: 12.2119 - val_f1: 0.0806\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 11.2262 - f1: 0.2561 - val_loss: 11.1996 - val_f1: 0.0613\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 10.5355 - f1: 0.0906 - val_loss: 10.5958 - val_f1: 0.0682\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 9.8701 - f1: 0.0963 - val_loss: 10.0304 - val_f1: 0.0505\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 9.5675 - f1: 0.0580 - val_loss: 9.8574 - val_f1: 0.0527\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 3s 26ms/sample - loss: 9.3520 - f1: 0.0974 - val_loss: 9.5754 - val_f1: 0.0625\n",
      "\n",
      "\n",
      "Running through training size 500\n",
      "Running through fold 0\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 3s 5ms/sample - loss: 26.8101 - f1: 0.0557 - val_loss: 22.8313 - val_f1: 0.0626\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 474us/sample - loss: 18.6939 - f1: 0.0529 - val_loss: 13.5143 - val_f1: 0.0101\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 470us/sample - loss: 11.8627 - f1: 0.0038 - val_loss: 10.5263 - val_f1: 0.0185\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 477us/sample - loss: 9.7908 - f1: 0.0057 - val_loss: 8.9227 - val_f1: 0.0283\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 477us/sample - loss: 8.3985 - f1: 0.0313 - val_loss: 7.7850 - val_f1: 0.0430\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 475us/sample - loss: 7.4223 - f1: 0.0368 - val_loss: 6.8894 - val_f1: 0.0619\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 473us/sample - loss: 6.5846 - f1: 0.0686 - val_loss: 6.3495 - val_f1: 0.0672\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 5.9441 - f1: 0.0616 - val_loss: 5.5813 - val_f1: 0.1186\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 480us/sample - loss: 5.2891 - f1: 0.1343 - val_loss: 5.0475 - val_f1: 0.1319\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 480us/sample - loss: 4.9108 - f1: 0.1586 - val_loss: 4.7390 - val_f1: 0.1754\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 4.5287 - f1: 0.1733 - val_loss: 4.4494 - val_f1: 0.2105\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 475us/sample - loss: 4.1702 - f1: 0.1940 - val_loss: 4.0536 - val_f1: 0.1911\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 3.7921 - f1: 0.2570 - val_loss: 3.7102 - val_f1: 0.2420\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 472us/sample - loss: 3.5587 - f1: 0.2696 - val_loss: 3.4957 - val_f1: 0.2539\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 476us/sample - loss: 3.2644 - f1: 0.2793 - val_loss: 3.2399 - val_f1: 0.2747\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 477us/sample - loss: 3.0997 - f1: 0.3227 - val_loss: 3.0338 - val_f1: 0.3031\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 479us/sample - loss: 2.7917 - f1: 0.4037 - val_loss: 2.8267 - val_f1: 0.3403\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 2.6577 - f1: 0.4088 - val_loss: 2.6103 - val_f1: 0.4300\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 465us/sample - loss: 2.5048 - f1: 0.4600 - val_loss: 2.5151 - val_f1: 0.4392\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 472us/sample - loss: 2.3499 - f1: 0.4877 - val_loss: 2.3627 - val_f1: 0.4726\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 2.2595 - f1: 0.5091 - val_loss: 2.2620 - val_f1: 0.4612\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 469us/sample - loss: 2.0903 - f1: 0.5601 - val_loss: 2.1769 - val_f1: 0.5044\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 478us/sample - loss: 2.0362 - f1: 0.5845 - val_loss: 2.0750 - val_f1: 0.5077\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 1.8627 - f1: 0.6202 - val_loss: 2.0729 - val_f1: 0.5136\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 1.8697 - f1: 0.6012 - val_loss: 1.9285 - val_f1: 0.5363\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 1.8137 - f1: 0.6071 - val_loss: 1.9180 - val_f1: 0.5303\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 1.7862 - f1: 0.6235 - val_loss: 1.7905 - val_f1: 0.5737\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 1.7357 - f1: 0.6163 - val_loss: 1.9067 - val_f1: 0.5489\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.6558 - f1: 0.6688 - val_loss: 1.7028 - val_f1: 0.6049\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 1.6093 - f1: 0.6634 - val_loss: 1.6341 - val_f1: 0.6053\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.5033 - f1: 0.6984 - val_loss: 1.6240 - val_f1: 0.6212\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.4777 - f1: 0.7070 - val_loss: 1.9606 - val_f1: 0.5014\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.4923 - f1: 0.6850 - val_loss: 1.5209 - val_f1: 0.6368\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 464us/sample - loss: 1.4185 - f1: 0.7129 - val_loss: 1.5070 - val_f1: 0.6343\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 474us/sample - loss: 1.4259 - f1: 0.7084 - val_loss: 1.5266 - val_f1: 0.6379\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 470us/sample - loss: 1.3679 - f1: 0.6912 - val_loss: 1.4652 - val_f1: 0.6528\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 1.3065 - f1: 0.7340 - val_loss: 1.4010 - val_f1: 0.6799\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 466us/sample - loss: 1.2322 - f1: 0.7464 - val_loss: 1.3746 - val_f1: 0.6736\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 467us/sample - loss: 1.3128 - f1: 0.7443 - val_loss: 1.3868 - val_f1: 0.6652\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 478us/sample - loss: 1.3231 - f1: 0.7055 - val_loss: 1.3679 - val_f1: 0.6680\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.2113 - f1: 0.7591 - val_loss: 1.3315 - val_f1: 0.6946\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 1.1751 - f1: 0.7911 - val_loss: 1.3029 - val_f1: 0.7010\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.1804 - f1: 0.7661 - val_loss: 1.2998 - val_f1: 0.6867\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 1.2006 - f1: 0.7360 - val_loss: 1.2516 - val_f1: 0.7341\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 1.1685 - f1: 0.7755 - val_loss: 1.2246 - val_f1: 0.7006\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.1362 - f1: 0.7651 - val_loss: 1.1990 - val_f1: 0.7042\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 1.1758 - f1: 0.7592 - val_loss: 1.2005 - val_f1: 0.7188\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 1.0641 - f1: 0.7798 - val_loss: 1.1770 - val_f1: 0.7264\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 1.0447 - f1: 0.8051 - val_loss: 1.2705 - val_f1: 0.7210\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 0.9703 - f1: 0.8231 - val_loss: 1.1385 - val_f1: 0.7413\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 0.9436 - f1: 0.8221 - val_loss: 1.1366 - val_f1: 0.7513\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 0.9825 - f1: 0.8147 - val_loss: 1.0822 - val_f1: 0.7439\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.0172 - f1: 0.8042 - val_loss: 1.1666 - val_f1: 0.7187\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 0.9616 - f1: 0.8058 - val_loss: 1.0652 - val_f1: 0.7552\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 0.9353 - f1: 0.8112 - val_loss: 1.0889 - val_f1: 0.7404\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 0.9119 - f1: 0.8363 - val_loss: 1.0647 - val_f1: 0.7787\n",
      "Epoch 57/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 0.9617 - f1: 0.8192 - val_loss: 1.1634 - val_f1: 0.7093\n",
      "Epoch 58/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 0.9578 - f1: 0.8226 - val_loss: 1.0367 - val_f1: 0.7545\n",
      "Epoch 59/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 0.9680 - f1: 0.7914 - val_loss: 1.0767 - val_f1: 0.7274\n",
      "Epoch 60/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.9373 - f1: 0.8134 - val_loss: 1.1195 - val_f1: 0.7202\n",
      "Epoch 61/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.8891 - f1: 0.8232 - val_loss: 1.0039 - val_f1: 0.7758\n",
      "Epoch 62/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 0.8705 - f1: 0.8370 - val_loss: 0.9923 - val_f1: 0.7850\n",
      "Epoch 63/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.8031 - f1: 0.8724 - val_loss: 0.9730 - val_f1: 0.7965\n",
      "Epoch 64/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 0.8363 - f1: 0.8432 - val_loss: 0.9442 - val_f1: 0.8009\n",
      "Epoch 65/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 0.8474 - f1: 0.8369 - val_loss: 0.9496 - val_f1: 0.7771\n",
      "Epoch 66/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 0.8270 - f1: 0.8573 - val_loss: 0.9139 - val_f1: 0.8036\n",
      "Epoch 67/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 0.8201 - f1: 0.8487 - val_loss: 0.9748 - val_f1: 0.7616\n",
      "Epoch 68/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 0.8665 - f1: 0.8350 - val_loss: 0.9945 - val_f1: 0.7789\n",
      "Epoch 69/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 0.8204 - f1: 0.8354 - val_loss: 0.9399 - val_f1: 0.7836\n",
      "Epoch 70/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 0.8238 - f1: 0.8514 - val_loss: 0.9213 - val_f1: 0.7921\n",
      "Epoch 71/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 0.8094 - f1: 0.8473 - val_loss: 0.9923 - val_f1: 0.7795\n",
      "Epoch 72/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 0.7444 - f1: 0.8809 - val_loss: 0.9283 - val_f1: 0.8029\n",
      "Epoch 73/2000\n",
      "500/500 [==============================] - 3s 5ms/sample - loss: 0.7975 - f1: 0.8548 - val_loss: 0.9568 - val_f1: 0.7779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running through fold 1\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 3s 5ms/sample - loss: 25.2547 - f1: 0.0813 - val_loss: 21.0246 - val_f1: 0.1230\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 18.1181 - f1: 0.1352 - val_loss: 14.3135 - val_f1: 0.0425\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 12.2272 - f1: 0.0343 - val_loss: 10.9061 - val_f1: 0.0269\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 10.1479 - f1: 0.0285 - val_loss: 9.3868 - val_f1: 0.0524\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 8.7583 - f1: 0.0703 - val_loss: 8.1924 - val_f1: 0.0903\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 7.6804 - f1: 0.0847 - val_loss: 7.3079 - val_f1: 0.0933\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 6.8027 - f1: 0.1031 - val_loss: 6.7297 - val_f1: 0.1327\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 6.3115 - f1: 0.0762 - val_loss: 6.1521 - val_f1: 0.0557\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 5.7259 - f1: 0.0928 - val_loss: 5.5124 - val_f1: 0.1356\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 5.1352 - f1: 0.1583 - val_loss: 5.0742 - val_f1: 0.1610\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 4.7278 - f1: 0.2077 - val_loss: 4.7505 - val_f1: 0.1643\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 4.4237 - f1: 0.2034 - val_loss: 4.5118 - val_f1: 0.1730\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 4.2237 - f1: 0.1826 - val_loss: 4.2662 - val_f1: 0.1681\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 3.8388 - f1: 0.2511 - val_loss: 3.8866 - val_f1: 0.1853\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 3.6388 - f1: 0.2751 - val_loss: 3.6700 - val_f1: 0.2868\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 3.2501 - f1: 0.3452 - val_loss: 3.2761 - val_f1: 0.3528\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 509us/sample - loss: 2.9581 - f1: 0.4426 - val_loss: 3.1066 - val_f1: 0.3761\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 494us/sample - loss: 2.8010 - f1: 0.4631 - val_loss: 2.9361 - val_f1: 0.3710\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 2.6837 - f1: 0.4661 - val_loss: 2.9044 - val_f1: 0.3846\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 2.5479 - f1: 0.4834 - val_loss: 2.7067 - val_f1: 0.4488\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 2.3823 - f1: 0.5501 - val_loss: 2.5423 - val_f1: 0.4651\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 2.2598 - f1: 0.5659 - val_loss: 2.4779 - val_f1: 0.4802\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 494us/sample - loss: 2.2027 - f1: 0.5777 - val_loss: 2.3584 - val_f1: 0.5024\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 2.1129 - f1: 0.5727 - val_loss: 2.2524 - val_f1: 0.5189\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 1.9914 - f1: 0.6220 - val_loss: 2.1417 - val_f1: 0.5505\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 1.9613 - f1: 0.5921 - val_loss: 2.0980 - val_f1: 0.5548\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.8165 - f1: 0.6888 - val_loss: 2.0642 - val_f1: 0.5721\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.8304 - f1: 0.6225 - val_loss: 2.0006 - val_f1: 0.5911\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 1.8440 - f1: 0.6517 - val_loss: 1.9472 - val_f1: 0.5712\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 1.7357 - f1: 0.6464 - val_loss: 1.8812 - val_f1: 0.5924\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 1.6485 - f1: 0.7101 - val_loss: 1.8376 - val_f1: 0.5832\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 1.6289 - f1: 0.6697 - val_loss: 1.8125 - val_f1: 0.5862\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 494us/sample - loss: 1.6681 - f1: 0.6403 - val_loss: 1.8221 - val_f1: 0.5763\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 1.5167 - f1: 0.7221 - val_loss: 1.6851 - val_f1: 0.6456\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.5283 - f1: 0.6993 - val_loss: 1.6434 - val_f1: 0.6298\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 1.4504 - f1: 0.6966 - val_loss: 1.6148 - val_f1: 0.6484\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 1.3776 - f1: 0.7328 - val_loss: 1.5550 - val_f1: 0.6616\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 1.3590 - f1: 0.7524 - val_loss: 1.5012 - val_f1: 0.6842\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 1.3764 - f1: 0.7111 - val_loss: 1.5244 - val_f1: 0.6521\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 1.2981 - f1: 0.7512 - val_loss: 1.4563 - val_f1: 0.6816\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.2894 - f1: 0.7297 - val_loss: 1.4983 - val_f1: 0.6471\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 1.2450 - f1: 0.7701 - val_loss: 1.4316 - val_f1: 0.6770\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.1976 - f1: 0.7596 - val_loss: 1.3953 - val_f1: 0.6914\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.2762 - f1: 0.7503 - val_loss: 1.3705 - val_f1: 0.6842\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 1.2003 - f1: 0.7372 - val_loss: 1.3548 - val_f1: 0.7025\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 1.1540 - f1: 0.7792 - val_loss: 1.3930 - val_f1: 0.6932\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.1498 - f1: 0.7810 - val_loss: 1.3301 - val_f1: 0.6895\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 1.1748 - f1: 0.7666 - val_loss: 1.3042 - val_f1: 0.7056\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 1.0904 - f1: 0.7713 - val_loss: 1.2904 - val_f1: 0.7074\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 1.1629 - f1: 0.7722 - val_loss: 1.2541 - val_f1: 0.7128\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.1118 - f1: 0.7693 - val_loss: 1.2691 - val_f1: 0.7175\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.1309 - f1: 0.7664 - val_loss: 1.2487 - val_f1: 0.7264\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.0122 - f1: 0.7940 - val_loss: 1.2785 - val_f1: 0.6976\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.0216 - f1: 0.7834 - val_loss: 1.2219 - val_f1: 0.7287\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 1.0753 - f1: 0.7891 - val_loss: 1.2054 - val_f1: 0.7221\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 494us/sample - loss: 0.9982 - f1: 0.7946 - val_loss: 1.1496 - val_f1: 0.7457\n",
      "Epoch 57/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 1.0274 - f1: 0.7960 - val_loss: 1.1706 - val_f1: 0.7313\n",
      "Epoch 58/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 0.9756 - f1: 0.8009 - val_loss: 1.1722 - val_f1: 0.7214\n",
      "Epoch 59/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.0892 - f1: 0.7540 - val_loss: 1.2458 - val_f1: 0.7147\n",
      "Epoch 60/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 0.9886 - f1: 0.8071 - val_loss: 1.1368 - val_f1: 0.7611\n",
      "Epoch 61/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 0.9609 - f1: 0.8162 - val_loss: 1.1198 - val_f1: 0.7550\n",
      "Epoch 62/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 0.9733 - f1: 0.8094 - val_loss: 1.0775 - val_f1: 0.7685\n",
      "Epoch 63/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.9592 - f1: 0.8193 - val_loss: 1.1309 - val_f1: 0.7410\n",
      "Epoch 64/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 0.9498 - f1: 0.8019 - val_loss: 1.0896 - val_f1: 0.7551\n",
      "Epoch 65/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.8933 - f1: 0.8324 - val_loss: 1.0591 - val_f1: 0.7640\n",
      "Epoch 66/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 0.9233 - f1: 0.8339 - val_loss: 1.0501 - val_f1: 0.7762\n",
      "Epoch 67/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.8959 - f1: 0.8086 - val_loss: 1.0646 - val_f1: 0.7579\n",
      "Epoch 68/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 0.8449 - f1: 0.8284 - val_loss: 1.0584 - val_f1: 0.7626\n",
      "Epoch 69/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 0.8259 - f1: 0.8529 - val_loss: 1.0425 - val_f1: 0.7593\n",
      "Epoch 70/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.9144 - f1: 0.8101 - val_loss: 1.1525 - val_f1: 0.7000\n",
      "Epoch 71/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 0.8977 - f1: 0.7982 - val_loss: 1.0799 - val_f1: 0.7434\n",
      "Epoch 72/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 0.8687 - f1: 0.8164 - val_loss: 1.0686 - val_f1: 0.7449\n",
      "Epoch 73/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 0.8826 - f1: 0.8238 - val_loss: 1.0324 - val_f1: 0.7610\n",
      "Epoch 74/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 0.8927 - f1: 0.8278 - val_loss: 1.0882 - val_f1: 0.7434\n",
      "Epoch 75/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 0.7820 - f1: 0.8578 - val_loss: 0.9993 - val_f1: 0.7764\n",
      "Epoch 76/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 0.8540 - f1: 0.8133 - val_loss: 0.9884 - val_f1: 0.7880\n",
      "Epoch 77/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 0.7945 - f1: 0.8468 - val_loss: 1.0645 - val_f1: 0.7492\n",
      "Epoch 78/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 0.8198 - f1: 0.8298 - val_loss: 0.9926 - val_f1: 0.7763\n",
      "Epoch 79/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.7981 - f1: 0.8514 - val_loss: 1.0177 - val_f1: 0.7569\n",
      "Epoch 80/2000\n",
      "500/500 [==============================] - 0s 512us/sample - loss: 0.7873 - f1: 0.8319 - val_loss: 0.9829 - val_f1: 0.7738\n",
      "Epoch 81/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 0.7934 - f1: 0.8418 - val_loss: 1.0279 - val_f1: 0.7563\n",
      "Epoch 82/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 0.8210 - f1: 0.8295 - val_loss: 1.0779 - val_f1: 0.7293\n",
      "Epoch 83/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 0.9232 - f1: 0.8023 - val_loss: 1.0419 - val_f1: 0.7541\n",
      "Epoch 84/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 0.8050 - f1: 0.8424 - val_loss: 0.9387 - val_f1: 0.8092\n",
      "Epoch 85/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 0.7764 - f1: 0.8430 - val_loss: 0.9774 - val_f1: 0.7774\n",
      "Epoch 86/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.7609 - f1: 0.8621 - val_loss: 0.9045 - val_f1: 0.8133\n",
      "Epoch 87/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 0.7176 - f1: 0.8637 - val_loss: 0.9687 - val_f1: 0.7771\n",
      "Epoch 88/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 0.7914 - f1: 0.8178 - val_loss: 0.9025 - val_f1: 0.8053\n",
      "Epoch 89/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.7443 - f1: 0.8329 - val_loss: 0.9061 - val_f1: 0.8063\n",
      "Epoch 90/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 0.7945 - f1: 0.8398 - val_loss: 0.9282 - val_f1: 0.7979\n",
      "Epoch 91/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 0.7319 - f1: 0.8512 - val_loss: 0.9673 - val_f1: 0.7779\n",
      "Epoch 92/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 0.7942 - f1: 0.8257 - val_loss: 1.0311 - val_f1: 0.7412\n",
      "Epoch 93/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 0.7575 - f1: 0.8438 - val_loss: 0.9782 - val_f1: 0.7838\n",
      "Epoch 94/2000\n",
      "500/500 [==============================] - 3s 5ms/sample - loss: 0.8137 - f1: 0.8333 - val_loss: 0.9405 - val_f1: 0.7815\n",
      "Running through fold 2\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 3s 5ms/sample - loss: 21.9489 - f1: 0.0632 - val_loss: 16.4310 - val_f1: 0.0232\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 14.0621 - f1: 0.0188 - val_loss: 12.4442 - val_f1: 0.0252\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 11.3699 - f1: 0.0223 - val_loss: 10.2790 - val_f1: 0.0262\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 511us/sample - loss: 9.5027 - f1: 0.0341 - val_loss: 8.6804 - val_f1: 0.0404\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 8.1678 - f1: 0.0320 - val_loss: 7.6335 - val_f1: 0.0254\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 511us/sample - loss: 7.2010 - f1: 0.0357 - val_loss: 6.7848 - val_f1: 0.0456\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 6.4435 - f1: 0.0605 - val_loss: 6.1702 - val_f1: 0.0500\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 513us/sample - loss: 5.9019 - f1: 0.0663 - val_loss: 5.7094 - val_f1: 0.0573\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 515us/sample - loss: 5.4752 - f1: 0.0628 - val_loss: 5.3642 - val_f1: 0.0488\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 5.1232 - f1: 0.0742 - val_loss: 4.9799 - val_f1: 0.0781\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 4.7693 - f1: 0.0991 - val_loss: 4.7414 - val_f1: 0.0892\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 4.5133 - f1: 0.0917 - val_loss: 4.4610 - val_f1: 0.1024\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 508us/sample - loss: 4.2089 - f1: 0.1184 - val_loss: 4.2648 - val_f1: 0.1168\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 4.0652 - f1: 0.1272 - val_loss: 4.0789 - val_f1: 0.1227\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 3.8860 - f1: 0.1262 - val_loss: 3.9418 - val_f1: 0.0974\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 508us/sample - loss: 3.7111 - f1: 0.1353 - val_loss: 3.7797 - val_f1: 0.1314\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 3.5587 - f1: 0.1585 - val_loss: 3.6203 - val_f1: 0.1410\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 3.3972 - f1: 0.1780 - val_loss: 3.4385 - val_f1: 0.1342\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 508us/sample - loss: 3.2425 - f1: 0.2053 - val_loss: 3.3092 - val_f1: 0.1601\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 3.1517 - f1: 0.2011 - val_loss: 3.1245 - val_f1: 0.1971\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 2.9354 - f1: 0.2220 - val_loss: 3.1211 - val_f1: 0.1632\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 2.8378 - f1: 0.2597 - val_loss: 2.8976 - val_f1: 0.2060\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 2.6886 - f1: 0.2621 - val_loss: 2.7589 - val_f1: 0.2481\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 2.5438 - f1: 0.3106 - val_loss: 2.6985 - val_f1: 0.2439\n",
      "Epoch 25/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 502us/sample - loss: 2.3865 - f1: 0.3128 - val_loss: 2.4236 - val_f1: 0.2856\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 2.3445 - f1: 0.3156 - val_loss: 2.3955 - val_f1: 0.3080\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 2.1369 - f1: 0.3777 - val_loss: 2.2217 - val_f1: 0.3458\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 515us/sample - loss: 2.1705 - f1: 0.3421 - val_loss: 2.1389 - val_f1: 0.3339\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 2.0433 - f1: 0.4153 - val_loss: 2.0771 - val_f1: 0.3779\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 1.9112 - f1: 0.4494 - val_loss: 1.9831 - val_f1: 0.4189\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.7901 - f1: 0.5031 - val_loss: 1.9056 - val_f1: 0.4438\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 511us/sample - loss: 1.7691 - f1: 0.5034 - val_loss: 1.9015 - val_f1: 0.4279\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.7258 - f1: 0.5135 - val_loss: 1.7875 - val_f1: 0.4700\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 1.7231 - f1: 0.5283 - val_loss: 1.8313 - val_f1: 0.4340\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 1.6573 - f1: 0.5237 - val_loss: 1.8753 - val_f1: 0.4590\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.6143 - f1: 0.5336 - val_loss: 1.6955 - val_f1: 0.5132\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.5721 - f1: 0.5708 - val_loss: 1.6937 - val_f1: 0.4946\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.6064 - f1: 0.5506 - val_loss: 1.5935 - val_f1: 0.5085\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.4657 - f1: 0.5911 - val_loss: 1.5286 - val_f1: 0.5553\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 1.5381 - f1: 0.5776 - val_loss: 1.6639 - val_f1: 0.5290\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.4467 - f1: 0.6135 - val_loss: 1.5147 - val_f1: 0.5485\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 1.4312 - f1: 0.5882 - val_loss: 1.4943 - val_f1: 0.5606\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 1.3545 - f1: 0.6193 - val_loss: 1.4221 - val_f1: 0.6019\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 1.4239 - f1: 0.6045 - val_loss: 1.4798 - val_f1: 0.5537\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 1.3377 - f1: 0.6356 - val_loss: 1.3937 - val_f1: 0.6104\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 1.3285 - f1: 0.6415 - val_loss: 1.3825 - val_f1: 0.6161\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 508us/sample - loss: 1.2561 - f1: 0.6728 - val_loss: 1.3878 - val_f1: 0.6127\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.3252 - f1: 0.6744 - val_loss: 1.3377 - val_f1: 0.5839\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 1.2392 - f1: 0.6631 - val_loss: 1.3682 - val_f1: 0.6164\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.2512 - f1: 0.6472 - val_loss: 1.3171 - val_f1: 0.6194\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 1.2637 - f1: 0.6483 - val_loss: 1.3027 - val_f1: 0.6261\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.2180 - f1: 0.6740 - val_loss: 1.2760 - val_f1: 0.6233\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.2180 - f1: 0.6597 - val_loss: 1.2920 - val_f1: 0.6197\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 1.1630 - f1: 0.7004 - val_loss: 1.2366 - val_f1: 0.6522\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 1.1050 - f1: 0.6918 - val_loss: 1.2126 - val_f1: 0.6534\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 1.1738 - f1: 0.6673 - val_loss: 1.2321 - val_f1: 0.6361\n",
      "Epoch 57/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 1.1324 - f1: 0.7084 - val_loss: 1.2110 - val_f1: 0.6566\n",
      "Epoch 58/2000\n",
      "500/500 [==============================] - 0s 513us/sample - loss: 1.1458 - f1: 0.7079 - val_loss: 1.1642 - val_f1: 0.6734\n",
      "Epoch 59/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 1.1609 - f1: 0.6864 - val_loss: 1.2545 - val_f1: 0.6519\n",
      "Epoch 60/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.1183 - f1: 0.7115 - val_loss: 1.1643 - val_f1: 0.6686\n",
      "Epoch 61/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 1.1101 - f1: 0.7029 - val_loss: 1.2183 - val_f1: 0.6447\n",
      "Epoch 62/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 1.0265 - f1: 0.7382 - val_loss: 1.1227 - val_f1: 0.6881\n",
      "Epoch 63/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 1.0274 - f1: 0.7512 - val_loss: 1.1689 - val_f1: 0.6584\n",
      "Epoch 64/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 1.0687 - f1: 0.7327 - val_loss: 1.0947 - val_f1: 0.7071\n",
      "Epoch 65/2000\n",
      "500/500 [==============================] - 0s 515us/sample - loss: 1.0386 - f1: 0.7437 - val_loss: 1.0979 - val_f1: 0.6987\n",
      "Epoch 66/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 0.9804 - f1: 0.7407 - val_loss: 1.0651 - val_f1: 0.7129\n",
      "Epoch 67/2000\n",
      "500/500 [==============================] - 0s 509us/sample - loss: 0.9487 - f1: 0.7750 - val_loss: 1.0803 - val_f1: 0.6954\n",
      "Epoch 68/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 0.9875 - f1: 0.7647 - val_loss: 1.0770 - val_f1: 0.6974\n",
      "Epoch 69/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 0.9330 - f1: 0.7457 - val_loss: 1.0392 - val_f1: 0.7338\n",
      "Epoch 70/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 0.9947 - f1: 0.7512 - val_loss: 1.0792 - val_f1: 0.6881\n",
      "Epoch 71/2000\n",
      "500/500 [==============================] - 0s 509us/sample - loss: 0.9803 - f1: 0.7310 - val_loss: 1.0382 - val_f1: 0.7042\n",
      "Epoch 72/2000\n",
      "500/500 [==============================] - 0s 522us/sample - loss: 1.0159 - f1: 0.7315 - val_loss: 1.1275 - val_f1: 0.6665\n",
      "Epoch 73/2000\n",
      "500/500 [==============================] - 0s 511us/sample - loss: 1.0937 - f1: 0.7021 - val_loss: 1.0411 - val_f1: 0.7092\n",
      "Epoch 74/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 0.9885 - f1: 0.7593 - val_loss: 1.0503 - val_f1: 0.7116\n",
      "Epoch 75/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 0.9329 - f1: 0.7636 - val_loss: 1.0325 - val_f1: 0.7160\n",
      "Epoch 76/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.0123 - f1: 0.7455 - val_loss: 1.0724 - val_f1: 0.6695\n",
      "Epoch 77/2000\n",
      "500/500 [==============================] - 0s 510us/sample - loss: 0.9892 - f1: 0.7524 - val_loss: 1.0700 - val_f1: 0.7134\n",
      "Epoch 78/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 0.9758 - f1: 0.7575 - val_loss: 1.0842 - val_f1: 0.6809\n",
      "Epoch 79/2000\n",
      "500/500 [==============================] - 3s 6ms/sample - loss: 1.0148 - f1: 0.7326 - val_loss: 1.0315 - val_f1: 0.7024\n",
      "Running through fold 3\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 3s 5ms/sample - loss: 23.5306 - f1: 0.0345 - val_loss: 18.4407 - val_f1: 0.0276\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 507us/sample - loss: 15.3255 - f1: 0.0334 - val_loss: 12.5027 - val_f1: 0.0058\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 11.4797 - f1: 0.0076 - val_loss: 10.3506 - val_f1: 0.0045\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 9.5630 - f1: 0.0152 - val_loss: 8.7577 - val_f1: 0.0045\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 8.1964 - f1: 0.0182 - val_loss: 7.6293 - val_f1: 0.0063\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 7.1748 - f1: 0.0252 - val_loss: 6.7233 - val_f1: 0.0362\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 513us/sample - loss: 6.4064 - f1: 0.0481 - val_loss: 6.0862 - val_f1: 0.0666\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 5.8050 - f1: 0.0836 - val_loss: 5.5121 - val_f1: 0.0734\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 512us/sample - loss: 5.2496 - f1: 0.1038 - val_loss: 5.1240 - val_f1: 0.1006\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 4.8828 - f1: 0.1170 - val_loss: 4.7875 - val_f1: 0.1069\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 4.6165 - f1: 0.1109 - val_loss: 4.4156 - val_f1: 0.1131\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 517us/sample - loss: 4.3699 - f1: 0.1459 - val_loss: 4.2378 - val_f1: 0.1511\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 4.0467 - f1: 0.1710 - val_loss: 4.0175 - val_f1: 0.1458\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 3.8541 - f1: 0.1779 - val_loss: 3.8842 - val_f1: 0.1568\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 510us/sample - loss: 3.6515 - f1: 0.2078 - val_loss: 3.5718 - val_f1: 0.1775\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 3.3742 - f1: 0.2365 - val_loss: 3.4161 - val_f1: 0.1857\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 3.1841 - f1: 0.3032 - val_loss: 3.1922 - val_f1: 0.2303\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 521us/sample - loss: 3.0160 - f1: 0.3037 - val_loss: 3.0542 - val_f1: 0.2349\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 2.8404 - f1: 0.3211 - val_loss: 2.9476 - val_f1: 0.2397\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 508us/sample - loss: 2.6988 - f1: 0.3341 - val_loss: 2.7785 - val_f1: 0.2917\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 510us/sample - loss: 2.5841 - f1: 0.3448 - val_loss: 2.6667 - val_f1: 0.3308\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 2.4897 - f1: 0.3641 - val_loss: 2.5819 - val_f1: 0.3306\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 2.3961 - f1: 0.4155 - val_loss: 2.5039 - val_f1: 0.3297\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 510us/sample - loss: 2.2971 - f1: 0.4161 - val_loss: 2.3766 - val_f1: 0.3666\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 509us/sample - loss: 2.1165 - f1: 0.4535 - val_loss: 2.2598 - val_f1: 0.4009\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 2.0576 - f1: 0.4918 - val_loss: 2.1955 - val_f1: 0.3971\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 2.0038 - f1: 0.5046 - val_loss: 2.1006 - val_f1: 0.4303\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 1.8962 - f1: 0.5017 - val_loss: 1.9938 - val_f1: 0.4633\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 513us/sample - loss: 1.8373 - f1: 0.5301 - val_loss: 1.9563 - val_f1: 0.4624\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 510us/sample - loss: 1.7836 - f1: 0.5649 - val_loss: 1.8943 - val_f1: 0.4755\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 507us/sample - loss: 1.8614 - f1: 0.5145 - val_loss: 1.8897 - val_f1: 0.4781\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 507us/sample - loss: 1.7297 - f1: 0.5605 - val_loss: 1.8737 - val_f1: 0.4931\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.6616 - f1: 0.5669 - val_loss: 1.7341 - val_f1: 0.5343\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 1.5267 - f1: 0.6170 - val_loss: 1.6777 - val_f1: 0.5646\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 510us/sample - loss: 1.5405 - f1: 0.6169 - val_loss: 1.7315 - val_f1: 0.5379\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 507us/sample - loss: 1.5505 - f1: 0.6033 - val_loss: 1.6367 - val_f1: 0.5504\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 1.4192 - f1: 0.6514 - val_loss: 1.4582 - val_f1: 0.6312\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 1.3374 - f1: 0.6763 - val_loss: 1.4077 - val_f1: 0.6344\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 507us/sample - loss: 1.2910 - f1: 0.7011 - val_loss: 1.4389 - val_f1: 0.6336\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.2728 - f1: 0.7201 - val_loss: 1.4184 - val_f1: 0.6628\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.2655 - f1: 0.7066 - val_loss: 1.3742 - val_f1: 0.6558\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 1.2222 - f1: 0.7139 - val_loss: 1.2763 - val_f1: 0.7028\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 508us/sample - loss: 1.1450 - f1: 0.7490 - val_loss: 1.2504 - val_f1: 0.7141\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.0893 - f1: 0.7830 - val_loss: 1.2222 - val_f1: 0.7236\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 504us/sample - loss: 1.1677 - f1: 0.7530 - val_loss: 1.2393 - val_f1: 0.6960\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 1.1270 - f1: 0.7727 - val_loss: 1.1612 - val_f1: 0.7393\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 1.0541 - f1: 0.7938 - val_loss: 1.1343 - val_f1: 0.7482\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 1.0093 - f1: 0.8025 - val_loss: 1.1112 - val_f1: 0.7562\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 1.0024 - f1: 0.8064 - val_loss: 1.1003 - val_f1: 0.7528\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 1.0011 - f1: 0.7997 - val_loss: 1.1048 - val_f1: 0.7531\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 0.9599 - f1: 0.7935 - val_loss: 1.1289 - val_f1: 0.7514\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 0.9742 - f1: 0.8052 - val_loss: 1.1324 - val_f1: 0.7309\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 495us/sample - loss: 0.9426 - f1: 0.8058 - val_loss: 1.2044 - val_f1: 0.7153\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 0.9899 - f1: 0.8140 - val_loss: 1.1063 - val_f1: 0.7372\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 0.9449 - f1: 0.8104 - val_loss: 1.0160 - val_f1: 0.7876\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 0.8582 - f1: 0.8350 - val_loss: 1.0096 - val_f1: 0.7896\n",
      "Epoch 57/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 0.9261 - f1: 0.8102 - val_loss: 1.0283 - val_f1: 0.7713\n",
      "Epoch 58/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 0.8547 - f1: 0.8403 - val_loss: 1.0417 - val_f1: 0.7563\n",
      "Epoch 59/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 0.9317 - f1: 0.8164 - val_loss: 1.0022 - val_f1: 0.7738\n",
      "Epoch 60/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 0.9203 - f1: 0.8268 - val_loss: 1.1071 - val_f1: 0.7466\n",
      "Epoch 61/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 0.9053 - f1: 0.8177 - val_loss: 0.9649 - val_f1: 0.7944\n",
      "Epoch 62/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 0.8669 - f1: 0.8327 - val_loss: 1.0093 - val_f1: 0.7690\n",
      "Epoch 63/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 0.9230 - f1: 0.8078 - val_loss: 1.0447 - val_f1: 0.7509\n",
      "Epoch 64/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 0.8871 - f1: 0.8133 - val_loss: 0.9542 - val_f1: 0.8027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/2000\n",
      "500/500 [==============================] - 0s 503us/sample - loss: 0.8754 - f1: 0.8154 - val_loss: 0.9431 - val_f1: 0.7984\n",
      "Epoch 66/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 0.8146 - f1: 0.8477 - val_loss: 0.9640 - val_f1: 0.7829\n",
      "Epoch 67/2000\n",
      "500/500 [==============================] - 0s 508us/sample - loss: 0.8089 - f1: 0.8378 - val_loss: 0.9148 - val_f1: 0.8116\n",
      "Epoch 68/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 0.7919 - f1: 0.8692 - val_loss: 0.9226 - val_f1: 0.7908\n",
      "Epoch 69/2000\n",
      "500/500 [==============================] - 0s 498us/sample - loss: 0.8537 - f1: 0.8295 - val_loss: 0.9105 - val_f1: 0.8081\n",
      "Epoch 70/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 0.8523 - f1: 0.8222 - val_loss: 0.9112 - val_f1: 0.7997\n",
      "Epoch 71/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 0.7882 - f1: 0.8489 - val_loss: 0.8809 - val_f1: 0.8111\n",
      "Epoch 72/2000\n",
      "500/500 [==============================] - 0s 506us/sample - loss: 0.8373 - f1: 0.8259 - val_loss: 0.8906 - val_f1: 0.8008\n",
      "Epoch 73/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 0.7270 - f1: 0.8639 - val_loss: 0.8821 - val_f1: 0.8127\n",
      "Epoch 74/2000\n",
      "500/500 [==============================] - 3s 6ms/sample - loss: 0.7606 - f1: 0.8577 - val_loss: 0.8759 - val_f1: 0.8078\n",
      "Running through fold 4\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 3s 6ms/sample - loss: 23.2234 - f1: 0.0271 - val_loss: 17.2596 - val_f1: 0.0156\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 502us/sample - loss: 14.6999 - f1: 0.0110 - val_loss: 12.7166 - val_f1: 0.0000e+00\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 11.6876 - f1: 0.0000e+00 - val_loss: 10.5939 - val_f1: 0.0000e+00\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 9.8477 - f1: 0.0000e+00 - val_loss: 9.0619 - val_f1: 0.0051\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 8.5064 - f1: 0.0000e+00 - val_loss: 7.9724 - val_f1: 0.0026\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 7.5611 - f1: 0.0000e+00 - val_loss: 7.1472 - val_f1: 0.0096\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 6.8312 - f1: 0.0149 - val_loss: 6.5203 - val_f1: 0.0169\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 6.2731 - f1: 0.0335 - val_loss: 6.0306 - val_f1: 0.0246\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 5.8314 - f1: 0.0110 - val_loss: 5.7022 - val_f1: 0.0135\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 5.4719 - f1: 0.0336 - val_loss: 5.3213 - val_f1: 0.0407\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 5.1619 - f1: 0.0625 - val_loss: 5.0270 - val_f1: 0.0614\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 4.8946 - f1: 0.0660 - val_loss: 4.8084 - val_f1: 0.0497\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 4.7087 - f1: 0.0464 - val_loss: 4.5593 - val_f1: 0.0516\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 4.4555 - f1: 0.0561 - val_loss: 4.3350 - val_f1: 0.0588\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 4.2031 - f1: 0.0820 - val_loss: 4.1358 - val_f1: 0.0884\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 3.9949 - f1: 0.0866 - val_loss: 3.9526 - val_f1: 0.0905\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 494us/sample - loss: 3.7865 - f1: 0.0887 - val_loss: 3.7308 - val_f1: 0.0787\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 3.6351 - f1: 0.0813 - val_loss: 3.5946 - val_f1: 0.0862\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 3.5353 - f1: 0.0968 - val_loss: 3.5290 - val_f1: 0.0989\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 3.3586 - f1: 0.1140 - val_loss: 3.3289 - val_f1: 0.0890\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 3.2437 - f1: 0.1000 - val_loss: 3.2439 - val_f1: 0.0983\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 3.1299 - f1: 0.1577 - val_loss: 3.0426 - val_f1: 0.1451\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 3.0899 - f1: 0.1455 - val_loss: 3.2484 - val_f1: 0.0914\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 2.9871 - f1: 0.1585 - val_loss: 2.9745 - val_f1: 0.1664\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 2.8106 - f1: 0.1922 - val_loss: 2.8024 - val_f1: 0.1955\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 2.7559 - f1: 0.1599 - val_loss: 2.7136 - val_f1: 0.1588\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 2.6146 - f1: 0.1710 - val_loss: 2.6371 - val_f1: 0.1692\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 2.4965 - f1: 0.2131 - val_loss: 2.5686 - val_f1: 0.2102\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 2.4534 - f1: 0.2199 - val_loss: 2.4369 - val_f1: 0.1976\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 535us/sample - loss: 2.4053 - f1: 0.2253 - val_loss: 2.3651 - val_f1: 0.2308\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 675us/sample - loss: 2.3255 - f1: 0.2511 - val_loss: 2.2980 - val_f1: 0.2429\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 514us/sample - loss: 2.1738 - f1: 0.2622 - val_loss: 2.1429 - val_f1: 0.2586\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 2.0320 - f1: 0.3315 - val_loss: 2.0589 - val_f1: 0.2847\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.9671 - f1: 0.3217 - val_loss: 2.0157 - val_f1: 0.3285\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.9343 - f1: 0.3363 - val_loss: 1.9919 - val_f1: 0.3366\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 1.9099 - f1: 0.3676 - val_loss: 1.9514 - val_f1: 0.3702\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 1.7695 - f1: 0.4573 - val_loss: 1.7640 - val_f1: 0.4335\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 1.6944 - f1: 0.4947 - val_loss: 1.7037 - val_f1: 0.4496\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 1.7205 - f1: 0.4475 - val_loss: 1.6508 - val_f1: 0.4778\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 1.6292 - f1: 0.5161 - val_loss: 1.6591 - val_f1: 0.4936\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 1.6119 - f1: 0.4830 - val_loss: 1.6590 - val_f1: 0.4768\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 1.5387 - f1: 0.5601 - val_loss: 1.5436 - val_f1: 0.5088\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 481us/sample - loss: 1.4942 - f1: 0.5649 - val_loss: 1.4959 - val_f1: 0.5408\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 479us/sample - loss: 1.3982 - f1: 0.6021 - val_loss: 1.4930 - val_f1: 0.5330\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.3887 - f1: 0.5867 - val_loss: 1.4261 - val_f1: 0.5588\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 485us/sample - loss: 1.3690 - f1: 0.6000 - val_loss: 1.3878 - val_f1: 0.5879\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 483us/sample - loss: 1.3320 - f1: 0.6266 - val_loss: 1.3939 - val_f1: 0.5783\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 1.3596 - f1: 0.6213 - val_loss: 1.3834 - val_f1: 0.5869\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 1.3330 - f1: 0.6039 - val_loss: 1.3475 - val_f1: 0.6095\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 1.3197 - f1: 0.6402 - val_loss: 1.3470 - val_f1: 0.6087\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 487us/sample - loss: 1.2449 - f1: 0.6563 - val_loss: 1.2903 - val_f1: 0.6240\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 1.1890 - f1: 0.7002 - val_loss: 1.2844 - val_f1: 0.6261\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 1.2248 - f1: 0.6811 - val_loss: 1.3255 - val_f1: 0.6030\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 1.2724 - f1: 0.6291 - val_loss: 1.2527 - val_f1: 0.6349\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 1.1780 - f1: 0.6867 - val_loss: 1.2214 - val_f1: 0.6501\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 533us/sample - loss: 1.1860 - f1: 0.6883 - val_loss: 1.2335 - val_f1: 0.6429\n",
      "Epoch 57/2000\n",
      "500/500 [==============================] - 0s 480us/sample - loss: 1.1259 - f1: 0.6947 - val_loss: 1.2357 - val_f1: 0.6378\n",
      "Epoch 58/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 1.1529 - f1: 0.6881 - val_loss: 1.2028 - val_f1: 0.6708\n",
      "Epoch 59/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 1.1834 - f1: 0.7000 - val_loss: 1.1525 - val_f1: 0.6673\n",
      "Epoch 60/2000\n",
      "500/500 [==============================] - 0s 481us/sample - loss: 1.1577 - f1: 0.6946 - val_loss: 1.1751 - val_f1: 0.6544\n",
      "Epoch 61/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 1.0960 - f1: 0.7030 - val_loss: 1.1312 - val_f1: 0.6810\n",
      "Epoch 62/2000\n",
      "500/500 [==============================] - 0s 494us/sample - loss: 1.0870 - f1: 0.7032 - val_loss: 1.1450 - val_f1: 0.6845\n",
      "Epoch 63/2000\n",
      "500/500 [==============================] - 0s 481us/sample - loss: 1.0222 - f1: 0.7305 - val_loss: 1.1266 - val_f1: 0.6863\n",
      "Epoch 64/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 1.0021 - f1: 0.7461 - val_loss: 1.0997 - val_f1: 0.6977\n",
      "Epoch 65/2000\n",
      "500/500 [==============================] - 0s 482us/sample - loss: 1.0436 - f1: 0.7480 - val_loss: 1.0721 - val_f1: 0.7046\n",
      "Epoch 66/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 1.0110 - f1: 0.7434 - val_loss: 1.0836 - val_f1: 0.6963\n",
      "Epoch 67/2000\n",
      "500/500 [==============================] - 0s 488us/sample - loss: 1.0459 - f1: 0.7275 - val_loss: 1.1054 - val_f1: 0.7040\n",
      "Epoch 68/2000\n",
      "500/500 [==============================] - 0s 492us/sample - loss: 1.0313 - f1: 0.7374 - val_loss: 1.0829 - val_f1: 0.7094\n",
      "Epoch 69/2000\n",
      "500/500 [==============================] - 0s 496us/sample - loss: 1.0300 - f1: 0.7330 - val_loss: 1.0886 - val_f1: 0.6958\n",
      "Epoch 70/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 1.0643 - f1: 0.7362 - val_loss: 1.0755 - val_f1: 0.7079\n",
      "Epoch 71/2000\n",
      "500/500 [==============================] - 0s 516us/sample - loss: 1.0338 - f1: 0.7334 - val_loss: 1.0530 - val_f1: 0.7220\n",
      "Epoch 72/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 1.0546 - f1: 0.7222 - val_loss: 1.0735 - val_f1: 0.7053\n",
      "Epoch 73/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 0.9528 - f1: 0.7705 - val_loss: 1.0131 - val_f1: 0.7381\n",
      "Epoch 74/2000\n",
      "500/500 [==============================] - 0s 499us/sample - loss: 0.9760 - f1: 0.7580 - val_loss: 1.0594 - val_f1: 0.7113\n",
      "Epoch 75/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 0.9559 - f1: 0.7587 - val_loss: 1.0833 - val_f1: 0.6904\n",
      "Epoch 76/2000\n",
      "500/500 [==============================] - 0s 505us/sample - loss: 0.9535 - f1: 0.7474 - val_loss: 0.9899 - val_f1: 0.7512\n",
      "Epoch 77/2000\n",
      "500/500 [==============================] - 0s 497us/sample - loss: 1.0335 - f1: 0.7287 - val_loss: 1.1715 - val_f1: 0.6714\n",
      "Epoch 78/2000\n",
      "500/500 [==============================] - 0s 501us/sample - loss: 0.9514 - f1: 0.7546 - val_loss: 1.0276 - val_f1: 0.7433\n",
      "Epoch 79/2000\n",
      "500/500 [==============================] - 0s 500us/sample - loss: 0.9446 - f1: 0.7747 - val_loss: 0.9945 - val_f1: 0.7516\n",
      "Epoch 80/2000\n",
      "500/500 [==============================] - 0s 491us/sample - loss: 0.9725 - f1: 0.7630 - val_loss: 1.0218 - val_f1: 0.7227\n",
      "Epoch 81/2000\n",
      "500/500 [==============================] - 0s 489us/sample - loss: 0.9412 - f1: 0.7606 - val_loss: 1.0539 - val_f1: 0.7316\n",
      "Epoch 82/2000\n",
      "500/500 [==============================] - 0s 493us/sample - loss: 0.9624 - f1: 0.7542 - val_loss: 1.0795 - val_f1: 0.6984\n",
      "Epoch 83/2000\n",
      "500/500 [==============================] - 0s 490us/sample - loss: 0.9681 - f1: 0.7518 - val_loss: 0.9917 - val_f1: 0.7444\n",
      "Epoch 84/2000\n",
      "500/500 [==============================] - 0s 486us/sample - loss: 0.8939 - f1: 0.7986 - val_loss: 1.0688 - val_f1: 0.7352\n",
      "Epoch 85/2000\n",
      "500/500 [==============================] - 0s 484us/sample - loss: 0.9496 - f1: 0.7631 - val_loss: 0.9795 - val_f1: 0.7479\n",
      "Epoch 86/2000\n",
      "500/500 [==============================] - 3s 6ms/sample - loss: 0.9162 - f1: 0.7553 - val_loss: 1.0451 - val_f1: 0.7231\n",
      "\n",
      "\n",
      "Running through training size 1000\n",
      "Running through fold 0\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 18.9418 - f1: 0.0232 - val_loss: 12.6380 - val_f1: 0.0000e+00\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 10.6250 - f1: 0.0076 - val_loss: 8.7851 - val_f1: 0.0134\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 7.7558 - f1: 0.0467 - val_loss: 6.9410 - val_f1: 0.0533\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 6.1955 - f1: 0.1241 - val_loss: 5.5669 - val_f1: 0.1396\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 5.1462 - f1: 0.1323 - val_loss: 4.7593 - val_f1: 0.1577\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 4.3785 - f1: 0.2297 - val_loss: 4.2224 - val_f1: 0.2096\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 3.8017 - f1: 0.2517 - val_loss: 3.6371 - val_f1: 0.2173\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 3.4435 - f1: 0.2886 - val_loss: 3.2062 - val_f1: 0.2778\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 3.0061 - f1: 0.3618 - val_loss: 2.9320 - val_f1: 0.3530\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 2.7253 - f1: 0.3991 - val_loss: 2.5620 - val_f1: 0.4077\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 2.4909 - f1: 0.4410 - val_loss: 2.3644 - val_f1: 0.4433\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 2.2597 - f1: 0.4927 - val_loss: 2.2020 - val_f1: 0.4916\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 2.0924 - f1: 0.5315 - val_loss: 2.0343 - val_f1: 0.5400\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 2.0074 - f1: 0.5620 - val_loss: 2.0549 - val_f1: 0.4923\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 1.8954 - f1: 0.5563 - val_loss: 1.7750 - val_f1: 0.5879\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.7316 - f1: 0.6203 - val_loss: 1.6873 - val_f1: 0.5983\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.6539 - f1: 0.6095 - val_loss: 1.6515 - val_f1: 0.5814\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.6380 - f1: 0.5971 - val_loss: 1.5471 - val_f1: 0.6233\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 1.6235 - f1: 0.6002 - val_loss: 1.5181 - val_f1: 0.6244\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 1.4557 - f1: 0.6539 - val_loss: 1.4351 - val_f1: 0.6497\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 1.4218 - f1: 0.6571 - val_loss: 1.3647 - val_f1: 0.6692\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 1.4166 - f1: 0.6678 - val_loss: 1.3834 - val_f1: 0.6517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.2922 - f1: 0.6993 - val_loss: 1.2804 - val_f1: 0.6763\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.2704 - f1: 0.6784 - val_loss: 1.3447 - val_f1: 0.6432\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 1.2608 - f1: 0.6825 - val_loss: 1.2251 - val_f1: 0.6797\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.2205 - f1: 0.6881 - val_loss: 1.1367 - val_f1: 0.7201\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 1.1835 - f1: 0.6955 - val_loss: 1.2018 - val_f1: 0.6778\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 1.1682 - f1: 0.7079 - val_loss: 1.1222 - val_f1: 0.7186\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 1.1209 - f1: 0.7247 - val_loss: 1.1620 - val_f1: 0.6937\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 1.1288 - f1: 0.7208 - val_loss: 1.0643 - val_f1: 0.7272\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.1119 - f1: 0.7290 - val_loss: 1.0751 - val_f1: 0.7108\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.0236 - f1: 0.7345 - val_loss: 1.0458 - val_f1: 0.7221\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 1.0912 - f1: 0.7228 - val_loss: 1.0159 - val_f1: 0.7358\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 1.0160 - f1: 0.7393 - val_loss: 1.0388 - val_f1: 0.7413\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.0055 - f1: 0.7319 - val_loss: 1.0787 - val_f1: 0.6969\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 1.0155 - f1: 0.7347 - val_loss: 0.9417 - val_f1: 0.7581\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 1.0235 - f1: 0.7507 - val_loss: 1.0464 - val_f1: 0.7207\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.9775 - f1: 0.7405 - val_loss: 0.9548 - val_f1: 0.7445\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.9114 - f1: 0.7706 - val_loss: 0.9256 - val_f1: 0.7652\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.9300 - f1: 0.7728 - val_loss: 0.9481 - val_f1: 0.7451\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.9014 - f1: 0.7812 - val_loss: 0.9126 - val_f1: 0.7529\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.9180 - f1: 0.7675 - val_loss: 0.9286 - val_f1: 0.7523\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.8961 - f1: 0.7825 - val_loss: 1.1389 - val_f1: 0.6574\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 0.9582 - f1: 0.7537 - val_loss: 0.8783 - val_f1: 0.7734\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.8411 - f1: 0.8065 - val_loss: 0.8669 - val_f1: 0.7670\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 0.9487 - f1: 0.7542 - val_loss: 0.8667 - val_f1: 0.7719\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.8538 - f1: 0.7886 - val_loss: 0.8469 - val_f1: 0.7882\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.8836 - f1: 0.7680 - val_loss: 0.9216 - val_f1: 0.7714\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 0.8621 - f1: 0.7965 - val_loss: 0.8666 - val_f1: 0.7697\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.8708 - f1: 0.7942 - val_loss: 0.8454 - val_f1: 0.7838\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 0.8219 - f1: 0.8101 - val_loss: 0.8411 - val_f1: 0.7817\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.8460 - f1: 0.7902 - val_loss: 0.7566 - val_f1: 0.8230\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.8558 - f1: 0.7815 - val_loss: 0.7987 - val_f1: 0.7936\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.8552 - f1: 0.7921 - val_loss: 0.7598 - val_f1: 0.8133\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7909 - f1: 0.8143 - val_loss: 0.8441 - val_f1: 0.7850\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.8616 - f1: 0.7604 - val_loss: 0.8475 - val_f1: 0.7649\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.8110 - f1: 0.8051 - val_loss: 0.7375 - val_f1: 0.8277\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.7384 - f1: 0.8363 - val_loss: 0.8203 - val_f1: 0.7803\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.7848 - f1: 0.8124 - val_loss: 0.7305 - val_f1: 0.8216\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7596 - f1: 0.8291 - val_loss: 0.7744 - val_f1: 0.8105\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7144 - f1: 0.8291 - val_loss: 0.8158 - val_f1: 0.7905\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.8028 - f1: 0.7996 - val_loss: 0.7893 - val_f1: 0.8056\n",
      "Running through fold 1\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 20.1819 - f1: 0.0428 - val_loss: 13.4036 - val_f1: 0.0383\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 11.1138 - f1: 0.0154 - val_loss: 9.4603 - val_f1: 0.0407\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 8.2273 - f1: 0.0563 - val_loss: 7.2208 - val_f1: 0.1010\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 6.5763 - f1: 0.0970 - val_loss: 5.8954 - val_f1: 0.1252\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 5.5400 - f1: 0.1249 - val_loss: 5.1215 - val_f1: 0.1661\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 4.7223 - f1: 0.2095 - val_loss: 4.4965 - val_f1: 0.1703\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 4.1635 - f1: 0.2475 - val_loss: 3.9214 - val_f1: 0.2585\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 3.6022 - f1: 0.3095 - val_loss: 3.4498 - val_f1: 0.3154\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 3.2073 - f1: 0.3571 - val_loss: 2.8679 - val_f1: 0.4021\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 2.8558 - f1: 0.4102 - val_loss: 2.7032 - val_f1: 0.4200\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 2.5890 - f1: 0.4607 - val_loss: 2.4808 - val_f1: 0.4121\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 2.3062 - f1: 0.5183 - val_loss: 2.1654 - val_f1: 0.5379\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 2.1297 - f1: 0.5341 - val_loss: 2.0248 - val_f1: 0.5648\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 2.0773 - f1: 0.5454 - val_loss: 1.9503 - val_f1: 0.5503\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 1.9229 - f1: 0.5680 - val_loss: 1.8637 - val_f1: 0.6009\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 1.8000 - f1: 0.6118 - val_loss: 1.7105 - val_f1: 0.5990\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.6747 - f1: 0.6376 - val_loss: 1.5711 - val_f1: 0.6519\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.6267 - f1: 0.6556 - val_loss: 1.5726 - val_f1: 0.6430\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.5443 - f1: 0.6786 - val_loss: 1.5281 - val_f1: 0.6284\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 1.4493 - f1: 0.6712 - val_loss: 1.4540 - val_f1: 0.6581\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.3974 - f1: 0.6842 - val_loss: 1.3079 - val_f1: 0.7172\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.2946 - f1: 0.7238 - val_loss: 1.2426 - val_f1: 0.7258\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.2890 - f1: 0.7100 - val_loss: 1.4477 - val_f1: 0.6388\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.3157 - f1: 0.7128 - val_loss: 1.4109 - val_f1: 0.6078\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.2631 - f1: 0.7108 - val_loss: 1.1733 - val_f1: 0.7386\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.1672 - f1: 0.7436 - val_loss: 1.1404 - val_f1: 0.7489\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 1.1403 - f1: 0.7614 - val_loss: 1.0853 - val_f1: 0.7512\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.0644 - f1: 0.7767 - val_loss: 1.0397 - val_f1: 0.7702\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 1.0795 - f1: 0.7577 - val_loss: 1.0677 - val_f1: 0.7319\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.0608 - f1: 0.7699 - val_loss: 1.0237 - val_f1: 0.7603\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.9957 - f1: 0.7981 - val_loss: 0.9840 - val_f1: 0.7728\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.9698 - f1: 0.7999 - val_loss: 0.9862 - val_f1: 0.7681\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.9773 - f1: 0.7689 - val_loss: 1.0435 - val_f1: 0.7097\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 0.9972 - f1: 0.7808 - val_loss: 0.9440 - val_f1: 0.7788\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.9210 - f1: 0.7972 - val_loss: 0.8921 - val_f1: 0.8051\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 0.8780 - f1: 0.8133 - val_loss: 0.8961 - val_f1: 0.7939\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.9060 - f1: 0.8074 - val_loss: 0.8953 - val_f1: 0.7919\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.8775 - f1: 0.8148 - val_loss: 0.8914 - val_f1: 0.7964\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.8747 - f1: 0.8016 - val_loss: 0.9201 - val_f1: 0.7746\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.8861 - f1: 0.8055 - val_loss: 0.8675 - val_f1: 0.7942\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.8222 - f1: 0.8443 - val_loss: 0.7887 - val_f1: 0.8287\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 0.8573 - f1: 0.8116 - val_loss: 0.8208 - val_f1: 0.8179\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 0.8226 - f1: 0.8334 - val_loss: 0.8007 - val_f1: 0.8283\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7882 - f1: 0.8321 - val_loss: 0.8423 - val_f1: 0.8100\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.8118 - f1: 0.8300 - val_loss: 0.8237 - val_f1: 0.8131\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.8369 - f1: 0.8120 - val_loss: 0.8148 - val_f1: 0.8056\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 0.7494 - f1: 0.8579 - val_loss: 0.8011 - val_f1: 0.8232\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.7480 - f1: 0.8441 - val_loss: 0.7398 - val_f1: 0.8484\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 0.8083 - f1: 0.8131 - val_loss: 0.8349 - val_f1: 0.8042\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.8578 - f1: 0.8028 - val_loss: 0.7808 - val_f1: 0.8368\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.8577 - f1: 0.8239 - val_loss: 0.8923 - val_f1: 0.7691\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.7867 - f1: 0.8263 - val_loss: 0.7439 - val_f1: 0.8344\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 0.8107 - f1: 0.8354 - val_loss: 0.7582 - val_f1: 0.8327\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.7973 - f1: 0.8143 - val_loss: 0.6949 - val_f1: 0.8637\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.6900 - f1: 0.8658 - val_loss: 0.7210 - val_f1: 0.8452\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.6527 - f1: 0.8768 - val_loss: 0.6840 - val_f1: 0.8587\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.7090 - f1: 0.8567 - val_loss: 0.7233 - val_f1: 0.8377\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7224 - f1: 0.8504 - val_loss: 0.6946 - val_f1: 0.8505\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.7228 - f1: 0.8555 - val_loss: 0.7388 - val_f1: 0.8314\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.7199 - f1: 0.8582 - val_loss: 0.6896 - val_f1: 0.8591\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.6933 - f1: 0.8666 - val_loss: 0.7983 - val_f1: 0.8104\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7411 - f1: 0.8542 - val_loss: 0.6982 - val_f1: 0.8472\n",
      "Epoch 63/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.7822 - f1: 0.8284 - val_loss: 0.8242 - val_f1: 0.8028\n",
      "Epoch 64/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7674 - f1: 0.8337 - val_loss: 0.7832 - val_f1: 0.8118\n",
      "Running through fold 2\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 18.2336 - f1: 0.0211 - val_loss: 12.3972 - val_f1: 0.0000e+00\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 10.3672 - f1: 0.0184 - val_loss: 8.5481 - val_f1: 0.0189\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 7.4827 - f1: 0.0578 - val_loss: 6.4868 - val_f1: 0.1083\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 5.8715 - f1: 0.1077 - val_loss: 5.2586 - val_f1: 0.1283\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 4.9130 - f1: 0.1802 - val_loss: 4.5946 - val_f1: 0.1897\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 4.3316 - f1: 0.1847 - val_loss: 4.0409 - val_f1: 0.2006\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 3.7946 - f1: 0.2139 - val_loss: 3.6043 - val_f1: 0.2247\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 3.3748 - f1: 0.2302 - val_loss: 3.3393 - val_f1: 0.2055\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 3.0952 - f1: 0.2602 - val_loss: 2.9315 - val_f1: 0.2736\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 2.6941 - f1: 0.3449 - val_loss: 2.5540 - val_f1: 0.3507\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 2.5116 - f1: 0.3620 - val_loss: 2.3562 - val_f1: 0.3662\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 2.2709 - f1: 0.3915 - val_loss: 2.3042 - val_f1: 0.3703\n",
      "Epoch 13/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 315us/sample - loss: 2.1928 - f1: 0.4267 - val_loss: 2.1374 - val_f1: 0.4385\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 2.0527 - f1: 0.4938 - val_loss: 1.9396 - val_f1: 0.4940\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 1.9280 - f1: 0.5123 - val_loss: 1.8490 - val_f1: 0.5295\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 1.7419 - f1: 0.5773 - val_loss: 1.9251 - val_f1: 0.5398\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.6812 - f1: 0.5863 - val_loss: 1.6222 - val_f1: 0.5888\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 1.5484 - f1: 0.6347 - val_loss: 1.6122 - val_f1: 0.5618\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.5095 - f1: 0.6354 - val_loss: 1.4505 - val_f1: 0.6298\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 1.4172 - f1: 0.6637 - val_loss: 1.3693 - val_f1: 0.6541\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 1.4158 - f1: 0.6563 - val_loss: 1.3851 - val_f1: 0.6668\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 1.3526 - f1: 0.6838 - val_loss: 1.2530 - val_f1: 0.6975\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 1.3080 - f1: 0.6738 - val_loss: 1.3089 - val_f1: 0.6774\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 1.2310 - f1: 0.7142 - val_loss: 1.2033 - val_f1: 0.6997\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 1.1861 - f1: 0.7096 - val_loss: 1.1159 - val_f1: 0.7276\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 1.1419 - f1: 0.7348 - val_loss: 1.1395 - val_f1: 0.7055\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 1.0919 - f1: 0.7452 - val_loss: 1.0816 - val_f1: 0.7253\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 1.1676 - f1: 0.7230 - val_loss: 1.1114 - val_f1: 0.7312\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 1.0998 - f1: 0.7290 - val_loss: 1.0926 - val_f1: 0.7134\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 1.0541 - f1: 0.7504 - val_loss: 1.0223 - val_f1: 0.7483\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 1.0073 - f1: 0.7663 - val_loss: 1.0847 - val_f1: 0.7211\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 1.0232 - f1: 0.7718 - val_loss: 1.0212 - val_f1: 0.7416\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.9957 - f1: 0.7643 - val_loss: 0.9096 - val_f1: 0.7870\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.9009 - f1: 0.7992 - val_loss: 0.9180 - val_f1: 0.7811\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.9673 - f1: 0.7885 - val_loss: 0.9143 - val_f1: 0.7799\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 309us/sample - loss: 0.8891 - f1: 0.8061 - val_loss: 0.9040 - val_f1: 0.7789\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 0.9266 - f1: 0.7807 - val_loss: 0.9123 - val_f1: 0.7721\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 0.9212 - f1: 0.8027 - val_loss: 0.8354 - val_f1: 0.8013\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.8928 - f1: 0.7918 - val_loss: 0.9357 - val_f1: 0.7490\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 0.9164 - f1: 0.7998 - val_loss: 1.0108 - val_f1: 0.7174\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.9091 - f1: 0.7858 - val_loss: 0.9152 - val_f1: 0.7774\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.8657 - f1: 0.8045 - val_loss: 0.8555 - val_f1: 0.7881\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 0.8221 - f1: 0.8051 - val_loss: 0.8923 - val_f1: 0.7806\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.7778 - f1: 0.8234 - val_loss: 0.7685 - val_f1: 0.8265\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 0.7870 - f1: 0.8201 - val_loss: 0.7557 - val_f1: 0.8287\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 347us/sample - loss: 0.7578 - f1: 0.8473 - val_loss: 0.7349 - val_f1: 0.8372\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 421us/sample - loss: 0.7858 - f1: 0.8397 - val_loss: 0.7981 - val_f1: 0.8053\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.7997 - f1: 0.8187 - val_loss: 0.7945 - val_f1: 0.7975\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 0.7412 - f1: 0.8625 - val_loss: 0.7352 - val_f1: 0.8334\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.7543 - f1: 0.8419 - val_loss: 0.6943 - val_f1: 0.8483\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 0.7404 - f1: 0.8369 - val_loss: 0.7055 - val_f1: 0.8501\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7718 - f1: 0.8180 - val_loss: 0.6742 - val_f1: 0.8605\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7118 - f1: 0.8532 - val_loss: 0.7026 - val_f1: 0.8360\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.7087 - f1: 0.8527 - val_loss: 0.6883 - val_f1: 0.8461\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 0.6706 - f1: 0.8590 - val_loss: 0.7032 - val_f1: 0.8437\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.7219 - f1: 0.8411 - val_loss: 0.8166 - val_f1: 0.7740\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 309us/sample - loss: 0.7369 - f1: 0.8342 - val_loss: 0.6832 - val_f1: 0.8449\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 312us/sample - loss: 0.6670 - f1: 0.8666 - val_loss: 0.6495 - val_f1: 0.8648\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 0.6838 - f1: 0.8433 - val_loss: 0.7083 - val_f1: 0.8293\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 314us/sample - loss: 0.7088 - f1: 0.8449 - val_loss: 0.6542 - val_f1: 0.8566\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.6497 - f1: 0.8704 - val_loss: 0.6556 - val_f1: 0.8506\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.6775 - f1: 0.8650 - val_loss: 0.7087 - val_f1: 0.8338\n",
      "Running through fold 3\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 18.7325 - f1: 0.0306 - val_loss: 12.5995 - val_f1: 0.0123\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 10.6535 - f1: 0.0186 - val_loss: 8.8464 - val_f1: 0.0425\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 7.8299 - f1: 0.0274 - val_loss: 6.8136 - val_f1: 0.0564\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 6.2341 - f1: 0.0548 - val_loss: 5.7763 - val_f1: 0.0390\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 5.2914 - f1: 0.0555 - val_loss: 4.9099 - val_f1: 0.0556\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 4.6789 - f1: 0.0511 - val_loss: 4.4278 - val_f1: 0.0327\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 4.1767 - f1: 0.0493 - val_loss: 3.9553 - val_f1: 0.0967\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 3.7490 - f1: 0.1135 - val_loss: 3.6002 - val_f1: 0.1315\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 3.4051 - f1: 0.1434 - val_loss: 3.2855 - val_f1: 0.1048\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 311us/sample - loss: 3.0911 - f1: 0.2109 - val_loss: 2.9894 - val_f1: 0.1434\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 2.8180 - f1: 0.2335 - val_loss: 2.7048 - val_f1: 0.2834\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 2.5824 - f1: 0.2940 - val_loss: 2.4011 - val_f1: 0.2949\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 2.3238 - f1: 0.3353 - val_loss: 2.3044 - val_f1: 0.3037\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 2.1452 - f1: 0.4015 - val_loss: 1.9930 - val_f1: 0.4092\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 1.9137 - f1: 0.4583 - val_loss: 1.7872 - val_f1: 0.4763\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 1.8384 - f1: 0.4866 - val_loss: 1.7812 - val_f1: 0.4820\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 300us/sample - loss: 1.7065 - f1: 0.5209 - val_loss: 1.7416 - val_f1: 0.5035\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 1.6379 - f1: 0.5305 - val_loss: 1.6021 - val_f1: 0.5067\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 343us/sample - loss: 1.5529 - f1: 0.5817 - val_loss: 1.5149 - val_f1: 0.5662\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 411us/sample - loss: 1.4762 - f1: 0.5959 - val_loss: 1.4464 - val_f1: 0.5613\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 1.4531 - f1: 0.5908 - val_loss: 1.4211 - val_f1: 0.5849\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 1.3658 - f1: 0.6289 - val_loss: 1.3836 - val_f1: 0.6161\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 1.2692 - f1: 0.6648 - val_loss: 1.2577 - val_f1: 0.6429\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 301us/sample - loss: 1.2924 - f1: 0.6579 - val_loss: 1.2450 - val_f1: 0.6491\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 412us/sample - loss: 1.2504 - f1: 0.6697 - val_loss: 1.2477 - val_f1: 0.6546\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 344us/sample - loss: 1.1959 - f1: 0.6947 - val_loss: 1.2513 - val_f1: 0.6316\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 1.1959 - f1: 0.6703 - val_loss: 1.1368 - val_f1: 0.6802\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 1.1217 - f1: 0.6859 - val_loss: 1.0964 - val_f1: 0.7044\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 1.0537 - f1: 0.7217 - val_loss: 1.0840 - val_f1: 0.6924\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 1.0596 - f1: 0.7230 - val_loss: 1.0690 - val_f1: 0.6982\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 1.0835 - f1: 0.7272 - val_loss: 1.0789 - val_f1: 0.6991\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 1.0689 - f1: 0.7116 - val_loss: 1.0044 - val_f1: 0.7298\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 1.0523 - f1: 0.7293 - val_loss: 1.0740 - val_f1: 0.6981\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 1.0797 - f1: 0.7242 - val_loss: 1.0327 - val_f1: 0.6996\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 0.9554 - f1: 0.7706 - val_loss: 1.0084 - val_f1: 0.7241\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 0.9304 - f1: 0.7790 - val_loss: 0.9715 - val_f1: 0.7333\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 0.9696 - f1: 0.7535 - val_loss: 0.9311 - val_f1: 0.7548\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 308us/sample - loss: 0.9343 - f1: 0.7785 - val_loss: 0.8963 - val_f1: 0.7728\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 304us/sample - loss: 0.8848 - f1: 0.7928 - val_loss: 1.0007 - val_f1: 0.7005\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 310us/sample - loss: 0.9577 - f1: 0.7613 - val_loss: 0.8993 - val_f1: 0.7590\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 0.9342 - f1: 0.7827 - val_loss: 0.9266 - val_f1: 0.7426\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 0.9250 - f1: 0.7760 - val_loss: 0.9414 - val_f1: 0.7389\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 0.9087 - f1: 0.7747 - val_loss: 0.8397 - val_f1: 0.7889\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 0.8606 - f1: 0.7902 - val_loss: 0.8643 - val_f1: 0.7845\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 0.8798 - f1: 0.7948 - val_loss: 0.8501 - val_f1: 0.7802\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 0.8395 - f1: 0.7958 - val_loss: 0.8190 - val_f1: 0.7931\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 303us/sample - loss: 0.8547 - f1: 0.8069 - val_loss: 0.8225 - val_f1: 0.7907\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 0.8505 - f1: 0.8063 - val_loss: 0.8343 - val_f1: 0.7815\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 0.8474 - f1: 0.8129 - val_loss: 0.7742 - val_f1: 0.8124\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 0.7706 - f1: 0.8193 - val_loss: 0.7450 - val_f1: 0.8253\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 0.8939 - f1: 0.7605 - val_loss: 0.8473 - val_f1: 0.7794\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 0.8408 - f1: 0.7977 - val_loss: 0.9036 - val_f1: 0.7506\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 306us/sample - loss: 0.8467 - f1: 0.8078 - val_loss: 0.8662 - val_f1: 0.7758\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 0.8009 - f1: 0.8114 - val_loss: 0.7613 - val_f1: 0.8057\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 0.8056 - f1: 0.8118 - val_loss: 0.7748 - val_f1: 0.8015\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 307us/sample - loss: 0.7913 - f1: 0.8224 - val_loss: 0.7343 - val_f1: 0.8235\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 305us/sample - loss: 0.7586 - f1: 0.8304 - val_loss: 0.7647 - val_f1: 0.8110\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 302us/sample - loss: 0.8162 - f1: 0.8165 - val_loss: 0.7534 - val_f1: 0.8127\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.7564 - f1: 0.8246 - val_loss: 0.7442 - val_f1: 0.8187\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7669 - f1: 0.8258 - val_loss: 0.7505 - val_f1: 0.8085\n",
      "Running through fold 4\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 18.8752 - f1: 0.0112 - val_loss: 11.9648 - val_f1: 0.0045\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 10.0104 - f1: 0.0166 - val_loss: 8.2706 - val_f1: 0.0499\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 7.2781 - f1: 0.0674 - val_loss: 6.3424 - val_f1: 0.1010\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 5.7594 - f1: 0.1127 - val_loss: 5.2870 - val_f1: 0.0881\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 4.9208 - f1: 0.1474 - val_loss: 4.6294 - val_f1: 0.1484\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 4.2744 - f1: 0.1638 - val_loss: 4.0067 - val_f1: 0.1696\n",
      "Epoch 7/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 327us/sample - loss: 3.8006 - f1: 0.2036 - val_loss: 3.5850 - val_f1: 0.1952\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 3.3730 - f1: 0.2471 - val_loss: 3.2228 - val_f1: 0.2292\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 3.0531 - f1: 0.2820 - val_loss: 3.0013 - val_f1: 0.2622\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 330us/sample - loss: 2.8704 - f1: 0.2889 - val_loss: 2.7016 - val_f1: 0.2761\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 2.6351 - f1: 0.3122 - val_loss: 2.5224 - val_f1: 0.3335\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 2.3509 - f1: 0.3737 - val_loss: 2.2458 - val_f1: 0.3780\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 2.1708 - f1: 0.3920 - val_loss: 2.1156 - val_f1: 0.3883\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.9667 - f1: 0.4617 - val_loss: 1.8793 - val_f1: 0.4551\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 1.8086 - f1: 0.4814 - val_loss: 1.8184 - val_f1: 0.4355\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 325us/sample - loss: 1.7730 - f1: 0.5089 - val_loss: 1.6164 - val_f1: 0.5266\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 1.6086 - f1: 0.5420 - val_loss: 1.5219 - val_f1: 0.5490\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 313us/sample - loss: 1.4884 - f1: 0.5751 - val_loss: 1.5513 - val_f1: 0.5406\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 1.5246 - f1: 0.5670 - val_loss: 1.4119 - val_f1: 0.5998\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 1.4445 - f1: 0.5810 - val_loss: 1.4926 - val_f1: 0.5428\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 1.3586 - f1: 0.6210 - val_loss: 1.3110 - val_f1: 0.6374\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.3615 - f1: 0.6064 - val_loss: 1.2882 - val_f1: 0.6170\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 1.2671 - f1: 0.6415 - val_loss: 1.2061 - val_f1: 0.6576\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.2421 - f1: 0.6472 - val_loss: 1.2093 - val_f1: 0.6519\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 1.1869 - f1: 0.6475 - val_loss: 1.1524 - val_f1: 0.6558\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 1.1522 - f1: 0.6750 - val_loss: 1.1417 - val_f1: 0.6708\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 1.1315 - f1: 0.6680 - val_loss: 1.0741 - val_f1: 0.6977\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 1.1104 - f1: 0.6900 - val_loss: 1.0532 - val_f1: 0.6977\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.0786 - f1: 0.6914 - val_loss: 1.0949 - val_f1: 0.6666\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 1.0600 - f1: 0.7003 - val_loss: 1.0483 - val_f1: 0.7023\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 1.0636 - f1: 0.7098 - val_loss: 0.9754 - val_f1: 0.7241\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 1.0396 - f1: 0.7106 - val_loss: 0.9539 - val_f1: 0.7489\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 0.9835 - f1: 0.7372 - val_loss: 1.0720 - val_f1: 0.6738\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 1.0356 - f1: 0.7191 - val_loss: 1.0287 - val_f1: 0.7098\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.9874 - f1: 0.7269 - val_loss: 0.9114 - val_f1: 0.7671\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.9712 - f1: 0.7278 - val_loss: 0.9942 - val_f1: 0.7252\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 315us/sample - loss: 0.9986 - f1: 0.7289 - val_loss: 0.9062 - val_f1: 0.7603\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 0.9561 - f1: 0.7510 - val_loss: 0.9922 - val_f1: 0.7220\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.9624 - f1: 0.7349 - val_loss: 0.8791 - val_f1: 0.7613\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.9103 - f1: 0.7608 - val_loss: 0.8405 - val_f1: 0.7858\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 320us/sample - loss: 0.9562 - f1: 0.7508 - val_loss: 1.0031 - val_f1: 0.6961\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 0.8836 - f1: 0.7746 - val_loss: 0.8326 - val_f1: 0.7979\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.9420 - f1: 0.7616 - val_loss: 0.9143 - val_f1: 0.7440\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.9271 - f1: 0.7551 - val_loss: 0.8492 - val_f1: 0.7792\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 319us/sample - loss: 0.8532 - f1: 0.7926 - val_loss: 0.8370 - val_f1: 0.7791\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 328us/sample - loss: 0.8355 - f1: 0.7892 - val_loss: 0.8141 - val_f1: 0.7796\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 321us/sample - loss: 0.8556 - f1: 0.7719 - val_loss: 0.7933 - val_f1: 0.8013\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 322us/sample - loss: 0.8235 - f1: 0.7918 - val_loss: 0.8362 - val_f1: 0.7846\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 324us/sample - loss: 0.8277 - f1: 0.7977 - val_loss: 0.7646 - val_f1: 0.8150\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 0.7965 - f1: 0.8115 - val_loss: 0.7878 - val_f1: 0.8097\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.8154 - f1: 0.8017 - val_loss: 0.8374 - val_f1: 0.7753\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.8250 - f1: 0.8042 - val_loss: 0.7674 - val_f1: 0.8138\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 325us/sample - loss: 0.8529 - f1: 0.7990 - val_loss: 0.8292 - val_f1: 0.7913\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 323us/sample - loss: 0.8144 - f1: 0.8056 - val_loss: 0.7942 - val_f1: 0.7950\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.7649 - f1: 0.8103 - val_loss: 0.7658 - val_f1: 0.8125\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 318us/sample - loss: 0.9057 - f1: 0.7658 - val_loss: 0.8204 - val_f1: 0.7773\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 317us/sample - loss: 0.7955 - f1: 0.7984 - val_loss: 0.7888 - val_f1: 0.8038\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 316us/sample - loss: 0.7548 - f1: 0.8294 - val_loss: 0.7899 - val_f1: 0.7940\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 3s 3ms/sample - loss: 0.7397 - f1: 0.8114 - val_loss: 0.8003 - val_f1: 0.8045\n",
      "\n",
      "\n",
      "Running through training size 5000\n",
      "Running through fold 0\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 4s 718us/sample - loss: 9.3970 - f1: 0.1007 - val_loss: 4.6025 - val_f1: 0.1981\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 3.4341 - f1: 0.2802 - val_loss: 2.4357 - val_f1: 0.4150\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.0406 - f1: 0.5167 - val_loss: 1.5660 - val_f1: 0.6286\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.5150 - f1: 0.6289 - val_loss: 1.2966 - val_f1: 0.6527\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.2484 - f1: 0.6920 - val_loss: 1.0256 - val_f1: 0.7563\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 1.0944 - f1: 0.7340 - val_loss: 0.9304 - val_f1: 0.7842\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.0095 - f1: 0.7433 - val_loss: 0.8235 - val_f1: 0.8066\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.9232 - f1: 0.7740 - val_loss: 0.7913 - val_f1: 0.8119\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.8828 - f1: 0.7862 - val_loss: 0.8709 - val_f1: 0.7740\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.8353 - f1: 0.7980 - val_loss: 0.8902 - val_f1: 0.7334\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.8377 - f1: 0.7898 - val_loss: 0.7275 - val_f1: 0.8188\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.8182 - f1: 0.7944 - val_loss: 0.6666 - val_f1: 0.8471\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7383 - f1: 0.8184 - val_loss: 0.7484 - val_f1: 0.8014\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7916 - f1: 0.8008 - val_loss: 0.6409 - val_f1: 0.8700\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7640 - f1: 0.8158 - val_loss: 0.6133 - val_f1: 0.8806\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7691 - f1: 0.8138 - val_loss: 0.7485 - val_f1: 0.8039\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6994 - f1: 0.8385 - val_loss: 0.6438 - val_f1: 0.8496\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6892 - f1: 0.8398 - val_loss: 0.5951 - val_f1: 0.8901\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6724 - f1: 0.8456 - val_loss: 0.5719 - val_f1: 0.8891\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6761 - f1: 0.8466 - val_loss: 0.5508 - val_f1: 0.8976\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6535 - f1: 0.8541 - val_loss: 0.5506 - val_f1: 0.8978\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6738 - f1: 0.8424 - val_loss: 0.5160 - val_f1: 0.9089\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6283 - f1: 0.8563 - val_loss: 0.5628 - val_f1: 0.8787\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 0.6395 - f1: 0.8597 - val_loss: 0.5163 - val_f1: 0.9088\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.5953 - f1: 0.8723 - val_loss: 0.4572 - val_f1: 0.9362\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6213 - f1: 0.8643 - val_loss: 0.5545 - val_f1: 0.8707\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6190 - f1: 0.8650 - val_loss: 0.5367 - val_f1: 0.8816\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.5740 - f1: 0.8781 - val_loss: 0.4942 - val_f1: 0.9064\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.5772 - f1: 0.8735 - val_loss: 0.5198 - val_f1: 0.8843\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.5993 - f1: 0.8682 - val_loss: 0.5149 - val_f1: 0.8960\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.5807 - f1: 0.8775 - val_loss: 0.5695 - val_f1: 0.8852\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.5812 - f1: 0.8761 - val_loss: 0.5083 - val_f1: 0.9052\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6169 - f1: 0.8661 - val_loss: 0.4756 - val_f1: 0.9162\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6073 - f1: 0.8650 - val_loss: 0.4885 - val_f1: 0.9206\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 4s 773us/sample - loss: 0.5337 - f1: 0.8948 - val_loss: 0.4277 - val_f1: 0.9382\n",
      "Running through fold 1\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 4s 782us/sample - loss: 10.5173 - f1: 0.0787 - val_loss: 5.1728 - val_f1: 0.1631\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 3.7215 - f1: 0.3132 - val_loss: 2.6821 - val_f1: 0.4689\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 2.2047 - f1: 0.5326 - val_loss: 1.7300 - val_f1: 0.6444\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 1.5551 - f1: 0.6673 - val_loss: 1.2531 - val_f1: 0.7485\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 155us/sample - loss: 1.2531 - f1: 0.7287 - val_loss: 1.0999 - val_f1: 0.7570\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 154us/sample - loss: 1.0830 - f1: 0.7624 - val_loss: 0.9377 - val_f1: 0.8068\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 0.9580 - f1: 0.7902 - val_loss: 0.8766 - val_f1: 0.8032\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 0.8633 - f1: 0.8153 - val_loss: 0.8977 - val_f1: 0.7960\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 155us/sample - loss: 0.8249 - f1: 0.8214 - val_loss: 0.6888 - val_f1: 0.8642\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 0.7869 - f1: 0.8217 - val_loss: 0.6702 - val_f1: 0.8672\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 155us/sample - loss: 0.7598 - f1: 0.8302 - val_loss: 0.6014 - val_f1: 0.8909\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 155us/sample - loss: 0.7270 - f1: 0.8395 - val_loss: 0.6322 - val_f1: 0.8646\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 0.6819 - f1: 0.8516 - val_loss: 0.5837 - val_f1: 0.8945\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 0.6788 - f1: 0.8557 - val_loss: 0.5859 - val_f1: 0.8883\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6486 - f1: 0.8590 - val_loss: 0.6184 - val_f1: 0.8686\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 187us/sample - loss: 0.6699 - f1: 0.8493 - val_loss: 0.6082 - val_f1: 0.8726\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 0.6178 - f1: 0.8746 - val_loss: 0.5315 - val_f1: 0.9007\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 0.6049 - f1: 0.8748 - val_loss: 0.6296 - val_f1: 0.8556\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 0.6681 - f1: 0.8556 - val_loss: 0.5904 - val_f1: 0.8843\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 0.6271 - f1: 0.8650 - val_loss: 0.5265 - val_f1: 0.9021\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6487 - f1: 0.8540 - val_loss: 0.6423 - val_f1: 0.8692\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 0.6419 - f1: 0.8611 - val_loss: 0.4749 - val_f1: 0.9316\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 0.5805 - f1: 0.8832 - val_loss: 0.5006 - val_f1: 0.9078\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 0.6035 - f1: 0.8730 - val_loss: 0.4751 - val_f1: 0.9234\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5785 - f1: 0.8809 - val_loss: 0.4754 - val_f1: 0.9161\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5405 - f1: 0.8925 - val_loss: 0.5071 - val_f1: 0.9013\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 0.5773 - f1: 0.8801 - val_loss: 0.5415 - val_f1: 0.8820\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5683 - f1: 0.8814 - val_loss: 0.5150 - val_f1: 0.8808\n",
      "Epoch 29/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.6291 - f1: 0.8602 - val_loss: 0.7133 - val_f1: 0.8028\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5580 - f1: 0.8852 - val_loss: 0.4265 - val_f1: 0.9395\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5578 - f1: 0.8807 - val_loss: 0.5093 - val_f1: 0.8885\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5177 - f1: 0.8959 - val_loss: 0.4156 - val_f1: 0.9457\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5216 - f1: 0.8964 - val_loss: 0.4252 - val_f1: 0.9369\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 0.5560 - f1: 0.8791 - val_loss: 0.5736 - val_f1: 0.8618\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.6075 - f1: 0.8668 - val_loss: 0.4490 - val_f1: 0.9280\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5160 - f1: 0.8951 - val_loss: 0.5294 - val_f1: 0.8966\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5244 - f1: 0.8999 - val_loss: 0.4170 - val_f1: 0.9371\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5058 - f1: 0.9050 - val_loss: 0.4077 - val_f1: 0.9375\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.5057 - f1: 0.9001 - val_loss: 0.4505 - val_f1: 0.9182\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 156us/sample - loss: 0.5252 - f1: 0.8997 - val_loss: 0.5732 - val_f1: 0.8799\n",
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.5367 - f1: 0.8916 - val_loss: 0.5031 - val_f1: 0.8944\n",
      "Epoch 42/2000\n",
      "5000/5000 [==============================] - 4s 781us/sample - loss: 0.5021 - f1: 0.9062 - val_loss: 0.4357 - val_f1: 0.9264\n",
      "Running through fold 2\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 4s 751us/sample - loss: 9.7062 - f1: 0.0333 - val_loss: 4.8231 - val_f1: 0.0302\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 3.7448 - f1: 0.1004 - val_loss: 2.9380 - val_f1: 0.2024\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 2.4959 - f1: 0.2388 - val_loss: 2.1610 - val_f1: 0.3139\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.8145 - f1: 0.4137 - val_loss: 1.4978 - val_f1: 0.5103\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.4147 - f1: 0.5549 - val_loss: 1.1989 - val_f1: 0.6332\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 1.2474 - f1: 0.6381 - val_loss: 1.0720 - val_f1: 0.6622\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.1352 - f1: 0.6710 - val_loss: 0.9627 - val_f1: 0.7048\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.0944 - f1: 0.6869 - val_loss: 0.9361 - val_f1: 0.7383\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 0.9831 - f1: 0.7338 - val_loss: 0.8814 - val_f1: 0.7506\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.9292 - f1: 0.7551 - val_loss: 0.7754 - val_f1: 0.7986\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.9227 - f1: 0.7515 - val_loss: 0.8240 - val_f1: 0.7617\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.8824 - f1: 0.7713 - val_loss: 0.7573 - val_f1: 0.8036\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.8344 - f1: 0.7929 - val_loss: 0.7088 - val_f1: 0.8313\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.8204 - f1: 0.7992 - val_loss: 0.6813 - val_f1: 0.8406\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.8283 - f1: 0.7952 - val_loss: 0.6771 - val_f1: 0.8364\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 159us/sample - loss: 0.7744 - f1: 0.8149 - val_loss: 0.7115 - val_f1: 0.8178\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.7617 - f1: 0.8138 - val_loss: 0.5962 - val_f1: 0.8704\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7667 - f1: 0.8150 - val_loss: 0.6205 - val_f1: 0.8613\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.7518 - f1: 0.8191 - val_loss: 0.6593 - val_f1: 0.8415\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7683 - f1: 0.8070 - val_loss: 0.6530 - val_f1: 0.8528\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7149 - f1: 0.8304 - val_loss: 0.6167 - val_f1: 0.8527\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7140 - f1: 0.8277 - val_loss: 0.5624 - val_f1: 0.8784\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6758 - f1: 0.8456 - val_loss: 0.5611 - val_f1: 0.8770\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6937 - f1: 0.8345 - val_loss: 0.5738 - val_f1: 0.8748\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6722 - f1: 0.8460 - val_loss: 0.5759 - val_f1: 0.8717\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.7016 - f1: 0.8274 - val_loss: 0.5613 - val_f1: 0.8710\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 4s 775us/sample - loss: 0.6599 - f1: 0.8480 - val_loss: 0.5811 - val_f1: 0.8764\n",
      "Running through fold 3\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 4s 760us/sample - loss: 9.7104 - f1: 0.0333 - val_loss: 4.9627 - val_f1: 0.0811\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 3.9178 - f1: 0.1313 - val_loss: 3.1015 - val_f1: 0.1706\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 2.6435 - f1: 0.2561 - val_loss: 2.0197 - val_f1: 0.3649\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.8090 - f1: 0.4561 - val_loss: 1.4593 - val_f1: 0.5622\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.4148 - f1: 0.5840 - val_loss: 1.1903 - val_f1: 0.6321\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.2512 - f1: 0.6346 - val_loss: 1.0150 - val_f1: 0.6868\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 1.0883 - f1: 0.6951 - val_loss: 0.9615 - val_f1: 0.6976\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.0100 - f1: 0.7224 - val_loss: 0.9985 - val_f1: 0.6858\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.9376 - f1: 0.7442 - val_loss: 0.8723 - val_f1: 0.7444\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.8842 - f1: 0.7628 - val_loss: 0.7828 - val_f1: 0.7892\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.8376 - f1: 0.7802 - val_loss: 0.7391 - val_f1: 0.8142\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.8313 - f1: 0.7811 - val_loss: 0.6817 - val_f1: 0.8316\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.8013 - f1: 0.7916 - val_loss: 0.6696 - val_f1: 0.8421\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7742 - f1: 0.8112 - val_loss: 0.6670 - val_f1: 0.8279\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7364 - f1: 0.8187 - val_loss: 0.6512 - val_f1: 0.8388\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.7510 - f1: 0.8113 - val_loss: 0.6397 - val_f1: 0.8314\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7157 - f1: 0.8216 - val_loss: 0.6769 - val_f1: 0.8418\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7171 - f1: 0.8253 - val_loss: 0.7441 - val_f1: 0.8107\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7140 - f1: 0.8303 - val_loss: 0.6515 - val_f1: 0.8392\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.6795 - f1: 0.8409 - val_loss: 0.5935 - val_f1: 0.8597\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.6640 - f1: 0.8395 - val_loss: 0.6801 - val_f1: 0.8198\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.6828 - f1: 0.8328 - val_loss: 0.5820 - val_f1: 0.8624\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6724 - f1: 0.8408 - val_loss: 0.5750 - val_f1: 0.8700\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6632 - f1: 0.8393 - val_loss: 0.5314 - val_f1: 0.8866\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6444 - f1: 0.8515 - val_loss: 0.5562 - val_f1: 0.8763\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6599 - f1: 0.8452 - val_loss: 0.5394 - val_f1: 0.8890\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.7252 - f1: 0.8196 - val_loss: 0.6162 - val_f1: 0.8609\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6536 - f1: 0.8422 - val_loss: 0.5557 - val_f1: 0.8849\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6390 - f1: 0.8546 - val_loss: 0.5178 - val_f1: 0.8950\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6693 - f1: 0.8424 - val_loss: 0.6420 - val_f1: 0.8362\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6406 - f1: 0.8468 - val_loss: 0.5246 - val_f1: 0.8899\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6134 - f1: 0.8620 - val_loss: 0.5579 - val_f1: 0.8754\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6099 - f1: 0.8623 - val_loss: 0.5083 - val_f1: 0.8937\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 4s 817us/sample - loss: 0.6501 - f1: 0.8455 - val_loss: 0.5184 - val_f1: 0.8957\n",
      "Running through fold 4\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 4s 830us/sample - loss: 8.9948 - f1: 0.0562 - val_loss: 4.2105 - val_f1: 0.1127\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 3.1989 - f1: 0.1958 - val_loss: 2.6911 - val_f1: 0.2871\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 2.0594 - f1: 0.3762 - val_loss: 1.6359 - val_f1: 0.5034\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 1.5732 - f1: 0.5216 - val_loss: 1.3441 - val_f1: 0.5907\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.3456 - f1: 0.5949 - val_loss: 1.1331 - val_f1: 0.6619\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.1896 - f1: 0.6571 - val_loss: 1.0171 - val_f1: 0.6990\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.1001 - f1: 0.6850 - val_loss: 1.0210 - val_f1: 0.6904\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 1.0418 - f1: 0.7058 - val_loss: 0.8874 - val_f1: 0.7435\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 1.0044 - f1: 0.7150 - val_loss: 0.9321 - val_f1: 0.7381\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.9465 - f1: 0.7403 - val_loss: 0.8756 - val_f1: 0.7435\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.9491 - f1: 0.7457 - val_loss: 0.8269 - val_f1: 0.7737\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.9384 - f1: 0.7400 - val_loss: 0.8006 - val_f1: 0.7652\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.8748 - f1: 0.7698 - val_loss: 0.7028 - val_f1: 0.8234\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.8761 - f1: 0.7637 - val_loss: 0.6965 - val_f1: 0.8295\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.8277 - f1: 0.7834 - val_loss: 0.7874 - val_f1: 0.7780\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.8265 - f1: 0.7839 - val_loss: 0.7230 - val_f1: 0.8019\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.8533 - f1: 0.7825 - val_loss: 0.7058 - val_f1: 0.8205\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.8194 - f1: 0.7883 - val_loss: 0.7461 - val_f1: 0.7758\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7872 - f1: 0.8009 - val_loss: 0.6312 - val_f1: 0.8564\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7901 - f1: 0.7994 - val_loss: 0.6153 - val_f1: 0.8616\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7478 - f1: 0.8180 - val_loss: 0.6238 - val_f1: 0.8510\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7477 - f1: 0.8149 - val_loss: 0.6725 - val_f1: 0.8291\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.7440 - f1: 0.8236 - val_loss: 0.5982 - val_f1: 0.8684\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.7468 - f1: 0.8194 - val_loss: 0.8556 - val_f1: 0.7482\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7319 - f1: 0.8286 - val_loss: 0.6841 - val_f1: 0.8297\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.7026 - f1: 0.8313 - val_loss: 0.5628 - val_f1: 0.8719\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.7077 - f1: 0.8278 - val_loss: 0.6555 - val_f1: 0.8276\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6869 - f1: 0.8373 - val_loss: 0.6621 - val_f1: 0.8399\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.7119 - f1: 0.8269 - val_loss: 0.6299 - val_f1: 0.8530\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6769 - f1: 0.8405 - val_loss: 0.5488 - val_f1: 0.8875\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6648 - f1: 0.8476 - val_loss: 0.5903 - val_f1: 0.8589\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.6496 - f1: 0.8511 - val_loss: 0.5319 - val_f1: 0.8857\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.6563 - f1: 0.8432 - val_loss: 0.4937 - val_f1: 0.9118\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6469 - f1: 0.8492 - val_loss: 0.5707 - val_f1: 0.8712\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6746 - f1: 0.8346 - val_loss: 0.5459 - val_f1: 0.8817\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6527 - f1: 0.8517 - val_loss: 0.5131 - val_f1: 0.8950\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6290 - f1: 0.8491 - val_loss: 0.5537 - val_f1: 0.8735\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.6664 - f1: 0.8427 - val_loss: 0.4827 - val_f1: 0.9088\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 162us/sample - loss: 0.6448 - f1: 0.8516 - val_loss: 0.4984 - val_f1: 0.9002\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.6384 - f1: 0.8492 - val_loss: 0.4786 - val_f1: 0.9094\n",
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 161us/sample - loss: 0.6302 - f1: 0.8516 - val_loss: 0.5921 - val_f1: 0.8478\n",
      "Epoch 42/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.6651 - f1: 0.8406 - val_loss: 0.6984 - val_f1: 0.8069\n",
      "Epoch 43/2000\n",
      "5000/5000 [==============================] - 4s 887us/sample - loss: 0.6306 - f1: 0.8538 - val_loss: 0.5486 - val_f1: 0.8756\n",
      "\n",
      "\n",
      "Running through training size 10000\n",
      "Running through fold 0\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 5s 462us/sample - loss: 6.5568 - f1: 0.2456 - val_loss: 2.3770 - val_f1: 0.5508\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 1.8060 - f1: 0.6336 - val_loss: 1.3186 - val_f1: 0.7346\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 1.1928 - f1: 0.7441 - val_loss: 0.9222 - val_f1: 0.8231\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.9217 - f1: 0.8032 - val_loss: 0.9665 - val_f1: 0.7821\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.8313 - f1: 0.8210 - val_loss: 0.7111 - val_f1: 0.8529\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.7554 - f1: 0.8357 - val_loss: 0.6976 - val_f1: 0.8561\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 137us/sample - loss: 0.7427 - f1: 0.8336 - val_loss: 0.6024 - val_f1: 0.8899\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.7027 - f1: 0.8455 - val_loss: 0.5892 - val_f1: 0.8948\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.6565 - f1: 0.8605 - val_loss: 0.8398 - val_f1: 0.7797\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6356 - f1: 0.8645 - val_loss: 0.5449 - val_f1: 0.8879\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6468 - f1: 0.8556 - val_loss: 0.4925 - val_f1: 0.9134\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6210 - f1: 0.8633 - val_loss: 0.4722 - val_f1: 0.9320\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5863 - f1: 0.8757 - val_loss: 0.4570 - val_f1: 0.9303\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.5584 - f1: 0.8866 - val_loss: 0.5131 - val_f1: 0.9039\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.6019 - f1: 0.8701 - val_loss: 0.5174 - val_f1: 0.9151\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 137us/sample - loss: 0.5280 - f1: 0.8950 - val_loss: 0.4403 - val_f1: 0.9255\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.5170 - f1: 0.8952 - val_loss: 0.4057 - val_f1: 0.9486\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.5535 - f1: 0.8818 - val_loss: 0.4095 - val_f1: 0.9354\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.5398 - f1: 0.8865 - val_loss: 0.6247 - val_f1: 0.8666\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5645 - f1: 0.8884 - val_loss: 0.4185 - val_f1: 0.9446\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.5415 - f1: 0.8913 - val_loss: 0.4822 - val_f1: 0.9081\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.5245 - f1: 0.8973 - val_loss: 0.5876 - val_f1: 0.8544\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.5379 - f1: 0.8868 - val_loss: 0.4665 - val_f1: 0.9296\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5306 - f1: 0.8940 - val_loss: 0.4048 - val_f1: 0.9519\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.4836 - f1: 0.9062 - val_loss: 0.5905 - val_f1: 0.8591\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.5420 - f1: 0.8858 - val_loss: 0.4424 - val_f1: 0.9201\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 5s 496us/sample - loss: 0.4869 - f1: 0.9055 - val_loss: 0.4506 - val_f1: 0.9234\n",
      "Running through fold 1\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 5s 465us/sample - loss: 6.6342 - f1: 0.0964 - val_loss: 2.8498 - val_f1: 0.2209\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 2.1020 - f1: 0.3852 - val_loss: 1.4196 - val_f1: 0.5715\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 1.3148 - f1: 0.6303 - val_loss: 1.1119 - val_f1: 0.6753\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 1.0543 - f1: 0.7173 - val_loss: 0.8420 - val_f1: 0.7763\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.9584 - f1: 0.7417 - val_loss: 0.7706 - val_f1: 0.8049\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.8601 - f1: 0.7753 - val_loss: 0.8885 - val_f1: 0.7193\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.8218 - f1: 0.7894 - val_loss: 0.7028 - val_f1: 0.8087\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.7753 - f1: 0.8020 - val_loss: 0.5997 - val_f1: 0.8737\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.7572 - f1: 0.8084 - val_loss: 0.6206 - val_f1: 0.8559\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6889 - f1: 0.8298 - val_loss: 0.6226 - val_f1: 0.8480\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.6799 - f1: 0.8322 - val_loss: 0.5593 - val_f1: 0.8797\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6803 - f1: 0.8341 - val_loss: 0.5490 - val_f1: 0.8749\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.6803 - f1: 0.8353 - val_loss: 0.5239 - val_f1: 0.8934\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6413 - f1: 0.8464 - val_loss: 0.5730 - val_f1: 0.8642\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.6227 - f1: 0.8521 - val_loss: 0.5784 - val_f1: 0.8534\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 0.6276 - f1: 0.8509 - val_loss: 0.5277 - val_f1: 0.8799\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5930 - f1: 0.8654 - val_loss: 0.5398 - val_f1: 0.8749\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.6086 - f1: 0.8618 - val_loss: 0.4663 - val_f1: 0.9048\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.5973 - f1: 0.8620 - val_loss: 0.5344 - val_f1: 0.8780\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 137us/sample - loss: 0.5781 - f1: 0.8694 - val_loss: 0.4299 - val_f1: 0.9291\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.6098 - f1: 0.8610 - val_loss: 0.5546 - val_f1: 0.8729\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.5872 - f1: 0.8685 - val_loss: 0.5234 - val_f1: 0.8939\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5782 - f1: 0.8775 - val_loss: 0.4211 - val_f1: 0.9396\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.5658 - f1: 0.8844 - val_loss: 0.4532 - val_f1: 0.9206\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5838 - f1: 0.8706 - val_loss: 0.5271 - val_f1: 0.8830\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 138us/sample - loss: 0.5322 - f1: 0.8927 - val_loss: 0.4523 - val_f1: 0.9289\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5182 - f1: 0.8978 - val_loss: 0.4181 - val_f1: 0.9294\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5768 - f1: 0.8771 - val_loss: 0.4687 - val_f1: 0.9121\n",
      "Epoch 29/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5199 - f1: 0.9030 - val_loss: 0.3876 - val_f1: 0.9387\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5713 - f1: 0.8784 - val_loss: 0.5084 - val_f1: 0.8919\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.5345 - f1: 0.8915 - val_loss: 0.4071 - val_f1: 0.9280\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 1s 137us/sample - loss: 0.5182 - f1: 0.8988 - val_loss: 0.4018 - val_f1: 0.9291\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 5s 492us/sample - loss: 0.5058 - f1: 0.9046 - val_loss: 0.3827 - val_f1: 0.9436\n",
      "Running through fold 2\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 5s 466us/sample - loss: 7.5665 - f1: 0.1672 - val_loss: 3.0663 - val_f1: 0.3568\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 2.0775 - f1: 0.5576 - val_loss: 1.4734 - val_f1: 0.6588\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 1.3084 - f1: 0.7106 - val_loss: 1.0588 - val_f1: 0.7571\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 1.0328 - f1: 0.7655 - val_loss: 0.9071 - val_f1: 0.7852\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.8990 - f1: 0.7946 - val_loss: 0.7423 - val_f1: 0.8425\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.8372 - f1: 0.8051 - val_loss: 0.7324 - val_f1: 0.8421\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 0.7665 - f1: 0.8323 - val_loss: 0.8790 - val_f1: 0.7646\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.7018 - f1: 0.8505 - val_loss: 0.5891 - val_f1: 0.8943\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.6862 - f1: 0.8504 - val_loss: 0.5479 - val_f1: 0.9082\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6532 - f1: 0.8602 - val_loss: 0.6163 - val_f1: 0.8748\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6528 - f1: 0.8591 - val_loss: 0.5285 - val_f1: 0.9102\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.6459 - f1: 0.8651 - val_loss: 0.5932 - val_f1: 0.8794\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.5978 - f1: 0.8773 - val_loss: 0.5124 - val_f1: 0.9173\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.5728 - f1: 0.8878 - val_loss: 0.6775 - val_f1: 0.8343\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.6040 - f1: 0.8773 - val_loss: 0.4767 - val_f1: 0.9225\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5643 - f1: 0.8901 - val_loss: 0.4786 - val_f1: 0.9218\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 0.5587 - f1: 0.8908 - val_loss: 0.5403 - val_f1: 0.8914\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5629 - f1: 0.8887 - val_loss: 0.4784 - val_f1: 0.9212\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 0.5506 - f1: 0.8909 - val_loss: 0.4139 - val_f1: 0.9437\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5944 - f1: 0.8802 - val_loss: 0.5998 - val_f1: 0.8699\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5061 - f1: 0.9084 - val_loss: 0.4465 - val_f1: 0.9370\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5144 - f1: 0.9042 - val_loss: 0.6399 - val_f1: 0.8581\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5482 - f1: 0.8912 - val_loss: 0.4595 - val_f1: 0.9212\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5149 - f1: 0.9055 - val_loss: 0.4329 - val_f1: 0.9385\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.5252 - f1: 0.8966 - val_loss: 0.3759 - val_f1: 0.9537\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.4824 - f1: 0.9147 - val_loss: 0.3697 - val_f1: 0.9648\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.4877 - f1: 0.9107 - val_loss: 0.3900 - val_f1: 0.9427\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.5051 - f1: 0.9046 - val_loss: 0.3925 - val_f1: 0.9464\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.4849 - f1: 0.9140 - val_loss: 0.5037 - val_f1: 0.8920\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.4999 - f1: 0.9102 - val_loss: 0.3954 - val_f1: 0.9423\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.4893 - f1: 0.9070 - val_loss: 0.3840 - val_f1: 0.9469\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.4858 - f1: 0.9109 - val_loss: 0.4309 - val_f1: 0.9364\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.4327 - f1: 0.9297 - val_loss: 0.3442 - val_f1: 0.9660\n",
      "Epoch 34/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.4993 - f1: 0.9053 - val_loss: 0.5016 - val_f1: 0.9018\n",
      "Epoch 35/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.4959 - f1: 0.9084 - val_loss: 0.3967 - val_f1: 0.9451\n",
      "Epoch 36/2000\n",
      "10000/10000 [==============================] - 5s 515us/sample - loss: 0.5477 - f1: 0.8931 - val_loss: 0.5501 - val_f1: 0.8942\n",
      "Running through fold 3\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 5s 493us/sample - loss: 7.0572 - f1: 0.1166 - val_loss: 2.9490 - val_f1: 0.2742\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 1.9826 - f1: 0.5076 - val_loss: 1.3600 - val_f1: 0.6606\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 1.2037 - f1: 0.7208 - val_loss: 0.9077 - val_f1: 0.8069\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.9885 - f1: 0.7709 - val_loss: 0.7531 - val_f1: 0.8531\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.8579 - f1: 0.7998 - val_loss: 0.6743 - val_f1: 0.8668\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 0.7764 - f1: 0.8231 - val_loss: 0.6516 - val_f1: 0.8662\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.7664 - f1: 0.8207 - val_loss: 0.6076 - val_f1: 0.8856\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6696 - f1: 0.8564 - val_loss: 0.5955 - val_f1: 0.8722\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.6590 - f1: 0.8589 - val_loss: 0.6155 - val_f1: 0.8707\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.6693 - f1: 0.8478 - val_loss: 0.6262 - val_f1: 0.8694\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6292 - f1: 0.8636 - val_loss: 0.5857 - val_f1: 0.8682\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5872 - f1: 0.8770 - val_loss: 0.6381 - val_f1: 0.8375\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5824 - f1: 0.8788 - val_loss: 0.4796 - val_f1: 0.9161\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5726 - f1: 0.8802 - val_loss: 0.5066 - val_f1: 0.9024\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5667 - f1: 0.8798 - val_loss: 0.4543 - val_f1: 0.9239\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5669 - f1: 0.8821 - val_loss: 0.5067 - val_f1: 0.8989\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.5366 - f1: 0.8944 - val_loss: 0.4575 - val_f1: 0.9159\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5364 - f1: 0.8888 - val_loss: 0.4404 - val_f1: 0.9312\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5121 - f1: 0.9008 - val_loss: 0.5156 - val_f1: 0.8916\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5775 - f1: 0.8739 - val_loss: 0.4870 - val_f1: 0.9033\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.5480 - f1: 0.8827 - val_loss: 0.4500 - val_f1: 0.9236\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5372 - f1: 0.8885 - val_loss: 0.4648 - val_f1: 0.9233\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 147us/sample - loss: 0.5499 - f1: 0.8853 - val_loss: 0.4121 - val_f1: 0.9407\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 0.4951 - f1: 0.9076 - val_loss: 0.4473 - val_f1: 0.9172\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.4884 - f1: 0.9004 - val_loss: 0.5466 - val_f1: 0.8726\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5381 - f1: 0.8899 - val_loss: 0.4224 - val_f1: 0.9358\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.5349 - f1: 0.8885 - val_loss: 0.4129 - val_f1: 0.9343\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 5s 506us/sample - loss: 0.4866 - f1: 0.9011 - val_loss: 0.4683 - val_f1: 0.9107\n",
      "Running through fold 4\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 5s 502us/sample - loss: 6.5973 - f1: 0.0779 - val_loss: 2.8413 - val_f1: 0.2457\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 1.9990 - f1: 0.4219 - val_loss: 1.3287 - val_f1: 0.6271\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 2s 155us/sample - loss: 1.2684 - f1: 0.6475 - val_loss: 1.0323 - val_f1: 0.7145\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 1.0205 - f1: 0.7302 - val_loss: 0.8469 - val_f1: 0.7755\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.9147 - f1: 0.7617 - val_loss: 0.8021 - val_f1: 0.7791\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.8406 - f1: 0.7836 - val_loss: 0.6937 - val_f1: 0.8358\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.8070 - f1: 0.7971 - val_loss: 0.7500 - val_f1: 0.8229\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.7210 - f1: 0.8245 - val_loss: 0.6661 - val_f1: 0.8357\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.7438 - f1: 0.8166 - val_loss: 0.6168 - val_f1: 0.8566\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 139us/sample - loss: 0.6953 - f1: 0.8369 - val_loss: 0.5674 - val_f1: 0.8816\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.6874 - f1: 0.8323 - val_loss: 0.5423 - val_f1: 0.8819\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6791 - f1: 0.8379 - val_loss: 0.5947 - val_f1: 0.8657\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 140us/sample - loss: 0.6408 - f1: 0.8485 - val_loss: 0.5767 - val_f1: 0.8733\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 0.6200 - f1: 0.8506 - val_loss: 0.5430 - val_f1: 0.8816\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6344 - f1: 0.8451 - val_loss: 0.5211 - val_f1: 0.8856\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6381 - f1: 0.8451 - val_loss: 0.5349 - val_f1: 0.8813\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5990 - f1: 0.8638 - val_loss: 0.5904 - val_f1: 0.8470\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5943 - f1: 0.8623 - val_loss: 0.5557 - val_f1: 0.8635\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6217 - f1: 0.8497 - val_loss: 0.4957 - val_f1: 0.8965\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.6037 - f1: 0.8581 - val_loss: 0.5154 - val_f1: 0.8882\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5943 - f1: 0.8619 - val_loss: 0.4701 - val_f1: 0.9002\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5771 - f1: 0.8638 - val_loss: 0.4398 - val_f1: 0.9149\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5753 - f1: 0.8660 - val_loss: 0.4693 - val_f1: 0.9073\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 144us/sample - loss: 0.5648 - f1: 0.8725 - val_loss: 0.4771 - val_f1: 0.9003\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5867 - f1: 0.8642 - val_loss: 0.4354 - val_f1: 0.9212\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 143us/sample - loss: 0.5295 - f1: 0.8819 - val_loss: 0.4433 - val_f1: 0.9126\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.5624 - f1: 0.8715 - val_loss: 0.4380 - val_f1: 0.9069\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5218 - f1: 0.8831 - val_loss: 0.4501 - val_f1: 0.8980\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5580 - f1: 0.8729 - val_loss: 0.4198 - val_f1: 0.9187\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5425 - f1: 0.8780 - val_loss: 0.4849 - val_f1: 0.8929\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 142us/sample - loss: 0.5572 - f1: 0.8718 - val_loss: 0.4133 - val_f1: 0.9148\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 5s 510us/sample - loss: 0.5677 - f1: 0.8690 - val_loss: 0.4668 - val_f1: 0.9055\n",
      "\n",
      "\n",
      "Running through training size 15000\n",
      "Running through fold 0\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 6s 368us/sample - loss: 5.2902 - f1: 0.4019 - val_loss: 1.7823 - val_f1: 0.6715\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 1.4119 - f1: 0.7268 - val_loss: 0.9888 - val_f1: 0.8099\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.9428 - f1: 0.8102 - val_loss: 0.8399 - val_f1: 0.8180\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.7810 - f1: 0.8421 - val_loss: 0.6511 - val_f1: 0.8770\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6909 - f1: 0.8642 - val_loss: 0.6094 - val_f1: 0.8936\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.6608 - f1: 0.8711 - val_loss: 0.5687 - val_f1: 0.8977\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.6132 - f1: 0.8825 - val_loss: 0.5419 - val_f1: 0.9005\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.5873 - f1: 0.8860 - val_loss: 0.5316 - val_f1: 0.8972\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.6032 - f1: 0.8802 - val_loss: 0.4503 - val_f1: 0.9412\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.5774 - f1: 0.8874 - val_loss: 0.4808 - val_f1: 0.9231\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 143us/sample - loss: 0.5570 - f1: 0.8990 - val_loss: 0.5321 - val_f1: 0.8993\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.5732 - f1: 0.8883 - val_loss: 0.5811 - val_f1: 0.8781\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.5233 - f1: 0.9042 - val_loss: 0.4586 - val_f1: 0.9261\n",
      "Epoch 14/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.5040 - f1: 0.9070 - val_loss: 0.4013 - val_f1: 0.9531\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.5106 - f1: 0.9092 - val_loss: 0.5093 - val_f1: 0.9178\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.5122 - f1: 0.9089 - val_loss: 0.4518 - val_f1: 0.9339\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.4947 - f1: 0.9107 - val_loss: 0.4213 - val_f1: 0.9387\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.4841 - f1: 0.9166 - val_loss: 0.4015 - val_f1: 0.9456\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5131 - f1: 0.9041 - val_loss: 0.4452 - val_f1: 0.9262\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5190 - f1: 0.9024 - val_loss: 0.4579 - val_f1: 0.9312\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 146us/sample - loss: 0.4657 - f1: 0.9214 - val_loss: 0.4083 - val_f1: 0.9418\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.5016 - f1: 0.9109 - val_loss: 0.4532 - val_f1: 0.9208\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5279 - f1: 0.8976 - val_loss: 0.6168 - val_f1: 0.8530\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 6s 390us/sample - loss: 0.5103 - f1: 0.9055 - val_loss: 0.4454 - val_f1: 0.9207\n",
      "Running through fold 1\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 6s 378us/sample - loss: 5.3255 - f1: 0.2011 - val_loss: 1.7887 - val_f1: 0.4895\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 1.3892 - f1: 0.6189 - val_loss: 0.9958 - val_f1: 0.7098\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 1.0144 - f1: 0.7163 - val_loss: 0.8082 - val_f1: 0.7814\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.8800 - f1: 0.7601 - val_loss: 0.7088 - val_f1: 0.8295\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.8190 - f1: 0.7814 - val_loss: 0.7109 - val_f1: 0.8206\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.7703 - f1: 0.7975 - val_loss: 0.6745 - val_f1: 0.8084\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.7256 - f1: 0.8144 - val_loss: 0.5596 - val_f1: 0.8815\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.6613 - f1: 0.8365 - val_loss: 0.5283 - val_f1: 0.8885\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.6687 - f1: 0.8393 - val_loss: 0.5271 - val_f1: 0.8849\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.6422 - f1: 0.8417 - val_loss: 0.5703 - val_f1: 0.8776\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.6518 - f1: 0.8395 - val_loss: 0.5169 - val_f1: 0.8938\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.6227 - f1: 0.8555 - val_loss: 0.6943 - val_f1: 0.8102\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5769 - f1: 0.8671 - val_loss: 0.5488 - val_f1: 0.8628\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.5755 - f1: 0.8668 - val_loss: 0.4495 - val_f1: 0.9213\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5942 - f1: 0.8625 - val_loss: 0.5017 - val_f1: 0.8854\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5642 - f1: 0.8690 - val_loss: 0.5024 - val_f1: 0.8920\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5582 - f1: 0.8741 - val_loss: 0.5055 - val_f1: 0.8852\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 0.5588 - f1: 0.8736 - val_loss: 0.4661 - val_f1: 0.9046\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5453 - f1: 0.8766 - val_loss: 0.4274 - val_f1: 0.9262\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5709 - f1: 0.8701 - val_loss: 0.5142 - val_f1: 0.8796\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5463 - f1: 0.8774 - val_loss: 0.4180 - val_f1: 0.9227\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5352 - f1: 0.8794 - val_loss: 0.4796 - val_f1: 0.8897\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5351 - f1: 0.8811 - val_loss: 0.3977 - val_f1: 0.9314\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5516 - f1: 0.8760 - val_loss: 0.4920 - val_f1: 0.8856\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5283 - f1: 0.8863 - val_loss: 0.4050 - val_f1: 0.9378\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5283 - f1: 0.8830 - val_loss: 0.7184 - val_f1: 0.8057\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5302 - f1: 0.8843 - val_loss: 0.4170 - val_f1: 0.9252\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5022 - f1: 0.8922 - val_loss: 0.6945 - val_f1: 0.8110\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5290 - f1: 0.8841 - val_loss: 0.4157 - val_f1: 0.9349\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5291 - f1: 0.8799 - val_loss: 0.4133 - val_f1: 0.9325\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5089 - f1: 0.8908 - val_loss: 0.4123 - val_f1: 0.9244\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5137 - f1: 0.8898 - val_loss: 0.4044 - val_f1: 0.9340\n",
      "Epoch 33/2000\n",
      "15000/15000 [==============================] - 6s 409us/sample - loss: 0.5184 - f1: 0.8872 - val_loss: 0.6292 - val_f1: 0.8416\n",
      "Running through fold 2\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 6s 375us/sample - loss: 5.4966 - f1: 0.2442 - val_loss: 1.7459 - val_f1: 0.5679\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 1.3789 - f1: 0.6647 - val_loss: 0.9680 - val_f1: 0.7795\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 146us/sample - loss: 0.9790 - f1: 0.7619 - val_loss: 0.8131 - val_f1: 0.8077\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.8531 - f1: 0.7924 - val_loss: 0.7391 - val_f1: 0.8297\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.7879 - f1: 0.8057 - val_loss: 0.6475 - val_f1: 0.8527\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.7189 - f1: 0.8293 - val_loss: 0.7110 - val_f1: 0.8186\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6842 - f1: 0.8358 - val_loss: 0.6051 - val_f1: 0.8638\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6668 - f1: 0.8470 - val_loss: 0.5446 - val_f1: 0.8839\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.6509 - f1: 0.8464 - val_loss: 0.5275 - val_f1: 0.8973\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6220 - f1: 0.8565 - val_loss: 0.5297 - val_f1: 0.8881\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6074 - f1: 0.8648 - val_loss: 0.5322 - val_f1: 0.9019\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5927 - f1: 0.8668 - val_loss: 0.4862 - val_f1: 0.8934\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5609 - f1: 0.8769 - val_loss: 0.4928 - val_f1: 0.8936\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5651 - f1: 0.8754 - val_loss: 0.5460 - val_f1: 0.8641\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5467 - f1: 0.8811 - val_loss: 0.3981 - val_f1: 0.9485\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.5393 - f1: 0.8858 - val_loss: 0.4175 - val_f1: 0.9277\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.5673 - f1: 0.8738 - val_loss: 0.6752 - val_f1: 0.8166\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.5313 - f1: 0.8886 - val_loss: 0.5323 - val_f1: 0.8802\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.5295 - f1: 0.8866 - val_loss: 0.4499 - val_f1: 0.9196\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 145us/sample - loss: 0.5339 - f1: 0.8835 - val_loss: 0.4116 - val_f1: 0.9343\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.5575 - f1: 0.8748 - val_loss: 0.4847 - val_f1: 0.8998\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.5315 - f1: 0.8922 - val_loss: 0.5822 - val_f1: 0.8589\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 133us/sample - loss: 0.5223 - f1: 0.8893 - val_loss: 0.4348 - val_f1: 0.9207\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.4964 - f1: 0.8963 - val_loss: 0.4420 - val_f1: 0.9112\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 6s 408us/sample - loss: 0.5218 - f1: 0.8864 - val_loss: 0.4095 - val_f1: 0.9274\n",
      "Running through fold 3\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 6s 378us/sample - loss: 5.6289 - f1: 0.1771 - val_loss: 1.7529 - val_f1: 0.5146\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 1.3704 - f1: 0.6376 - val_loss: 0.9484 - val_f1: 0.7682\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.9391 - f1: 0.7675 - val_loss: 0.7297 - val_f1: 0.8265\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.7929 - f1: 0.8090 - val_loss: 0.5997 - val_f1: 0.8807\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.7318 - f1: 0.8280 - val_loss: 0.7039 - val_f1: 0.8315\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.6846 - f1: 0.8393 - val_loss: 0.5765 - val_f1: 0.8692\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 0.6507 - f1: 0.8506 - val_loss: 0.6116 - val_f1: 0.8659\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 142us/sample - loss: 0.6301 - f1: 0.8570 - val_loss: 0.5407 - val_f1: 0.8810\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.6065 - f1: 0.8644 - val_loss: 0.4894 - val_f1: 0.9080\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.6069 - f1: 0.8641 - val_loss: 0.5514 - val_f1: 0.8818\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5919 - f1: 0.8673 - val_loss: 0.4766 - val_f1: 0.9149\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5902 - f1: 0.8696 - val_loss: 0.4801 - val_f1: 0.9089\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.5976 - f1: 0.8649 - val_loss: 0.4535 - val_f1: 0.9265\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5816 - f1: 0.8716 - val_loss: 0.4752 - val_f1: 0.9069\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5542 - f1: 0.8781 - val_loss: 0.4210 - val_f1: 0.9340\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5593 - f1: 0.8773 - val_loss: 0.4925 - val_f1: 0.9012\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5597 - f1: 0.8798 - val_loss: 0.4061 - val_f1: 0.9361\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5701 - f1: 0.8738 - val_loss: 0.4134 - val_f1: 0.9411\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5315 - f1: 0.8886 - val_loss: 0.4718 - val_f1: 0.8959\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.4991 - f1: 0.8974 - val_loss: 0.6017 - val_f1: 0.8225\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5526 - f1: 0.8749 - val_loss: 0.4081 - val_f1: 0.9298\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5281 - f1: 0.8861 - val_loss: 0.6010 - val_f1: 0.8473\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5332 - f1: 0.8826 - val_loss: 0.3868 - val_f1: 0.9378\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.4966 - f1: 0.8953 - val_loss: 0.3764 - val_f1: 0.9406\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.4926 - f1: 0.8964 - val_loss: 0.3651 - val_f1: 0.9358\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5174 - f1: 0.8869 - val_loss: 0.3555 - val_f1: 0.9482\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.4966 - f1: 0.8975 - val_loss: 0.3546 - val_f1: 0.9517\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5362 - f1: 0.8825 - val_loss: 0.3528 - val_f1: 0.9515\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.4698 - f1: 0.9037 - val_loss: 0.4083 - val_f1: 0.9202\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.5401 - f1: 0.8833 - val_loss: 0.3978 - val_f1: 0.9304\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.4977 - f1: 0.8967 - val_loss: 0.3816 - val_f1: 0.9350\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.4984 - f1: 0.8930 - val_loss: 0.3594 - val_f1: 0.9426\n",
      "Epoch 33/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.5454 - f1: 0.8797 - val_loss: 0.3840 - val_f1: 0.9453\n",
      "Epoch 34/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.4862 - f1: 0.8978 - val_loss: 0.3604 - val_f1: 0.9421\n",
      "Epoch 35/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.5288 - f1: 0.8837 - val_loss: 0.4283 - val_f1: 0.9244\n",
      "Epoch 36/2000\n",
      "15000/15000 [==============================] - 2s 138us/sample - loss: 0.4877 - f1: 0.8960 - val_loss: 0.4610 - val_f1: 0.9048\n",
      "Epoch 37/2000\n",
      "15000/15000 [==============================] - 6s 433us/sample - loss: 0.4613 - f1: 0.9081 - val_loss: 0.7526 - val_f1: 0.7990\n",
      "Running through fold 4\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 6s 386us/sample - loss: 6.0312 - f1: 0.2428 - val_loss: 1.7881 - val_f1: 0.5962\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 1.3986 - f1: 0.6807 - val_loss: 1.0131 - val_f1: 0.7731\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.9520 - f1: 0.7856 - val_loss: 0.7920 - val_f1: 0.8230\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.8161 - f1: 0.8135 - val_loss: 0.6730 - val_f1: 0.8583\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.7555 - f1: 0.8277 - val_loss: 0.5693 - val_f1: 0.8994\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6874 - f1: 0.8491 - val_loss: 0.5606 - val_f1: 0.8968\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 146us/sample - loss: 0.6584 - f1: 0.8615 - val_loss: 0.5557 - val_f1: 0.8917\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.6381 - f1: 0.8667 - val_loss: 0.5068 - val_f1: 0.9141\n",
      "Epoch 9/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.6124 - f1: 0.8739 - val_loss: 0.5794 - val_f1: 0.8778\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5853 - f1: 0.8795 - val_loss: 0.4923 - val_f1: 0.9107\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6034 - f1: 0.8708 - val_loss: 0.6031 - val_f1: 0.8488\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 135us/sample - loss: 0.5888 - f1: 0.8764 - val_loss: 0.5199 - val_f1: 0.8830\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5627 - f1: 0.8868 - val_loss: 0.5751 - val_f1: 0.8657\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5752 - f1: 0.8824 - val_loss: 0.4542 - val_f1: 0.9218\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5719 - f1: 0.8847 - val_loss: 0.4465 - val_f1: 0.9310\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5461 - f1: 0.8911 - val_loss: 0.3907 - val_f1: 0.9519\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 141us/sample - loss: 0.5492 - f1: 0.8902 - val_loss: 0.4522 - val_f1: 0.9279\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.5395 - f1: 0.8920 - val_loss: 0.3947 - val_f1: 0.9398\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5336 - f1: 0.8928 - val_loss: 0.4321 - val_f1: 0.9281\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 140us/sample - loss: 0.5283 - f1: 0.8951 - val_loss: 0.3824 - val_f1: 0.9472\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5172 - f1: 0.8973 - val_loss: 0.4245 - val_f1: 0.9295\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5498 - f1: 0.8890 - val_loss: 0.4041 - val_f1: 0.9456\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5186 - f1: 0.8979 - val_loss: 0.4113 - val_f1: 0.9440\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5613 - f1: 0.8808 - val_loss: 0.4335 - val_f1: 0.9384\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5487 - f1: 0.8878 - val_loss: 0.4269 - val_f1: 0.9338\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 6s 412us/sample - loss: 0.5069 - f1: 0.9025 - val_loss: 0.4208 - val_f1: 0.9254\n",
      "\n",
      "\n",
      "Running through training size 20000\n",
      "Running through fold 0\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 6s 323us/sample - loss: 4.2826 - f1: 0.2431 - val_loss: 1.4504 - val_f1: 0.5440\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 1.2090 - f1: 0.6488 - val_loss: 0.9051 - val_f1: 0.7324\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.9042 - f1: 0.7654 - val_loss: 0.6776 - val_f1: 0.8447\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.8084 - f1: 0.8003 - val_loss: 0.6285 - val_f1: 0.8649\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.7311 - f1: 0.8260 - val_loss: 0.5471 - val_f1: 0.8835\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.6950 - f1: 0.8341 - val_loss: 0.5901 - val_f1: 0.8510\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.6513 - f1: 0.8466 - val_loss: 0.5073 - val_f1: 0.8977\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.6486 - f1: 0.8436 - val_loss: 0.5041 - val_f1: 0.9054\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.6505 - f1: 0.8482 - val_loss: 0.5642 - val_f1: 0.8663\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.6111 - f1: 0.8587 - val_loss: 0.6243 - val_f1: 0.8475\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.6231 - f1: 0.8551 - val_loss: 0.4292 - val_f1: 0.9315\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 140us/sample - loss: 0.6061 - f1: 0.8600 - val_loss: 0.5360 - val_f1: 0.8813\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.5748 - f1: 0.8708 - val_loss: 0.4629 - val_f1: 0.9124\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.5837 - f1: 0.8649 - val_loss: 0.4726 - val_f1: 0.9057\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.5671 - f1: 0.8719 - val_loss: 0.4850 - val_f1: 0.8990\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.5927 - f1: 0.8636 - val_loss: 0.4308 - val_f1: 0.9177\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.5621 - f1: 0.8723 - val_loss: 0.4497 - val_f1: 0.9111\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.5735 - f1: 0.8695 - val_loss: 0.5596 - val_f1: 0.8567\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.5604 - f1: 0.8740 - val_loss: 0.5262 - val_f1: 0.8690\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.5455 - f1: 0.8776 - val_loss: 0.5249 - val_f1: 0.8791\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 7s 336us/sample - loss: 0.5497 - f1: 0.8766 - val_loss: 0.3965 - val_f1: 0.9282\n",
      "Running through fold 1\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 6s 318us/sample - loss: 4.1449 - f1: 0.4303 - val_loss: 1.3101 - val_f1: 0.7324\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 1.0658 - f1: 0.7726 - val_loss: 0.7358 - val_f1: 0.8647\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.7996 - f1: 0.8257 - val_loss: 0.6706 - val_f1: 0.8763\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6881 - f1: 0.8528 - val_loss: 0.5194 - val_f1: 0.9224\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6388 - f1: 0.8667 - val_loss: 0.4562 - val_f1: 0.9367\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5697 - f1: 0.8887 - val_loss: 0.6318 - val_f1: 0.8563\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5738 - f1: 0.8863 - val_loss: 0.4612 - val_f1: 0.9313\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5478 - f1: 0.8941 - val_loss: 0.4458 - val_f1: 0.9361\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5471 - f1: 0.8932 - val_loss: 0.4843 - val_f1: 0.9188\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5042 - f1: 0.9081 - val_loss: 0.4285 - val_f1: 0.9251\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5210 - f1: 0.9026 - val_loss: 0.3831 - val_f1: 0.9672\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.4844 - f1: 0.9133 - val_loss: 0.4721 - val_f1: 0.9111\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.4893 - f1: 0.9086 - val_loss: 0.4001 - val_f1: 0.9417\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4800 - f1: 0.9106 - val_loss: 0.3677 - val_f1: 0.9511\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.4817 - f1: 0.9107 - val_loss: 0.4335 - val_f1: 0.9228\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4813 - f1: 0.9115 - val_loss: 0.3741 - val_f1: 0.9627\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4465 - f1: 0.9222 - val_loss: 0.5166 - val_f1: 0.8894\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.4691 - f1: 0.9143 - val_loss: 0.3373 - val_f1: 0.9655\n",
      "Epoch 19/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.4725 - f1: 0.9135 - val_loss: 0.4744 - val_f1: 0.9006\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.4628 - f1: 0.9162 - val_loss: 0.5154 - val_f1: 0.8820\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 7s 346us/sample - loss: 0.4677 - f1: 0.9115 - val_loss: 0.3814 - val_f1: 0.9483\n",
      "Running through fold 2\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 7s 325us/sample - loss: 4.3667 - f1: 0.4200 - val_loss: 1.5284 - val_f1: 0.6722\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 1.1379 - f1: 0.7594 - val_loss: 0.8610 - val_f1: 0.8043\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.8244 - f1: 0.8240 - val_loss: 0.6445 - val_f1: 0.8832\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.7180 - f1: 0.8482 - val_loss: 0.5629 - val_f1: 0.9001\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.6627 - f1: 0.8593 - val_loss: 0.5391 - val_f1: 0.9081\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6100 - f1: 0.8745 - val_loss: 0.5072 - val_f1: 0.9222\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.6042 - f1: 0.8726 - val_loss: 0.4781 - val_f1: 0.9299\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5561 - f1: 0.8871 - val_loss: 0.5292 - val_f1: 0.8842\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5382 - f1: 0.8928 - val_loss: 0.4977 - val_f1: 0.9092\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5489 - f1: 0.8865 - val_loss: 0.4476 - val_f1: 0.9267\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5186 - f1: 0.9003 - val_loss: 0.4292 - val_f1: 0.9333\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5205 - f1: 0.8968 - val_loss: 0.4587 - val_f1: 0.9154\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5058 - f1: 0.9009 - val_loss: 0.4102 - val_f1: 0.9307\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5564 - f1: 0.8819 - val_loss: 0.4854 - val_f1: 0.9133\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5278 - f1: 0.8953 - val_loss: 0.4021 - val_f1: 0.9416\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4993 - f1: 0.8985 - val_loss: 0.4628 - val_f1: 0.9111\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4997 - f1: 0.9004 - val_loss: 0.3620 - val_f1: 0.9543\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4900 - f1: 0.9000 - val_loss: 0.5974 - val_f1: 0.8571\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4673 - f1: 0.9108 - val_loss: 0.3733 - val_f1: 0.9454\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.4784 - f1: 0.9031 - val_loss: 0.3677 - val_f1: 0.9490\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.4536 - f1: 0.9122 - val_loss: 0.5087 - val_f1: 0.8837\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.5147 - f1: 0.8960 - val_loss: 0.3768 - val_f1: 0.9510\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.4953 - f1: 0.9000 - val_loss: 0.4342 - val_f1: 0.9230\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.5050 - f1: 0.8959 - val_loss: 0.5434 - val_f1: 0.8812\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 139us/sample - loss: 0.4783 - f1: 0.9040 - val_loss: 0.3674 - val_f1: 0.9445\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.5278 - f1: 0.8892 - val_loss: 0.6683 - val_f1: 0.8260\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 7s 352us/sample - loss: 0.5257 - f1: 0.8927 - val_loss: 0.4886 - val_f1: 0.8967\n",
      "Running through fold 3\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 6s 323us/sample - loss: 4.2280 - f1: 0.3355 - val_loss: 1.3442 - val_f1: 0.6581\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 1.0864 - f1: 0.7260 - val_loss: 0.7756 - val_f1: 0.8119\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.8538 - f1: 0.7861 - val_loss: 0.6747 - val_f1: 0.8334\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.7598 - f1: 0.8118 - val_loss: 0.7000 - val_f1: 0.8289\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.7192 - f1: 0.8227 - val_loss: 0.6211 - val_f1: 0.8470\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6821 - f1: 0.8367 - val_loss: 0.5509 - val_f1: 0.8728\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.6506 - f1: 0.8455 - val_loss: 0.5209 - val_f1: 0.8996\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6263 - f1: 0.8551 - val_loss: 0.6426 - val_f1: 0.8426\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.6136 - f1: 0.8577 - val_loss: 0.4774 - val_f1: 0.9076\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5986 - f1: 0.8600 - val_loss: 0.4674 - val_f1: 0.9113\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5890 - f1: 0.8651 - val_loss: 0.5745 - val_f1: 0.8504\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5832 - f1: 0.8673 - val_loss: 0.4203 - val_f1: 0.9287\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.5458 - f1: 0.8784 - val_loss: 0.4615 - val_f1: 0.9106\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.5621 - f1: 0.8734 - val_loss: 0.4733 - val_f1: 0.8998\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5393 - f1: 0.8793 - val_loss: 0.4920 - val_f1: 0.8824\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.5714 - f1: 0.8732 - val_loss: 0.5160 - val_f1: 0.8854\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 0.5181 - f1: 0.8903 - val_loss: 0.4065 - val_f1: 0.9363\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 0.5375 - f1: 0.8832 - val_loss: 0.4100 - val_f1: 0.9324\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 0.5443 - f1: 0.8821 - val_loss: 0.4829 - val_f1: 0.8907\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5393 - f1: 0.8805 - val_loss: 0.4230 - val_f1: 0.9218\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 0.5245 - f1: 0.8854 - val_loss: 0.4219 - val_f1: 0.9137\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 7s 346us/sample - loss: 0.5503 - f1: 0.8789 - val_loss: 0.4371 - val_f1: 0.9134\n",
      "Running through fold 4\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 7s 329us/sample - loss: 4.3225 - f1: 0.4199 - val_loss: 1.3541 - val_f1: 0.6951\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 1.0813 - f1: 0.7660 - val_loss: 0.7622 - val_f1: 0.8524\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.7958 - f1: 0.8226 - val_loss: 0.6245 - val_f1: 0.8798\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6973 - f1: 0.8466 - val_loss: 0.5800 - val_f1: 0.8945\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6351 - f1: 0.8603 - val_loss: 0.5763 - val_f1: 0.8783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.6209 - f1: 0.8687 - val_loss: 0.5600 - val_f1: 0.8807\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5797 - f1: 0.8809 - val_loss: 0.4857 - val_f1: 0.9080\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5618 - f1: 0.8822 - val_loss: 0.4564 - val_f1: 0.9281\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5418 - f1: 0.8927 - val_loss: 0.4598 - val_f1: 0.9228\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5338 - f1: 0.8922 - val_loss: 0.4590 - val_f1: 0.9145\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5525 - f1: 0.8853 - val_loss: 0.5370 - val_f1: 0.8803\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 136us/sample - loss: 0.4883 - f1: 0.9078 - val_loss: 0.4447 - val_f1: 0.9145\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 134us/sample - loss: 0.5118 - f1: 0.8954 - val_loss: 0.4444 - val_f1: 0.9278\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5167 - f1: 0.8971 - val_loss: 0.4202 - val_f1: 0.9415\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.4830 - f1: 0.9102 - val_loss: 0.3703 - val_f1: 0.9564\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5057 - f1: 0.9004 - val_loss: 0.5075 - val_f1: 0.8933\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 135us/sample - loss: 0.5262 - f1: 0.8939 - val_loss: 0.3753 - val_f1: 0.9548\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 133us/sample - loss: 0.5337 - f1: 0.8950 - val_loss: 0.4462 - val_f1: 0.9237\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.4815 - f1: 0.9059 - val_loss: 0.4079 - val_f1: 0.9306\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.4992 - f1: 0.9012 - val_loss: 0.5013 - val_f1: 0.8904\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.5056 - f1: 0.8999 - val_loss: 0.4795 - val_f1: 0.9057\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 141us/sample - loss: 0.4936 - f1: 0.9049 - val_loss: 0.3954 - val_f1: 0.9419\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.4922 - f1: 0.9061 - val_loss: 0.3769 - val_f1: 0.9406\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.4887 - f1: 0.9037 - val_loss: 0.3734 - val_f1: 0.9472\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 7s 352us/sample - loss: 0.4809 - f1: 0.9092 - val_loss: 0.3410 - val_f1: 0.9611\n"
     ]
    }
   ],
   "source": [
    "mlb=LabelBinarizer()\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    print('\\n\\nRunning through training size '+str(train_size))\n",
    "    k_folds_errors = []\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, train_size=train_size)\n",
    "    k = 0\n",
    "    for train_index, _ in sss.split(training_spectra, training_keys):\n",
    "        print('Running through fold '+str(k))\n",
    "        training_keys_binarized = mlb.fit_transform(training_keys.reshape([training_keys.shape[0],1]))\n",
    "        testing_keys_binarized = mlb.transform(testing_keys)\n",
    "        model = compile_model(\n",
    "            build_dnn_model,\n",
    "            model_features)\n",
    "\n",
    "        csv_logger = tf.keras.callbacks.CSVLogger('./final-models-keras/'+model_id_save_as+'_trainsize'+str(train_size)+'_fold'+str(k)+'.log')\n",
    "\n",
    "        output = model.fit(\n",
    "            x=training_spectra_scaled[train_index],\n",
    "            y=training_keys_binarized[train_index],\n",
    "            epochs=num_epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(testing_spectra_scaled,\n",
    "                             testing_keys_binarized),\n",
    "            shuffle=True,\n",
    "            callbacks=[earlystop_callback, csv_logger],\n",
    "        )\n",
    "        model.save('./final-models-keras/'+model_id_save_as+'_trainsize'+str(train_size)+'_fold'+str(k)+'.hdf5')\n",
    "        k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36_update)",
   "language": "python",
   "name": "conda_tensorflow_p36_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

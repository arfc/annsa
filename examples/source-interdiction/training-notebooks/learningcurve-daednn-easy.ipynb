{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_device_id = str(4)\n",
    "model_id_save_as = 'learningcurve-daednn-easy-final'\n",
    "architecture_id = 'final-models/learningcurve-dnn-easy-final-features'\n",
    "model_class_id = 'DNN'\n",
    "testing_dataset_id = '../../source-interdiction/dataset_generation/validation_dataset_200keV_log10time_100.npy'\n",
    "training_dataset_id = '../../source-interdiction/dataset_generation/training_dataset_200keV_log10time_10000.npy'\n",
    "difficulty_setting = 'easy'\n",
    "\n",
    "train_sizes = [50, 100, 500, 1000, 5000, 10000, 15000, 20000,]\n",
    "earlystop_patience = 10\n",
    "num_epochs = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = GPU_device_id\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(5)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model, training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from annsa.model_classes import build_dnn_model, compile_model, f1\n",
    "from annsa.load_dataset import load_easy, load_full, dataset_to_spectrakeys\n",
    "from annsa.load_pretrained_network import load_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = np.load(training_dataset_id)\n",
    "training_spectra, training_keys = dataset_to_spectrakeys(training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = np.load(testing_dataset_id)\n",
    "testing_spectra, testing_keys = dataset_to_spectrakeys(testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = load_features(architecture_id)\n",
    "model_features.input_dim = 1024\n",
    "model_features.dropout_rate = model_features.dropout_probability\n",
    "model_features.loss = tf.keras.losses.categorical_crossentropy\n",
    "model_features.optimizer = tf.keras.optimizers.Adam\n",
    "model_features.output_function = tf.nn.softmax\n",
    "model_features.metrics = [f1]\n",
    "model_features.learning_rate = model_features.learining_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dae_model = load_model('./final-models-keras/daepretrain-easy-final.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_spectra_scaled = model_features.scaler.transform(training_spectra)\n",
    "testing_spectra_scaled = model_features.scaler.transform(testing_spectra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_f1',\n",
    "    patience=earlystop_patience,\n",
    "    mode='max',\n",
    "    min_delta=0.01,\n",
    "    restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Running through training size 50\n",
      "Running through fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36_update/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1639: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 14ms/sample - loss: 44.8959 - f1: 0.0000e+00 - val_loss: 43.8906 - val_f1: 0.0221\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.3198 - f1: 0.0437 - val_loss: 42.6352 - val_f1: 0.0303\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 41.8928 - f1: 0.0434 - val_loss: 41.5152 - val_f1: 0.0539\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.9294 - f1: 0.0469 - val_loss: 40.4604 - val_f1: 0.0608\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 39.8346 - f1: 0.0882 - val_loss: 39.3656 - val_f1: 0.0473\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 38.3892 - f1: 0.0453 - val_loss: 38.1758 - val_f1: 0.0185\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 37.0418 - f1: 0.0000e+00 - val_loss: 37.0398 - val_f1: 0.0204\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 36.1246 - f1: 0.0169 - val_loss: 36.0272 - val_f1: 0.0359\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 34.7824 - f1: 0.0730 - val_loss: 35.0660 - val_f1: 0.0488\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.8338 - f1: 0.0806 - val_loss: 34.2688 - val_f1: 0.0619\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.3220 - f1: 0.0345 - val_loss: 33.6014 - val_f1: 0.0889\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 32.4198 - f1: 0.0794 - val_loss: 32.7959 - val_f1: 0.0688\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.5259 - f1: 0.1589 - val_loss: 32.1309 - val_f1: 0.0482\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 30.9540 - f1: 0.1098 - val_loss: 31.5447 - val_f1: 0.0476\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 30.3255 - f1: 0.1097 - val_loss: 30.9316 - val_f1: 0.0486\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.6055 - f1: 0.1216 - val_loss: 30.3239 - val_f1: 0.0611\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.4117 - f1: 0.1255 - val_loss: 29.7665 - val_f1: 0.0727\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 28.5870 - f1: 0.1422 - val_loss: 29.2638 - val_f1: 0.0802\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 28.4761 - f1: 0.1650 - val_loss: 28.7338 - val_f1: 0.0822\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 27.7790 - f1: 0.1734 - val_loss: 28.1667 - val_f1: 0.0752\n",
      "Epoch 21/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 27.1079 - f1: 0.1571 - val_loss: 27.6934 - val_f1: 0.0706\n",
      "Running through fold 1\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 11ms/sample - loss: 44.9244 - f1: 0.0317 - val_loss: 43.0236 - val_f1: 0.0282\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 43.2865 - f1: 0.0312 - val_loss: 42.0306 - val_f1: 0.0403\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 41.8311 - f1: 0.0000e+00 - val_loss: 40.8686 - val_f1: 0.0565\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.4776 - f1: 0.0476 - val_loss: 40.0011 - val_f1: 0.0847\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 39.3975 - f1: 0.0903 - val_loss: 39.1262 - val_f1: 0.0853\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 38.8112 - f1: 0.0911 - val_loss: 38.1526 - val_f1: 0.0829\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 37.3415 - f1: 0.1424 - val_loss: 37.0888 - val_f1: 0.0771\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 37.0136 - f1: 0.1082 - val_loss: 36.0469 - val_f1: 0.0633\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 35.8035 - f1: 0.0656 - val_loss: 35.0892 - val_f1: 0.0715\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 34.3098 - f1: 0.1513 - val_loss: 34.3972 - val_f1: 0.0945\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.8764 - f1: 0.1211 - val_loss: 33.7884 - val_f1: 0.0927\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.1023 - f1: 0.2107 - val_loss: 33.1665 - val_f1: 0.0895\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 32.4125 - f1: 0.1584 - val_loss: 32.5503 - val_f1: 0.0889\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.3438 - f1: 0.2341 - val_loss: 31.9552 - val_f1: 0.0933\n",
      "Running through fold 2\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 11ms/sample - loss: 43.9931 - f1: 0.0156 - val_loss: 43.1311 - val_f1: 0.0383\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.5394 - f1: 0.0595 - val_loss: 42.0201 - val_f1: 0.0543\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 41.1456 - f1: 0.0588 - val_loss: 40.7618 - val_f1: 0.0821\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.0509 - f1: 0.1185 - val_loss: 39.7272 - val_f1: 0.1078\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 39.2122 - f1: 0.1092 - val_loss: 38.8022 - val_f1: 0.0976\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 38.1594 - f1: 0.1365 - val_loss: 37.9261 - val_f1: 0.0922\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 37.2311 - f1: 0.0903 - val_loss: 37.0403 - val_f1: 0.0927\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 36.4007 - f1: 0.1151 - val_loss: 36.1929 - val_f1: 0.0949\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 35.5797 - f1: 0.1333 - val_loss: 35.3640 - val_f1: 0.1041\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 34.6772 - f1: 0.1389 - val_loss: 34.5572 - val_f1: 0.1181\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.9845 - f1: 0.1627 - val_loss: 33.7772 - val_f1: 0.1155\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.1877 - f1: 0.1689 - val_loss: 33.0166 - val_f1: 0.1169\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 32.4698 - f1: 0.1261 - val_loss: 32.2986 - val_f1: 0.1213\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.7740 - f1: 0.1539 - val_loss: 31.6491 - val_f1: 0.1272\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.1386 - f1: 0.2063 - val_loss: 31.0536 - val_f1: 0.1144\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 30.6190 - f1: 0.1206 - val_loss: 30.5004 - val_f1: 0.1033\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.9776 - f1: 0.1482 - val_loss: 29.9455 - val_f1: 0.1042\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.4717 - f1: 0.1382 - val_loss: 29.4006 - val_f1: 0.1053\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 28.8580 - f1: 0.1643 - val_loss: 28.8530 - val_f1: 0.1058\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 28.4293 - f1: 0.1660 - val_loss: 28.3335 - val_f1: 0.1055\n",
      "Running through fold 3\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 11ms/sample - loss: 44.6476 - f1: 0.0159 - val_loss: 43.5484 - val_f1: 0.0243\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 42.8007 - f1: 0.0447 - val_loss: 42.0226 - val_f1: 0.0537\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.8872 - f1: 0.1492 - val_loss: 41.0253 - val_f1: 0.0580\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.2227 - f1: 0.1055 - val_loss: 39.9014 - val_f1: 0.0626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 38.6086 - f1: 0.0747 - val_loss: 38.6886 - val_f1: 0.0857\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 37.4182 - f1: 0.1690 - val_loss: 37.8027 - val_f1: 0.0947\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 36.7279 - f1: 0.1079 - val_loss: 37.1231 - val_f1: 0.0700\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 35.6526 - f1: 0.1048 - val_loss: 36.2339 - val_f1: 0.0565\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 34.7674 - f1: 0.1310 - val_loss: 35.2878 - val_f1: 0.0654\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.6065 - f1: 0.1989 - val_loss: 34.4577 - val_f1: 0.0804\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 32.8593 - f1: 0.1664 - val_loss: 33.6572 - val_f1: 0.0948\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.6637 - f1: 0.2127 - val_loss: 32.9747 - val_f1: 0.1130\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.4003 - f1: 0.1788 - val_loss: 32.4236 - val_f1: 0.0989\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 30.2627 - f1: 0.1287 - val_loss: 31.9217 - val_f1: 0.0934\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.7458 - f1: 0.1835 - val_loss: 31.2881 - val_f1: 0.1011\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.2430 - f1: 0.1577 - val_loss: 30.5987 - val_f1: 0.1143\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 28.4718 - f1: 0.2024 - val_loss: 29.9641 - val_f1: 0.1264\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 27.9525 - f1: 0.1778 - val_loss: 29.3530 - val_f1: 0.1253\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 27.2251 - f1: 0.2216 - val_loss: 28.8193 - val_f1: 0.1117\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 26.5083 - f1: 0.1874 - val_loss: 28.2599 - val_f1: 0.1009\n",
      "Epoch 21/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 25.9689 - f1: 0.1803 - val_loss: 27.6457 - val_f1: 0.1094\n",
      "Epoch 22/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 25.4214 - f1: 0.2825 - val_loss: 27.0636 - val_f1: 0.1096\n",
      "Epoch 23/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 24.5339 - f1: 0.2249 - val_loss: 26.4938 - val_f1: 0.1138\n",
      "Epoch 24/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 24.1521 - f1: 0.2054 - val_loss: 25.9605 - val_f1: 0.1203\n",
      "Epoch 25/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 23.5827 - f1: 0.2662 - val_loss: 25.4285 - val_f1: 0.1244\n",
      "Epoch 26/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 22.8752 - f1: 0.3427 - val_loss: 24.9422 - val_f1: 0.1216\n",
      "Epoch 27/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 22.6644 - f1: 0.3167 - val_loss: 24.4760 - val_f1: 0.1294\n",
      "Running through fold 4\n",
      "Train on 50 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "50/50 [==============================] - 1s 11ms/sample - loss: 43.4010 - f1: 0.0164 - val_loss: 42.3400 - val_f1: 0.0617\n",
      "Epoch 2/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 41.6048 - f1: 0.0929 - val_loss: 41.0890 - val_f1: 0.0592\n",
      "Epoch 3/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 40.5982 - f1: 0.0754 - val_loss: 40.0610 - val_f1: 0.0441\n",
      "Epoch 4/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 39.7718 - f1: 0.0786 - val_loss: 39.1420 - val_f1: 0.0498\n",
      "Epoch 5/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 38.4552 - f1: 0.0972 - val_loss: 38.1576 - val_f1: 0.0591\n",
      "Epoch 6/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 37.8326 - f1: 0.1047 - val_loss: 37.2201 - val_f1: 0.0631\n",
      "Epoch 7/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 36.6502 - f1: 0.1080 - val_loss: 36.2911 - val_f1: 0.0674\n",
      "Epoch 8/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 35.6484 - f1: 0.0820 - val_loss: 35.4224 - val_f1: 0.0751\n",
      "Epoch 9/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 34.6191 - f1: 0.1206 - val_loss: 34.3979 - val_f1: 0.0839\n",
      "Epoch 10/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 33.7675 - f1: 0.1545 - val_loss: 33.4175 - val_f1: 0.0942\n",
      "Epoch 11/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 32.9092 - f1: 0.1677 - val_loss: 32.5052 - val_f1: 0.0954\n",
      "Epoch 12/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.8544 - f1: 0.1255 - val_loss: 31.7676 - val_f1: 0.0745\n",
      "Epoch 13/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 31.1524 - f1: 0.1395 - val_loss: 31.0829 - val_f1: 0.0885\n",
      "Epoch 14/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 30.5059 - f1: 0.1917 - val_loss: 30.3818 - val_f1: 0.0961\n",
      "Epoch 15/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.6422 - f1: 0.2045 - val_loss: 29.7203 - val_f1: 0.1039\n",
      "Epoch 16/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 29.1168 - f1: 0.1646 - val_loss: 29.1467 - val_f1: 0.1026\n",
      "Epoch 17/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 28.6913 - f1: 0.1757 - val_loss: 28.6434 - val_f1: 0.1196\n",
      "Epoch 18/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 28.3766 - f1: 0.1843 - val_loss: 28.2253 - val_f1: 0.1290\n",
      "Epoch 19/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 27.1437 - f1: 0.2341 - val_loss: 27.8096 - val_f1: 0.1273\n",
      "Epoch 20/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 27.4189 - f1: 0.2005 - val_loss: 27.4138 - val_f1: 0.1301\n",
      "Epoch 21/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 26.2114 - f1: 0.2976 - val_loss: 26.9886 - val_f1: 0.1305\n",
      "Epoch 22/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 25.8292 - f1: 0.2699 - val_loss: 26.4824 - val_f1: 0.1282\n",
      "Epoch 23/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 26.5653 - f1: 0.1952 - val_loss: 26.0062 - val_f1: 0.1355\n",
      "Epoch 24/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 25.0755 - f1: 0.3024 - val_loss: 25.5706 - val_f1: 0.1475\n",
      "Epoch 25/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 24.6010 - f1: 0.3318 - val_loss: 25.2122 - val_f1: 0.1598\n",
      "Epoch 26/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 24.2857 - f1: 0.3795 - val_loss: 24.7931 - val_f1: 0.1593\n",
      "Epoch 27/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 23.8550 - f1: 0.3758 - val_loss: 24.4125 - val_f1: 0.1515\n",
      "Epoch 28/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 23.6667 - f1: 0.2409 - val_loss: 24.1831 - val_f1: 0.1480\n",
      "Epoch 29/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 23.3919 - f1: 0.2679 - val_loss: 23.8948 - val_f1: 0.1519\n",
      "Epoch 30/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 23.3388 - f1: 0.2185 - val_loss: 23.4940 - val_f1: 0.1672\n",
      "Epoch 31/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 22.5967 - f1: 0.3229 - val_loss: 23.2349 - val_f1: 0.1737\n",
      "Epoch 32/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 22.3717 - f1: 0.3485 - val_loss: 23.0098 - val_f1: 0.1740\n",
      "Epoch 33/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 21.9723 - f1: 0.3732 - val_loss: 22.7487 - val_f1: 0.1750\n",
      "Epoch 34/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 21.9486 - f1: 0.2916 - val_loss: 22.4661 - val_f1: 0.1754\n",
      "Epoch 35/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 21.5254 - f1: 0.3433 - val_loss: 22.2041 - val_f1: 0.1663\n",
      "Epoch 36/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 21.2218 - f1: 0.2714 - val_loss: 22.0974 - val_f1: 0.1449\n",
      "Epoch 37/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 20.8591 - f1: 0.2435 - val_loss: 21.9164 - val_f1: 0.1428\n",
      "Epoch 38/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 3ms/sample - loss: 20.7961 - f1: 0.2576 - val_loss: 21.4317 - val_f1: 0.1684\n",
      "Epoch 39/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 20.8646 - f1: 0.3166 - val_loss: 21.0680 - val_f1: 0.1990\n",
      "Epoch 40/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 20.3952 - f1: 0.3300 - val_loss: 20.9378 - val_f1: 0.1958\n",
      "Epoch 41/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 20.3585 - f1: 0.3022 - val_loss: 20.8749 - val_f1: 0.1942\n",
      "Epoch 42/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 19.8789 - f1: 0.3256 - val_loss: 20.7101 - val_f1: 0.1912\n",
      "Epoch 43/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 19.7274 - f1: 0.3542 - val_loss: 20.4326 - val_f1: 0.1983\n",
      "Epoch 44/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 19.7387 - f1: 0.3124 - val_loss: 20.1606 - val_f1: 0.2025\n",
      "Epoch 45/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 19.2316 - f1: 0.3862 - val_loss: 20.0020 - val_f1: 0.1910\n",
      "Epoch 46/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 18.8839 - f1: 0.3368 - val_loss: 19.8394 - val_f1: 0.1857\n",
      "Epoch 47/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 18.8134 - f1: 0.3732 - val_loss: 19.6507 - val_f1: 0.1837\n",
      "Epoch 48/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 18.6989 - f1: 0.3649 - val_loss: 19.4911 - val_f1: 0.1887\n",
      "Epoch 49/2000\n",
      "50/50 [==============================] - 0s 3ms/sample - loss: 18.6552 - f1: 0.3040 - val_loss: 19.2775 - val_f1: 0.1959\n",
      "\n",
      "\n",
      "Running through training size 100\n",
      "Running through fold 0\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 44.3320 - f1: 0.0159 - val_loss: 42.5182 - val_f1: 0.0319\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 41.8084 - f1: 0.0472 - val_loss: 40.4037 - val_f1: 0.0600\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 40.0760 - f1: 0.0391 - val_loss: 38.5949 - val_f1: 0.0807\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.2038 - f1: 0.1406 - val_loss: 36.7480 - val_f1: 0.0883\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.4142 - f1: 0.1334 - val_loss: 35.2259 - val_f1: 0.0796\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 34.5731 - f1: 0.1265 - val_loss: 33.8562 - val_f1: 0.0789\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 33.3591 - f1: 0.0708 - val_loss: 32.2472 - val_f1: 0.1007\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 31.6863 - f1: 0.0960 - val_loss: 31.1930 - val_f1: 0.0927\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 30.7072 - f1: 0.1343 - val_loss: 29.8818 - val_f1: 0.1281\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 29.5207 - f1: 0.1824 - val_loss: 28.5624 - val_f1: 0.1509\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 28.1041 - f1: 0.1926 - val_loss: 27.4213 - val_f1: 0.1082\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 26.9366 - f1: 0.1598 - val_loss: 26.5752 - val_f1: 0.0895\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 25.7273 - f1: 0.1833 - val_loss: 25.5318 - val_f1: 0.1226\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 25.0120 - f1: 0.1528 - val_loss: 24.9944 - val_f1: 0.0928\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 24.3130 - f1: 0.1561 - val_loss: 24.1445 - val_f1: 0.1088\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.4469 - f1: 0.1211 - val_loss: 23.4369 - val_f1: 0.1616\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.9625 - f1: 0.1540 - val_loss: 22.8282 - val_f1: 0.1714\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 22.0401 - f1: 0.1643 - val_loss: 22.2671 - val_f1: 0.1650\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 21.3875 - f1: 0.1458 - val_loss: 21.8571 - val_f1: 0.1574\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.0876 - f1: 0.3008 - val_loss: 21.4588 - val_f1: 0.1719\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.4350 - f1: 0.2343 - val_loss: 20.9440 - val_f1: 0.1663\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.9406 - f1: 0.1778 - val_loss: 20.4317 - val_f1: 0.1547\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.3431 - f1: 0.2591 - val_loss: 19.9835 - val_f1: 0.1610\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.0049 - f1: 0.2315 - val_loss: 19.6966 - val_f1: 0.1725\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 18.7122 - f1: 0.2443 - val_loss: 19.2509 - val_f1: 0.1776\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.2518 - f1: 0.1695 - val_loss: 18.7692 - val_f1: 0.1987\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 1ms/sample - loss: 17.6231 - f1: 0.2180 - val_loss: 18.3335 - val_f1: 0.2021\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.2456 - f1: 0.3816 - val_loss: 17.8656 - val_f1: 0.2071\n",
      "Epoch 29/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.7048 - f1: 0.2230 - val_loss: 17.7277 - val_f1: 0.1783\n",
      "Epoch 30/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.5214 - f1: 0.3249 - val_loss: 17.4011 - val_f1: 0.1619\n",
      "Epoch 31/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.2677 - f1: 0.1864 - val_loss: 16.7438 - val_f1: 0.1672\n",
      "Epoch 32/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.8456 - f1: 0.1695 - val_loss: 16.1253 - val_f1: 0.1709\n",
      "Epoch 33/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.2563 - f1: 0.2406 - val_loss: 15.6627 - val_f1: 0.1571\n",
      "Epoch 34/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.9000 - f1: 0.1539 - val_loss: 15.1761 - val_f1: 0.1488\n",
      "Epoch 35/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.2789 - f1: 0.2414 - val_loss: 14.7207 - val_f1: 0.1399\n",
      "Epoch 36/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 13.5837 - f1: 0.2548 - val_loss: 14.4036 - val_f1: 0.1429\n",
      "Running through fold 1\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 43.9293 - f1: 0.0234 - val_loss: 41.4010 - val_f1: 0.0947\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.8926 - f1: 0.1260 - val_loss: 38.9310 - val_f1: 0.1184\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.4574 - f1: 0.2054 - val_loss: 37.3185 - val_f1: 0.1355\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.8927 - f1: 0.1016 - val_loss: 35.6408 - val_f1: 0.1456\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 34.9990 - f1: 0.1109 - val_loss: 33.9071 - val_f1: 0.1341\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 32.8461 - f1: 0.1894 - val_loss: 32.2523 - val_f1: 0.1373\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 31.0203 - f1: 0.2434 - val_loss: 30.6229 - val_f1: 0.1531\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 29.5961 - f1: 0.2197 - val_loss: 29.2420 - val_f1: 0.1701\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 28.4487 - f1: 0.1466 - val_loss: 27.8475 - val_f1: 0.1642\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.8068 - f1: 0.2355 - val_loss: 26.6867 - val_f1: 0.1850\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 25.4819 - f1: 0.2447 - val_loss: 25.6303 - val_f1: 0.1771\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.9763 - f1: 0.1975 - val_loss: 24.8133 - val_f1: 0.1549\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.2497 - f1: 0.1682 - val_loss: 23.9675 - val_f1: 0.1372\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.2425 - f1: 0.2333 - val_loss: 22.9360 - val_f1: 0.1533\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.2960 - f1: 0.2326 - val_loss: 21.8699 - val_f1: 0.1581\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.3603 - f1: 0.2054 - val_loss: 21.0691 - val_f1: 0.1455\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.5642 - f1: 0.1818 - val_loss: 19.9983 - val_f1: 0.1215\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.9641 - f1: 0.1179 - val_loss: 19.2715 - val_f1: 0.1130\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.3000 - f1: 0.2143 - val_loss: 18.7180 - val_f1: 0.1213\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.9433 - f1: 0.0888 - val_loss: 18.2014 - val_f1: 0.1350\n",
      "Running through fold 2\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 42.8888 - f1: 0.0481 - val_loss: 41.0695 - val_f1: 0.0780\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.5122 - f1: 0.0561 - val_loss: 39.6260 - val_f1: 0.0506\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.5344 - f1: 0.0629 - val_loss: 37.7019 - val_f1: 0.0731\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.4681 - f1: 0.1197 - val_loss: 35.4700 - val_f1: 0.0862\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 34.4002 - f1: 0.1594 - val_loss: 34.1623 - val_f1: 0.1055\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 33.0548 - f1: 0.1128 - val_loss: 32.8019 - val_f1: 0.1262\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 31.6322 - f1: 0.1350 - val_loss: 31.4333 - val_f1: 0.1456\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 30.4503 - f1: 0.1651 - val_loss: 30.2416 - val_f1: 0.1750\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 29.5119 - f1: 0.2186 - val_loss: 29.1811 - val_f1: 0.1797\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 28.0504 - f1: 0.2060 - val_loss: 28.2211 - val_f1: 0.1776\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 27.2251 - f1: 0.1844 - val_loss: 27.2721 - val_f1: 0.1733\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.1178 - f1: 0.2536 - val_loss: 26.5055 - val_f1: 0.1858\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 25.7100 - f1: 0.2250 - val_loss: 25.9179 - val_f1: 0.1779\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.9638 - f1: 0.1726 - val_loss: 25.2587 - val_f1: 0.1842\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.1190 - f1: 0.3309 - val_loss: 24.6495 - val_f1: 0.1849\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.7644 - f1: 0.1947 - val_loss: 24.3300 - val_f1: 0.1778\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.3489 - f1: 0.2844 - val_loss: 23.7867 - val_f1: 0.1870\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.8009 - f1: 0.1722 - val_loss: 23.2025 - val_f1: 0.1993\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.9800 - f1: 0.2680 - val_loss: 22.5607 - val_f1: 0.2087\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.5193 - f1: 0.3138 - val_loss: 22.3695 - val_f1: 0.1709\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.1757 - f1: 0.2420 - val_loss: 21.8295 - val_f1: 0.1499\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.5375 - f1: 0.1774 - val_loss: 21.2541 - val_f1: 0.1843\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.1373 - f1: 0.2702 - val_loss: 20.8402 - val_f1: 0.1917\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.4619 - f1: 0.3007 - val_loss: 20.4248 - val_f1: 0.1937\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.1162 - f1: 0.2895 - val_loss: 20.1044 - val_f1: 0.2047\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.7575 - f1: 0.3106 - val_loss: 19.7380 - val_f1: 0.2086\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.3126 - f1: 0.3118 - val_loss: 19.4539 - val_f1: 0.1978\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.0700 - f1: 0.2962 - val_loss: 18.9979 - val_f1: 0.1961\n",
      "Running through fold 3\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 42.8599 - f1: 0.0484 - val_loss: 41.5047 - val_f1: 0.0702\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.5298 - f1: 0.1187 - val_loss: 39.7193 - val_f1: 0.0865\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.6123 - f1: 0.1575 - val_loss: 38.2592 - val_f1: 0.0905\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 37.4512 - f1: 0.0864 - val_loss: 36.9032 - val_f1: 0.0921\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 35.5346 - f1: 0.1274 - val_loss: 35.5016 - val_f1: 0.0921\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 34.2268 - f1: 0.0881 - val_loss: 34.1764 - val_f1: 0.0889\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 33.0619 - f1: 0.1177 - val_loss: 32.9472 - val_f1: 0.1063\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 31.6837 - f1: 0.1280 - val_loss: 31.7870 - val_f1: 0.1080\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 30.7282 - f1: 0.1861 - val_loss: 30.8029 - val_f1: 0.1114\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 29.6556 - f1: 0.1155 - val_loss: 29.9481 - val_f1: 0.1156\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 28.8694 - f1: 0.1832 - val_loss: 29.1241 - val_f1: 0.1293\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 28.0579 - f1: 0.2153 - val_loss: 28.4136 - val_f1: 0.1420\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 27.3993 - f1: 0.2127 - val_loss: 27.9217 - val_f1: 0.1323\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.7654 - f1: 0.2114 - val_loss: 27.3669 - val_f1: 0.1272\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.0923 - f1: 0.2175 - val_loss: 26.7834 - val_f1: 0.1281\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 25.6236 - f1: 0.2282 - val_loss: 26.1634 - val_f1: 0.1366\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.8391 - f1: 0.2422 - val_loss: 25.5514 - val_f1: 0.1595\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.5484 - f1: 0.2302 - val_loss: 24.9128 - val_f1: 0.1607\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.8781 - f1: 0.1947 - val_loss: 24.5903 - val_f1: 0.1230\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.3047 - f1: 0.1520 - val_loss: 24.2364 - val_f1: 0.1195\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.3713 - f1: 0.1358 - val_loss: 23.5955 - val_f1: 0.1692\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.6084 - f1: 0.2475 - val_loss: 23.4995 - val_f1: 0.1528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.2301 - f1: 0.1821 - val_loss: 23.1664 - val_f1: 0.1354\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.8930 - f1: 0.1896 - val_loss: 22.8543 - val_f1: 0.1288\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.5600 - f1: 0.1900 - val_loss: 22.3685 - val_f1: 0.1461\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.9498 - f1: 0.2604 - val_loss: 22.2018 - val_f1: 0.1506\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.8936 - f1: 0.2792 - val_loss: 21.9530 - val_f1: 0.1481\n",
      "Running through fold 4\n",
      "Train on 100 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 1s 6ms/sample - loss: 43.0242 - f1: 0.0159 - val_loss: 41.3655 - val_f1: 0.0469\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 40.5253 - f1: 0.0485 - val_loss: 39.4717 - val_f1: 0.0833\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 38.1815 - f1: 0.0655 - val_loss: 37.6117 - val_f1: 0.0760\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 36.4526 - f1: 0.1269 - val_loss: 35.8240 - val_f1: 0.0692\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 34.9759 - f1: 0.0481 - val_loss: 34.4536 - val_f1: 0.0648\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 32.8864 - f1: 0.0840 - val_loss: 32.7866 - val_f1: 0.0798\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 31.6741 - f1: 0.0745 - val_loss: 31.3984 - val_f1: 0.0717\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 30.3131 - f1: 0.0664 - val_loss: 30.2070 - val_f1: 0.0716\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 29.0663 - f1: 0.1356 - val_loss: 29.0015 - val_f1: 0.0874\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 27.7401 - f1: 0.1115 - val_loss: 27.7947 - val_f1: 0.1202\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 26.6381 - f1: 0.1381 - val_loss: 27.0557 - val_f1: 0.1321\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 25.9135 - f1: 0.2203 - val_loss: 26.1578 - val_f1: 0.1357\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.9556 - f1: 0.1615 - val_loss: 25.3516 - val_f1: 0.1404\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 24.0720 - f1: 0.1530 - val_loss: 24.7422 - val_f1: 0.1558\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.4277 - f1: 0.2354 - val_loss: 24.0901 - val_f1: 0.1511\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 23.0857 - f1: 0.1640 - val_loss: 23.7555 - val_f1: 0.1552\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 22.3977 - f1: 0.2969 - val_loss: 23.2274 - val_f1: 0.1568\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.8098 - f1: 0.3315 - val_loss: 22.8156 - val_f1: 0.1621\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.3342 - f1: 0.2902 - val_loss: 22.3771 - val_f1: 0.1658\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 21.0611 - f1: 0.3423 - val_loss: 21.9610 - val_f1: 0.1544\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.5604 - f1: 0.2766 - val_loss: 21.5704 - val_f1: 0.1452\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 20.1633 - f1: 0.2864 - val_loss: 21.1496 - val_f1: 0.1628\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.6935 - f1: 0.3279 - val_loss: 20.7975 - val_f1: 0.1701\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.5391 - f1: 0.4105 - val_loss: 20.5521 - val_f1: 0.1559\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 19.3223 - f1: 0.3548 - val_loss: 20.1514 - val_f1: 0.1609\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.7437 - f1: 0.2918 - val_loss: 19.7446 - val_f1: 0.1681\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.5343 - f1: 0.2815 - val_loss: 19.4567 - val_f1: 0.1624\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.2794 - f1: 0.2145 - val_loss: 19.3571 - val_f1: 0.1612\n",
      "Epoch 29/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 18.1811 - f1: 0.2458 - val_loss: 19.1005 - val_f1: 0.1730\n",
      "Epoch 30/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.6311 - f1: 0.3231 - val_loss: 18.8381 - val_f1: 0.2086\n",
      "Epoch 31/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 17.2283 - f1: 0.3020 - val_loss: 18.6174 - val_f1: 0.2051\n",
      "Epoch 32/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.8728 - f1: 0.4058 - val_loss: 18.1799 - val_f1: 0.2058\n",
      "Epoch 33/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.7846 - f1: 0.3213 - val_loss: 18.0143 - val_f1: 0.1983\n",
      "Epoch 34/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.3580 - f1: 0.3983 - val_loss: 18.0261 - val_f1: 0.2058\n",
      "Epoch 35/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.3060 - f1: 0.3214 - val_loss: 17.9544 - val_f1: 0.2038\n",
      "Epoch 36/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 16.1981 - f1: 0.3422 - val_loss: 17.4496 - val_f1: 0.2188\n",
      "Epoch 37/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.7356 - f1: 0.4071 - val_loss: 17.1464 - val_f1: 0.2142\n",
      "Epoch 38/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.6753 - f1: 0.2960 - val_loss: 16.9425 - val_f1: 0.2296\n",
      "Epoch 39/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.4367 - f1: 0.3681 - val_loss: 16.7542 - val_f1: 0.2374\n",
      "Epoch 40/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.2619 - f1: 0.3726 - val_loss: 16.6513 - val_f1: 0.2339\n",
      "Epoch 41/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 15.0493 - f1: 0.3577 - val_loss: 16.6171 - val_f1: 0.2055\n",
      "Epoch 42/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.9935 - f1: 0.3845 - val_loss: 16.3967 - val_f1: 0.1810\n",
      "Epoch 43/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.7389 - f1: 0.3276 - val_loss: 16.1398 - val_f1: 0.1932\n",
      "Epoch 44/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.7031 - f1: 0.2706 - val_loss: 15.9622 - val_f1: 0.2025\n",
      "Epoch 45/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.3172 - f1: 0.4661 - val_loss: 15.7822 - val_f1: 0.2108\n",
      "Epoch 46/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.1567 - f1: 0.4645 - val_loss: 15.6353 - val_f1: 0.2047\n",
      "Epoch 47/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 14.0284 - f1: 0.4850 - val_loss: 15.4012 - val_f1: 0.2050\n",
      "Epoch 48/2000\n",
      "100/100 [==============================] - 0s 2ms/sample - loss: 13.7527 - f1: 0.2630 - val_loss: 15.1293 - val_f1: 0.2092\n",
      "\n",
      "\n",
      "Running through training size 500\n",
      "Running through fold 0\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 1ms/sample - loss: 39.3360 - f1: 0.0614 - val_loss: 34.3563 - val_f1: 0.0887\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 31.4504 - f1: 0.0980 - val_loss: 27.8274 - val_f1: 0.1202\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 25.7148 - f1: 0.1019 - val_loss: 23.2215 - val_f1: 0.1245\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 21.4682 - f1: 0.1231 - val_loss: 19.4685 - val_f1: 0.1134\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 18.1507 - f1: 0.1045 - val_loss: 16.5905 - val_f1: 0.0753\n",
      "Epoch 6/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 382us/sample - loss: 15.9611 - f1: 0.0719 - val_loss: 14.7079 - val_f1: 0.0625\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 379us/sample - loss: 14.2810 - f1: 0.0915 - val_loss: 13.3871 - val_f1: 0.1124\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 378us/sample - loss: 13.0070 - f1: 0.1282 - val_loss: 12.4159 - val_f1: 0.1268\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 12.0603 - f1: 0.1419 - val_loss: 11.5597 - val_f1: 0.1295\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 11.3104 - f1: 0.1585 - val_loss: 10.8097 - val_f1: 0.1440\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 10.5558 - f1: 0.1652 - val_loss: 10.2239 - val_f1: 0.1490\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 9.9754 - f1: 0.1386 - val_loss: 9.6331 - val_f1: 0.1607\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 9.4034 - f1: 0.1910 - val_loss: 9.0882 - val_f1: 0.1627\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 377us/sample - loss: 8.8357 - f1: 0.1923 - val_loss: 8.6465 - val_f1: 0.1736\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 8.3209 - f1: 0.1893 - val_loss: 8.1911 - val_f1: 0.1751\n",
      "Epoch 16/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 7.8787 - f1: 0.1830 - val_loss: 7.7563 - val_f1: 0.1905\n",
      "Epoch 17/2000\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 7.4870 - f1: 0.2275 - val_loss: 7.3730 - val_f1: 0.1947\n",
      "Epoch 18/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 7.0915 - f1: 0.2065 - val_loss: 7.0997 - val_f1: 0.2186\n",
      "Epoch 19/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 6.8669 - f1: 0.2164 - val_loss: 6.7798 - val_f1: 0.1907\n",
      "Epoch 20/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 6.5143 - f1: 0.2475 - val_loss: 6.5274 - val_f1: 0.2205\n",
      "Epoch 21/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 6.2818 - f1: 0.2553 - val_loss: 6.2699 - val_f1: 0.2254\n",
      "Epoch 22/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 6.0315 - f1: 0.2736 - val_loss: 6.0313 - val_f1: 0.2440\n",
      "Epoch 23/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 5.7985 - f1: 0.3056 - val_loss: 5.8946 - val_f1: 0.2324\n",
      "Epoch 24/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 5.6404 - f1: 0.2743 - val_loss: 5.6897 - val_f1: 0.2504\n",
      "Epoch 25/2000\n",
      "500/500 [==============================] - 0s 379us/sample - loss: 5.3771 - f1: 0.3060 - val_loss: 5.4849 - val_f1: 0.2488\n",
      "Epoch 26/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 5.2649 - f1: 0.2926 - val_loss: 5.3158 - val_f1: 0.2586\n",
      "Epoch 27/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 5.1105 - f1: 0.2966 - val_loss: 5.1126 - val_f1: 0.2927\n",
      "Epoch 28/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 4.8593 - f1: 0.3289 - val_loss: 4.9054 - val_f1: 0.2942\n",
      "Epoch 29/2000\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 4.6703 - f1: 0.3598 - val_loss: 4.7417 - val_f1: 0.2896\n",
      "Epoch 30/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 4.5172 - f1: 0.3286 - val_loss: 4.6072 - val_f1: 0.2934\n",
      "Epoch 31/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 4.3341 - f1: 0.3562 - val_loss: 4.4321 - val_f1: 0.3087\n",
      "Epoch 32/2000\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 4.2223 - f1: 0.3524 - val_loss: 4.3259 - val_f1: 0.3025\n",
      "Epoch 33/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 4.1272 - f1: 0.3583 - val_loss: 4.2504 - val_f1: 0.3162\n",
      "Epoch 34/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 3.9986 - f1: 0.3481 - val_loss: 4.1332 - val_f1: 0.3073\n",
      "Epoch 35/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 3.8583 - f1: 0.3619 - val_loss: 3.9622 - val_f1: 0.3081\n",
      "Epoch 36/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 3.8109 - f1: 0.3722 - val_loss: 3.9250 - val_f1: 0.3209\n",
      "Epoch 37/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 3.7107 - f1: 0.3935 - val_loss: 3.7993 - val_f1: 0.3455\n",
      "Epoch 38/2000\n",
      "500/500 [==============================] - 0s 383us/sample - loss: 3.5812 - f1: 0.4004 - val_loss: 3.6688 - val_f1: 0.3352\n",
      "Epoch 39/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 3.4472 - f1: 0.4315 - val_loss: 3.5438 - val_f1: 0.3673\n",
      "Epoch 40/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 3.3564 - f1: 0.4440 - val_loss: 3.4564 - val_f1: 0.3770\n",
      "Epoch 41/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 3.2798 - f1: 0.4487 - val_loss: 3.3741 - val_f1: 0.3956\n",
      "Epoch 42/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 3.1332 - f1: 0.4581 - val_loss: 3.2941 - val_f1: 0.3965\n",
      "Epoch 43/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 3.0546 - f1: 0.4809 - val_loss: 3.1485 - val_f1: 0.4211\n",
      "Epoch 44/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 3.0218 - f1: 0.4827 - val_loss: 3.1474 - val_f1: 0.3873\n",
      "Epoch 45/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 2.9557 - f1: 0.4582 - val_loss: 3.0571 - val_f1: 0.4247\n",
      "Epoch 46/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 2.8423 - f1: 0.5002 - val_loss: 2.9258 - val_f1: 0.4461\n",
      "Epoch 47/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 2.7484 - f1: 0.5347 - val_loss: 2.9400 - val_f1: 0.4433\n",
      "Epoch 48/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 2.6632 - f1: 0.5433 - val_loss: 2.9301 - val_f1: 0.4703\n",
      "Epoch 49/2000\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 2.6755 - f1: 0.5157 - val_loss: 2.7926 - val_f1: 0.4587\n",
      "Epoch 50/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 2.4946 - f1: 0.5995 - val_loss: 2.6461 - val_f1: 0.5483\n",
      "Epoch 51/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 2.4751 - f1: 0.5725 - val_loss: 2.6241 - val_f1: 0.5150\n",
      "Epoch 52/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 2.4453 - f1: 0.5829 - val_loss: 2.5349 - val_f1: 0.5274\n",
      "Epoch 53/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 2.3200 - f1: 0.6027 - val_loss: 2.5178 - val_f1: 0.5680\n",
      "Epoch 54/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 2.3329 - f1: 0.5842 - val_loss: 2.5045 - val_f1: 0.5076\n",
      "Epoch 55/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 2.3556 - f1: 0.5898 - val_loss: 2.4616 - val_f1: 0.5652\n",
      "Epoch 56/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 2.3051 - f1: 0.6377 - val_loss: 2.2666 - val_f1: 0.6142\n",
      "Epoch 57/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 2.1475 - f1: 0.6528 - val_loss: 2.2891 - val_f1: 0.6054\n",
      "Epoch 58/2000\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 2.1246 - f1: 0.6702 - val_loss: 2.2726 - val_f1: 0.6059\n",
      "Epoch 59/2000\n",
      "500/500 [==============================] - 0s 382us/sample - loss: 2.1318 - f1: 0.6615 - val_loss: 2.2292 - val_f1: 0.6211\n",
      "Epoch 60/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 2.0553 - f1: 0.7006 - val_loss: 2.1361 - val_f1: 0.6483\n",
      "Epoch 61/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 2.0001 - f1: 0.6993 - val_loss: 2.1029 - val_f1: 0.6677\n",
      "Epoch 62/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 1.9928 - f1: 0.7028 - val_loss: 2.0554 - val_f1: 0.6701\n",
      "Epoch 63/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 1.9967 - f1: 0.6770 - val_loss: 2.0441 - val_f1: 0.6836\n",
      "Epoch 64/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 1.9946 - f1: 0.7118 - val_loss: 2.1833 - val_f1: 0.6211\n",
      "Epoch 65/2000\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 2.0106 - f1: 0.6913 - val_loss: 1.9726 - val_f1: 0.7033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 1.8364 - f1: 0.7515 - val_loss: 1.9911 - val_f1: 0.6891\n",
      "Epoch 67/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 1.8361 - f1: 0.7470 - val_loss: 2.0079 - val_f1: 0.6942\n",
      "Epoch 68/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 1.9092 - f1: 0.7079 - val_loss: 2.0116 - val_f1: 0.6998\n",
      "Epoch 69/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 1.8734 - f1: 0.7458 - val_loss: 1.9352 - val_f1: 0.7200\n",
      "Epoch 70/2000\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 1.7784 - f1: 0.7578 - val_loss: 1.9655 - val_f1: 0.6941\n",
      "Epoch 71/2000\n",
      "500/500 [==============================] - 0s 381us/sample - loss: 1.7684 - f1: 0.7656 - val_loss: 1.9205 - val_f1: 0.7234\n",
      "Epoch 72/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 1.7419 - f1: 0.7815 - val_loss: 1.8637 - val_f1: 0.7417\n",
      "Epoch 73/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 1.7972 - f1: 0.7256 - val_loss: 1.9681 - val_f1: 0.6904\n",
      "Epoch 74/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 1.8328 - f1: 0.7198 - val_loss: 1.8268 - val_f1: 0.7366\n",
      "Epoch 75/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 1.7734 - f1: 0.7308 - val_loss: 1.8412 - val_f1: 0.7193\n",
      "Epoch 76/2000\n",
      "500/500 [==============================] - 0s 384us/sample - loss: 1.6866 - f1: 0.7829 - val_loss: 1.7838 - val_f1: 0.7509\n",
      "Epoch 77/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 1.5951 - f1: 0.8171 - val_loss: 1.7305 - val_f1: 0.7647\n",
      "Epoch 78/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 1.6637 - f1: 0.8028 - val_loss: 1.7830 - val_f1: 0.7376\n",
      "Epoch 79/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 1.6072 - f1: 0.7877 - val_loss: 1.7275 - val_f1: 0.7512\n",
      "Epoch 80/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 1.5740 - f1: 0.8145 - val_loss: 1.7035 - val_f1: 0.7647\n",
      "Epoch 81/2000\n",
      "500/500 [==============================] - 0s 387us/sample - loss: 1.5616 - f1: 0.8314 - val_loss: 1.6848 - val_f1: 0.7725\n",
      "Epoch 82/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 1.5212 - f1: 0.8281 - val_loss: 1.7132 - val_f1: 0.7644\n",
      "Epoch 83/2000\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 1.5295 - f1: 0.8122 - val_loss: 1.8239 - val_f1: 0.7160\n",
      "Epoch 84/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 1.6834 - f1: 0.7832 - val_loss: 1.7049 - val_f1: 0.7615\n",
      "Epoch 85/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 1.6731 - f1: 0.7829 - val_loss: 1.8281 - val_f1: 0.7319\n",
      "Epoch 86/2000\n",
      "500/500 [==============================] - 0s 386us/sample - loss: 1.6095 - f1: 0.8032 - val_loss: 1.7962 - val_f1: 0.7516\n",
      "Epoch 87/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 1.7100 - f1: 0.7729 - val_loss: 1.7788 - val_f1: 0.7483\n",
      "Running through fold 1\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 40.5067 - f1: 0.0526 - val_loss: 35.1597 - val_f1: 0.0418\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 32.7108 - f1: 0.0650 - val_loss: 28.8362 - val_f1: 0.1271\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 27.4087 - f1: 0.1246 - val_loss: 24.6356 - val_f1: 0.1768\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 23.5221 - f1: 0.2070 - val_loss: 21.7162 - val_f1: 0.2129\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 20.7313 - f1: 0.2360 - val_loss: 19.1967 - val_f1: 0.2450\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 18.2632 - f1: 0.2409 - val_loss: 16.7549 - val_f1: 0.2105\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 15.4758 - f1: 0.1909 - val_loss: 13.8461 - val_f1: 0.1822\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 13.0165 - f1: 0.1640 - val_loss: 12.1948 - val_f1: 0.1451\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 11.5383 - f1: 0.1660 - val_loss: 11.1826 - val_f1: 0.1889\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 10.6216 - f1: 0.1725 - val_loss: 10.3781 - val_f1: 0.1635\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 9.9418 - f1: 0.1746 - val_loss: 9.7705 - val_f1: 0.1511\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 9.2599 - f1: 0.1928 - val_loss: 9.1587 - val_f1: 0.1728\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 8.7312 - f1: 0.1878 - val_loss: 8.6294 - val_f1: 0.1792\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 385us/sample - loss: 8.2441 - f1: 0.1902 - val_loss: 8.2078 - val_f1: 0.1821\n",
      "Epoch 15/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 7.7563 - f1: 0.2055 - val_loss: 7.7350 - val_f1: 0.1711\n",
      "Running through fold 2\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 39.3935 - f1: 0.0920 - val_loss: 35.1160 - val_f1: 0.1176\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 32.6281 - f1: 0.1563 - val_loss: 29.6300 - val_f1: 0.1768\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 27.7330 - f1: 0.1659 - val_loss: 25.2786 - val_f1: 0.1769\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 23.8188 - f1: 0.2230 - val_loss: 22.2850 - val_f1: 0.2322\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 21.2279 - f1: 0.2095 - val_loss: 19.7065 - val_f1: 0.1970\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 18.5052 - f1: 0.1678 - val_loss: 16.8769 - val_f1: 0.1327\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 15.4294 - f1: 0.1050 - val_loss: 14.1943 - val_f1: 0.0819\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 388us/sample - loss: 13.3466 - f1: 0.0418 - val_loss: 12.6819 - val_f1: 0.0421\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 12.1610 - f1: 0.0278 - val_loss: 11.7597 - val_f1: 0.0438\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 395us/sample - loss: 11.3297 - f1: 0.0387 - val_loss: 10.9773 - val_f1: 0.0513\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 10.6171 - f1: 0.0382 - val_loss: 10.3577 - val_f1: 0.0557\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 9.9176 - f1: 0.0491 - val_loss: 9.7548 - val_f1: 0.0633\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 9.3826 - f1: 0.0841 - val_loss: 9.2527 - val_f1: 0.0915\n",
      "Epoch 14/2000\n",
      "500/500 [==============================] - 0s 389us/sample - loss: 8.9042 - f1: 0.1154 - val_loss: 8.7559 - val_f1: 0.0924\n",
      "Running through fold 3\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 39.7882 - f1: 0.1006 - val_loss: 35.3666 - val_f1: 0.1198\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 401us/sample - loss: 32.8216 - f1: 0.1428 - val_loss: 30.0343 - val_f1: 0.1714\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 28.2603 - f1: 0.1914 - val_loss: 26.2513 - val_f1: 0.1979\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 25.1549 - f1: 0.1953 - val_loss: 23.5944 - val_f1: 0.2038\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 22.5178 - f1: 0.2036 - val_loss: 21.5773 - val_f1: 0.1822\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 20.4038 - f1: 0.2160 - val_loss: 19.6717 - val_f1: 0.1894\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 18.6308 - f1: 0.2268 - val_loss: 17.4717 - val_f1: 0.1850\n",
      "Epoch 8/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s 397us/sample - loss: 16.6186 - f1: 0.2109 - val_loss: 15.6104 - val_f1: 0.1526\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 14.4066 - f1: 0.1804 - val_loss: 13.3698 - val_f1: 0.1082\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 12.2392 - f1: 0.0680 - val_loss: 11.4876 - val_f1: 0.0653\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 11.0098 - f1: 0.0730 - val_loss: 10.6108 - val_f1: 0.0849\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 398us/sample - loss: 10.1629 - f1: 0.0722 - val_loss: 9.9692 - val_f1: 0.0505\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 403us/sample - loss: 9.5369 - f1: 0.0552 - val_loss: 9.3446 - val_f1: 0.0431\n",
      "Running through fold 4\n",
      "Train on 500 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "500/500 [==============================] - 1s 2ms/sample - loss: 40.2013 - f1: 0.0634 - val_loss: 35.8292 - val_f1: 0.0838\n",
      "Epoch 2/2000\n",
      "500/500 [==============================] - 0s 396us/sample - loss: 33.4346 - f1: 0.1184 - val_loss: 30.2656 - val_f1: 0.1413\n",
      "Epoch 3/2000\n",
      "500/500 [==============================] - 0s 399us/sample - loss: 28.0365 - f1: 0.1595 - val_loss: 25.5506 - val_f1: 0.1628\n",
      "Epoch 4/2000\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 23.8894 - f1: 0.1422 - val_loss: 21.0276 - val_f1: 0.1327\n",
      "Epoch 5/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 19.4894 - f1: 0.0994 - val_loss: 16.6785 - val_f1: 0.0845\n",
      "Epoch 6/2000\n",
      "500/500 [==============================] - 0s 390us/sample - loss: 15.8167 - f1: 0.0784 - val_loss: 14.5251 - val_f1: 0.0887\n",
      "Epoch 7/2000\n",
      "500/500 [==============================] - 0s 392us/sample - loss: 13.8794 - f1: 0.0737 - val_loss: 13.0785 - val_f1: 0.0643\n",
      "Epoch 8/2000\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 12.6783 - f1: 0.0585 - val_loss: 11.9866 - val_f1: 0.0645\n",
      "Epoch 9/2000\n",
      "500/500 [==============================] - 0s 393us/sample - loss: 11.6816 - f1: 0.0664 - val_loss: 11.1080 - val_f1: 0.0835\n",
      "Epoch 10/2000\n",
      "500/500 [==============================] - 0s 394us/sample - loss: 10.8534 - f1: 0.0675 - val_loss: 10.3678 - val_f1: 0.0744\n",
      "Epoch 11/2000\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 10.1590 - f1: 0.0762 - val_loss: 9.7208 - val_f1: 0.0945\n",
      "Epoch 12/2000\n",
      "500/500 [==============================] - 0s 391us/sample - loss: 9.4982 - f1: 0.0812 - val_loss: 9.1250 - val_f1: 0.0957\n",
      "Epoch 13/2000\n",
      "500/500 [==============================] - 0s 397us/sample - loss: 8.9610 - f1: 0.0882 - val_loss: 8.5877 - val_f1: 0.1072\n",
      "\n",
      "\n",
      "Running through training size 1000\n",
      "Running through fold 0\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 876us/sample - loss: 35.6584 - f1: 0.0931 - val_loss: 28.8033 - val_f1: 0.1758\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 24.8055 - f1: 0.1440 - val_loss: 19.9675 - val_f1: 0.0921\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 17.2978 - f1: 0.0782 - val_loss: 15.0437 - val_f1: 0.0593\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 13.7739 - f1: 0.0718 - val_loss: 12.6360 - val_f1: 0.0684\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 11.8015 - f1: 0.0903 - val_loss: 11.0547 - val_f1: 0.0753\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 10.4330 - f1: 0.0899 - val_loss: 9.8621 - val_f1: 0.0939\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 9.3368 - f1: 0.1083 - val_loss: 8.8723 - val_f1: 0.0911\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 8.4389 - f1: 0.1129 - val_loss: 8.1317 - val_f1: 0.1218\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 7.7144 - f1: 0.1352 - val_loss: 7.3966 - val_f1: 0.1370\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 7.0847 - f1: 0.1547 - val_loss: 6.8421 - val_f1: 0.1157\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 6.5508 - f1: 0.1552 - val_loss: 6.3366 - val_f1: 0.1550\n",
      "Running through fold 1\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 895us/sample - loss: 36.6888 - f1: 0.0804 - val_loss: 29.5254 - val_f1: 0.1212\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 25.1447 - f1: 0.1311 - val_loss: 20.1903 - val_f1: 0.0982\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 16.4331 - f1: 0.0505 - val_loss: 14.0078 - val_f1: 0.0480\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 249us/sample - loss: 12.8822 - f1: 0.0550 - val_loss: 11.8393 - val_f1: 0.0450\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 11.0475 - f1: 0.0546 - val_loss: 10.3515 - val_f1: 0.0449\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 9.6981 - f1: 0.0687 - val_loss: 9.1163 - val_f1: 0.0496\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 8.5669 - f1: 0.0685 - val_loss: 8.0470 - val_f1: 0.0657\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 249us/sample - loss: 7.5958 - f1: 0.0919 - val_loss: 7.2582 - val_f1: 0.0705\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 6.8693 - f1: 0.1017 - val_loss: 6.5850 - val_f1: 0.1061\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 250us/sample - loss: 6.2517 - f1: 0.1380 - val_loss: 5.9964 - val_f1: 0.1149\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 5.7548 - f1: 0.1279 - val_loss: 5.5298 - val_f1: 0.1145\n",
      "Running through fold 2\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 923us/sample - loss: 36.4423 - f1: 0.0443 - val_loss: 28.6957 - val_f1: 0.0607\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 24.5328 - f1: 0.1004 - val_loss: 19.9853 - val_f1: 0.1166\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 16.6770 - f1: 0.0534 - val_loss: 13.9701 - val_f1: 0.0196\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 12.8032 - f1: 0.0157 - val_loss: 11.6786 - val_f1: 0.0452\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 10.9711 - f1: 0.0639 - val_loss: 10.2253 - val_f1: 0.0404\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 9.6258 - f1: 0.0631 - val_loss: 9.0514 - val_f1: 0.0544\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 8.5783 - f1: 0.0620 - val_loss: 7.9711 - val_f1: 0.0697\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 7.5519 - f1: 0.0865 - val_loss: 7.1486 - val_f1: 0.1062\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 6.8327 - f1: 0.1439 - val_loss: 6.5195 - val_f1: 0.1540\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 6.2634 - f1: 0.1661 - val_loss: 6.0183 - val_f1: 0.1369\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 5.7715 - f1: 0.1680 - val_loss: 5.5004 - val_f1: 0.1943\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 5.3136 - f1: 0.1952 - val_loss: 5.1388 - val_f1: 0.1888\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 5.0416 - f1: 0.2115 - val_loss: 4.8215 - val_f1: 0.2002\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 4.6207 - f1: 0.2347 - val_loss: 4.5002 - val_f1: 0.2371\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 4.3515 - f1: 0.2466 - val_loss: 4.2171 - val_f1: 0.2474\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 4.1188 - f1: 0.2459 - val_loss: 4.0221 - val_f1: 0.2416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 3.9001 - f1: 0.2517 - val_loss: 3.7621 - val_f1: 0.2577\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 3.6283 - f1: 0.2616 - val_loss: 3.4823 - val_f1: 0.2766\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 3.4017 - f1: 0.2879 - val_loss: 3.3134 - val_f1: 0.2802\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 3.2181 - f1: 0.3016 - val_loss: 3.1002 - val_f1: 0.3031\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 2.9334 - f1: 0.3398 - val_loss: 2.9739 - val_f1: 0.3064\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 2.8102 - f1: 0.3580 - val_loss: 2.7005 - val_f1: 0.3500\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.5780 - f1: 0.3781 - val_loss: 2.5504 - val_f1: 0.3660\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.3694 - f1: 0.4401 - val_loss: 2.2358 - val_f1: 0.4436\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 2.2790 - f1: 0.4391 - val_loss: 2.3005 - val_f1: 0.3848\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 2.1505 - f1: 0.4557 - val_loss: 2.0493 - val_f1: 0.4546\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.9914 - f1: 0.5092 - val_loss: 1.8183 - val_f1: 0.5271\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.9063 - f1: 0.5300 - val_loss: 1.7576 - val_f1: 0.5250\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.7585 - f1: 0.5664 - val_loss: 1.6022 - val_f1: 0.6007\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.6773 - f1: 0.5987 - val_loss: 1.6509 - val_f1: 0.5994\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.6714 - f1: 0.6093 - val_loss: 1.5495 - val_f1: 0.6200\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.6395 - f1: 0.6131 - val_loss: 1.6168 - val_f1: 0.5738\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 1.5471 - f1: 0.6356 - val_loss: 1.4250 - val_f1: 0.6289\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 1.5101 - f1: 0.6513 - val_loss: 1.4260 - val_f1: 0.6382\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.4406 - f1: 0.6624 - val_loss: 1.3993 - val_f1: 0.6524\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 249us/sample - loss: 1.4094 - f1: 0.6617 - val_loss: 1.3165 - val_f1: 0.6555\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.3892 - f1: 0.6612 - val_loss: 1.3168 - val_f1: 0.6661\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.3600 - f1: 0.6832 - val_loss: 1.2363 - val_f1: 0.6871\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.2987 - f1: 0.6911 - val_loss: 1.1677 - val_f1: 0.7243\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.2650 - f1: 0.6988 - val_loss: 1.1488 - val_f1: 0.7125\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.2139 - f1: 0.7243 - val_loss: 1.1202 - val_f1: 0.7581\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.2769 - f1: 0.7352 - val_loss: 1.2172 - val_f1: 0.7439\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.2944 - f1: 0.7198 - val_loss: 1.1610 - val_f1: 0.7623\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.2321 - f1: 0.7436 - val_loss: 1.1745 - val_f1: 0.7518\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.2258 - f1: 0.7353 - val_loss: 1.1619 - val_f1: 0.7248\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.1028 - f1: 0.7707 - val_loss: 1.0529 - val_f1: 0.7780\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.0526 - f1: 0.7770 - val_loss: 1.0367 - val_f1: 0.7804\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 1.0879 - f1: 0.7866 - val_loss: 0.9520 - val_f1: 0.8065\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 1.0992 - f1: 0.7769 - val_loss: 0.9733 - val_f1: 0.7865\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.1001 - f1: 0.7726 - val_loss: 0.9958 - val_f1: 0.8038\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.1497 - f1: 0.7480 - val_loss: 0.9914 - val_f1: 0.7706\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.1333 - f1: 0.7472 - val_loss: 0.9315 - val_f1: 0.8058\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.0959 - f1: 0.7606 - val_loss: 1.2020 - val_f1: 0.7109\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 1.2139 - f1: 0.7376 - val_loss: 1.0332 - val_f1: 0.7595\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.0811 - f1: 0.7776 - val_loss: 0.9057 - val_f1: 0.8108\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 0.9689 - f1: 0.8115 - val_loss: 0.8765 - val_f1: 0.8231\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 1.0209 - f1: 0.7956 - val_loss: 0.8785 - val_f1: 0.8224\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 0.9578 - f1: 0.8048 - val_loss: 0.8830 - val_f1: 0.7999\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.1086 - f1: 0.7333 - val_loss: 0.9183 - val_f1: 0.7836\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 0.9612 - f1: 0.7840 - val_loss: 0.8690 - val_f1: 0.8011\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 0.9740 - f1: 0.7841 - val_loss: 0.8826 - val_f1: 0.8224\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 0.9372 - f1: 0.8141 - val_loss: 0.8497 - val_f1: 0.8225\n",
      "Epoch 63/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 0.9011 - f1: 0.8187 - val_loss: 0.8336 - val_f1: 0.8217\n",
      "Epoch 64/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 0.8978 - f1: 0.8037 - val_loss: 0.8283 - val_f1: 0.8164\n",
      "Epoch 65/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 0.9056 - f1: 0.8133 - val_loss: 0.8753 - val_f1: 0.7919\n",
      "Epoch 66/2000\n",
      "1000/1000 [==============================] - 0s 259us/sample - loss: 0.8763 - f1: 0.8210 - val_loss: 0.8662 - val_f1: 0.8012\n",
      "Running through fold 3\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 975us/sample - loss: 36.5223 - f1: 0.0956 - val_loss: 29.9635 - val_f1: 0.0920\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 25.8389 - f1: 0.1439 - val_loss: 21.7968 - val_f1: 0.1488\n",
      "Epoch 3/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 18.6243 - f1: 0.1234 - val_loss: 15.2193 - val_f1: 0.1010\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 13.4393 - f1: 0.0613 - val_loss: 12.0609 - val_f1: 0.0443\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 11.3262 - f1: 0.0491 - val_loss: 10.5526 - val_f1: 0.0443\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 10.0122 - f1: 0.0420 - val_loss: 9.4443 - val_f1: 0.0643\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 8.9427 - f1: 0.0649 - val_loss: 8.4405 - val_f1: 0.0784\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 8.0413 - f1: 0.1060 - val_loss: 7.7175 - val_f1: 0.1006\n",
      "Epoch 9/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 255us/sample - loss: 7.3283 - f1: 0.0966 - val_loss: 6.9698 - val_f1: 0.1259\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 6.6752 - f1: 0.1284 - val_loss: 6.3564 - val_f1: 0.1309\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 6.1333 - f1: 0.1286 - val_loss: 5.9290 - val_f1: 0.1609\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 5.6514 - f1: 0.1537 - val_loss: 5.4669 - val_f1: 0.1740\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 5.2614 - f1: 0.1661 - val_loss: 5.2121 - val_f1: 0.1885\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 4.9423 - f1: 0.1832 - val_loss: 4.7785 - val_f1: 0.2157\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 4.6286 - f1: 0.1972 - val_loss: 4.5359 - val_f1: 0.2039\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 4.3790 - f1: 0.1840 - val_loss: 4.2437 - val_f1: 0.2318\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 4.1323 - f1: 0.2121 - val_loss: 4.0297 - val_f1: 0.2205\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 259us/sample - loss: 3.8094 - f1: 0.2492 - val_loss: 3.7321 - val_f1: 0.2799\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 3.5969 - f1: 0.2694 - val_loss: 3.5517 - val_f1: 0.2954\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 3.4054 - f1: 0.2825 - val_loss: 3.3669 - val_f1: 0.2979\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 3.2836 - f1: 0.2857 - val_loss: 3.2221 - val_f1: 0.2891\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 3.0747 - f1: 0.3277 - val_loss: 2.9687 - val_f1: 0.3628\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.8765 - f1: 0.3812 - val_loss: 2.7980 - val_f1: 0.4117\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.7279 - f1: 0.4067 - val_loss: 2.7725 - val_f1: 0.3855\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 2.6347 - f1: 0.4507 - val_loss: 2.6029 - val_f1: 0.4440\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 2.5468 - f1: 0.4650 - val_loss: 2.5816 - val_f1: 0.4360\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 2.4272 - f1: 0.5231 - val_loss: 2.4432 - val_f1: 0.4825\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 2.4773 - f1: 0.4592 - val_loss: 2.3455 - val_f1: 0.5173\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 2.3595 - f1: 0.4904 - val_loss: 2.3256 - val_f1: 0.5041\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.2308 - f1: 0.5363 - val_loss: 2.1772 - val_f1: 0.5328\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.1327 - f1: 0.5779 - val_loss: 2.1102 - val_f1: 0.5789\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 2.0958 - f1: 0.5911 - val_loss: 2.2205 - val_f1: 0.5390\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 2.0728 - f1: 0.6013 - val_loss: 2.0816 - val_f1: 0.5740\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 2.0919 - f1: 0.5707 - val_loss: 1.9692 - val_f1: 0.6014\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.9876 - f1: 0.6003 - val_loss: 1.9138 - val_f1: 0.6195\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.8906 - f1: 0.6632 - val_loss: 1.9551 - val_f1: 0.6262\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.8958 - f1: 0.6277 - val_loss: 1.8384 - val_f1: 0.6663\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 1.9199 - f1: 0.6326 - val_loss: 1.8722 - val_f1: 0.6233\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.8658 - f1: 0.6293 - val_loss: 1.7978 - val_f1: 0.6824\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.8744 - f1: 0.6524 - val_loss: 1.7701 - val_f1: 0.6710\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.7652 - f1: 0.6820 - val_loss: 1.8351 - val_f1: 0.6664\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.8123 - f1: 0.6837 - val_loss: 1.8112 - val_f1: 0.7042\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 259us/sample - loss: 1.7989 - f1: 0.6918 - val_loss: 1.9005 - val_f1: 0.6498\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.7750 - f1: 0.6879 - val_loss: 1.8187 - val_f1: 0.6737\n",
      "Epoch 45/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 1.8189 - f1: 0.6916 - val_loss: 1.7449 - val_f1: 0.7208\n",
      "Epoch 46/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.6826 - f1: 0.7167 - val_loss: 1.6533 - val_f1: 0.7351\n",
      "Epoch 47/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.6704 - f1: 0.7359 - val_loss: 1.7902 - val_f1: 0.6623\n",
      "Epoch 48/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.6366 - f1: 0.7245 - val_loss: 1.6825 - val_f1: 0.7162\n",
      "Epoch 49/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.6421 - f1: 0.7183 - val_loss: 1.6193 - val_f1: 0.7270\n",
      "Epoch 50/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.6081 - f1: 0.7278 - val_loss: 1.6345 - val_f1: 0.7203\n",
      "Epoch 51/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.6946 - f1: 0.7126 - val_loss: 1.6883 - val_f1: 0.7316\n",
      "Epoch 52/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 1.6416 - f1: 0.7448 - val_loss: 1.6863 - val_f1: 0.7248\n",
      "Epoch 53/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.6340 - f1: 0.7227 - val_loss: 1.5983 - val_f1: 0.7303\n",
      "Epoch 54/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.6256 - f1: 0.7202 - val_loss: 1.5752 - val_f1: 0.7520\n",
      "Epoch 55/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 1.5806 - f1: 0.7286 - val_loss: 1.6554 - val_f1: 0.6872\n",
      "Epoch 56/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.5192 - f1: 0.7593 - val_loss: 1.5388 - val_f1: 0.7446\n",
      "Epoch 57/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.5362 - f1: 0.7609 - val_loss: 1.5338 - val_f1: 0.7399\n",
      "Epoch 58/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.5227 - f1: 0.7476 - val_loss: 1.5398 - val_f1: 0.7276\n",
      "Epoch 59/2000\n",
      "1000/1000 [==============================] - 0s 252us/sample - loss: 1.5172 - f1: 0.7493 - val_loss: 1.5848 - val_f1: 0.7087\n",
      "Epoch 60/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 1.5465 - f1: 0.7389 - val_loss: 1.8508 - val_f1: 0.6523\n",
      "Epoch 61/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.5666 - f1: 0.7276 - val_loss: 1.5587 - val_f1: 0.7246\n",
      "Epoch 62/2000\n",
      "1000/1000 [==============================] - 0s 261us/sample - loss: 1.4832 - f1: 0.7642 - val_loss: 1.5054 - val_f1: 0.7461\n",
      "Epoch 63/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.5111 - f1: 0.7581 - val_loss: 1.5320 - val_f1: 0.7293\n",
      "Epoch 64/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 1.5031 - f1: 0.7541 - val_loss: 1.6077 - val_f1: 0.7085\n",
      "Running through fold 4\n",
      "Train on 1000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 36.1786 - f1: 0.0964 - val_loss: 29.4254 - val_f1: 0.1840\n",
      "Epoch 2/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 26.2313 - f1: 0.1866 - val_loss: 23.1943 - val_f1: 0.1521\n",
      "Epoch 3/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 261us/sample - loss: 21.1669 - f1: 0.2058 - val_loss: 19.1478 - val_f1: 0.2401\n",
      "Epoch 4/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 17.7191 - f1: 0.2481 - val_loss: 16.2436 - val_f1: 0.2397\n",
      "Epoch 5/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 15.2412 - f1: 0.2328 - val_loss: 13.9840 - val_f1: 0.1782\n",
      "Epoch 6/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 12.4649 - f1: 0.1694 - val_loss: 10.9419 - val_f1: 0.1787\n",
      "Epoch 7/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 10.0774 - f1: 0.1538 - val_loss: 9.5820 - val_f1: 0.1374\n",
      "Epoch 8/2000\n",
      "1000/1000 [==============================] - 0s 251us/sample - loss: 8.9244 - f1: 0.1657 - val_loss: 8.5046 - val_f1: 0.1545\n",
      "Epoch 9/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 7.9444 - f1: 0.1630 - val_loss: 7.6122 - val_f1: 0.1988\n",
      "Epoch 10/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 7.1606 - f1: 0.2295 - val_loss: 6.9211 - val_f1: 0.1853\n",
      "Epoch 11/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 6.4911 - f1: 0.2652 - val_loss: 6.3101 - val_f1: 0.2423\n",
      "Epoch 12/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 5.7874 - f1: 0.3036 - val_loss: 5.7335 - val_f1: 0.2648\n",
      "Epoch 13/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 5.2744 - f1: 0.3537 - val_loss: 5.2833 - val_f1: 0.3148\n",
      "Epoch 14/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 4.8175 - f1: 0.3910 - val_loss: 4.7588 - val_f1: 0.3902\n",
      "Epoch 15/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 4.4400 - f1: 0.4278 - val_loss: 4.3885 - val_f1: 0.4331\n",
      "Epoch 16/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 4.0640 - f1: 0.4627 - val_loss: 4.2173 - val_f1: 0.4519\n",
      "Epoch 17/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 3.8454 - f1: 0.4772 - val_loss: 3.8809 - val_f1: 0.4851\n",
      "Epoch 18/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 3.5452 - f1: 0.5307 - val_loss: 3.5394 - val_f1: 0.5360\n",
      "Epoch 19/2000\n",
      "1000/1000 [==============================] - 0s 260us/sample - loss: 3.2734 - f1: 0.5845 - val_loss: 3.2237 - val_f1: 0.5874\n",
      "Epoch 20/2000\n",
      "1000/1000 [==============================] - 0s 261us/sample - loss: 3.0499 - f1: 0.6241 - val_loss: 3.1081 - val_f1: 0.6055\n",
      "Epoch 21/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 2.9392 - f1: 0.6342 - val_loss: 2.9365 - val_f1: 0.6028\n",
      "Epoch 22/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 2.7521 - f1: 0.6640 - val_loss: 2.7170 - val_f1: 0.6695\n",
      "Epoch 23/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 2.6876 - f1: 0.6635 - val_loss: 2.6414 - val_f1: 0.6673\n",
      "Epoch 24/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 2.5285 - f1: 0.6675 - val_loss: 2.5492 - val_f1: 0.6967\n",
      "Epoch 25/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.4328 - f1: 0.7053 - val_loss: 2.4663 - val_f1: 0.6944\n",
      "Epoch 26/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.3374 - f1: 0.7164 - val_loss: 2.3722 - val_f1: 0.7041\n",
      "Epoch 27/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 2.2012 - f1: 0.7516 - val_loss: 2.3196 - val_f1: 0.7292\n",
      "Epoch 28/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 2.2373 - f1: 0.7247 - val_loss: 2.2699 - val_f1: 0.6932\n",
      "Epoch 29/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.1024 - f1: 0.7399 - val_loss: 2.1862 - val_f1: 0.7101\n",
      "Epoch 30/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 2.1338 - f1: 0.7380 - val_loss: 2.1148 - val_f1: 0.7334\n",
      "Epoch 31/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.9968 - f1: 0.7643 - val_loss: 2.0509 - val_f1: 0.7417\n",
      "Epoch 32/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 2.0079 - f1: 0.7433 - val_loss: 1.9935 - val_f1: 0.7419\n",
      "Epoch 33/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.9461 - f1: 0.7610 - val_loss: 2.0023 - val_f1: 0.7264\n",
      "Epoch 34/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.9401 - f1: 0.7418 - val_loss: 1.8880 - val_f1: 0.7956\n",
      "Epoch 35/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.8196 - f1: 0.7725 - val_loss: 1.9722 - val_f1: 0.7305\n",
      "Epoch 36/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.7707 - f1: 0.7898 - val_loss: 1.7855 - val_f1: 0.7865\n",
      "Epoch 37/2000\n",
      "1000/1000 [==============================] - 0s 257us/sample - loss: 1.6745 - f1: 0.8121 - val_loss: 1.7618 - val_f1: 0.7872\n",
      "Epoch 38/2000\n",
      "1000/1000 [==============================] - 0s 258us/sample - loss: 1.6886 - f1: 0.7922 - val_loss: 1.7995 - val_f1: 0.7469\n",
      "Epoch 39/2000\n",
      "1000/1000 [==============================] - 0s 256us/sample - loss: 1.6631 - f1: 0.7912 - val_loss: 1.7508 - val_f1: 0.7731\n",
      "Epoch 40/2000\n",
      "1000/1000 [==============================] - 0s 253us/sample - loss: 1.6584 - f1: 0.7913 - val_loss: 1.6910 - val_f1: 0.7827\n",
      "Epoch 41/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.6462 - f1: 0.7691 - val_loss: 1.7211 - val_f1: 0.7522\n",
      "Epoch 42/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.7603 - f1: 0.7396 - val_loss: 1.7346 - val_f1: 0.7725\n",
      "Epoch 43/2000\n",
      "1000/1000 [==============================] - 0s 255us/sample - loss: 1.6900 - f1: 0.7676 - val_loss: 1.6881 - val_f1: 0.7885\n",
      "Epoch 44/2000\n",
      "1000/1000 [==============================] - 0s 254us/sample - loss: 1.6282 - f1: 0.7821 - val_loss: 1.7389 - val_f1: 0.7299\n",
      "\n",
      "\n",
      "Running through training size 5000\n",
      "Running through fold 0\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 1s 282us/sample - loss: 20.0435 - f1: 0.0513 - val_loss: 10.2470 - val_f1: 0.0444\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 7.8700 - f1: 0.0853 - val_loss: 6.2122 - val_f1: 0.1323\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 5.1697 - f1: 0.1442 - val_loss: 4.3416 - val_f1: 0.2193\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 3.8188 - f1: 0.1945 - val_loss: 3.3144 - val_f1: 0.2301\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 2.8998 - f1: 0.2712 - val_loss: 2.6681 - val_f1: 0.3029\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 2.2487 - f1: 0.3874 - val_loss: 2.0218 - val_f1: 0.4431\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.8183 - f1: 0.5137 - val_loss: 1.8012 - val_f1: 0.5288\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.5078 - f1: 0.6191 - val_loss: 1.4111 - val_f1: 0.6356\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.4006 - f1: 0.6542 - val_loss: 1.2647 - val_f1: 0.6866\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 1.2940 - f1: 0.6920 - val_loss: 1.0903 - val_f1: 0.7432\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.1729 - f1: 0.7221 - val_loss: 1.0186 - val_f1: 0.7580\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.0818 - f1: 0.7392 - val_loss: 0.9385 - val_f1: 0.7719\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.9941 - f1: 0.7603 - val_loss: 0.8533 - val_f1: 0.8107\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.9265 - f1: 0.7874 - val_loss: 0.7697 - val_f1: 0.8324\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.9123 - f1: 0.7828 - val_loss: 0.9505 - val_f1: 0.7552\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.8910 - f1: 0.7904 - val_loss: 0.7327 - val_f1: 0.8452\n",
      "Epoch 17/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 132us/sample - loss: 0.8290 - f1: 0.8116 - val_loss: 0.7515 - val_f1: 0.8492\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7914 - f1: 0.8203 - val_loss: 0.6860 - val_f1: 0.8600\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.8376 - f1: 0.7988 - val_loss: 0.8418 - val_f1: 0.7819\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7928 - f1: 0.8208 - val_loss: 0.7877 - val_f1: 0.8060\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.7563 - f1: 0.8211 - val_loss: 0.7618 - val_f1: 0.8064\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7743 - f1: 0.8196 - val_loss: 0.7075 - val_f1: 0.8388\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.7561 - f1: 0.8185 - val_loss: 0.6437 - val_f1: 0.8674\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.7576 - f1: 0.8207 - val_loss: 0.6362 - val_f1: 0.8673\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7363 - f1: 0.8307 - val_loss: 0.7031 - val_f1: 0.8351\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.7018 - f1: 0.8441 - val_loss: 0.6157 - val_f1: 0.8751\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7056 - f1: 0.8382 - val_loss: 0.6314 - val_f1: 0.8669\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.6956 - f1: 0.8368 - val_loss: 0.6439 - val_f1: 0.8521\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.6767 - f1: 0.8491 - val_loss: 0.6062 - val_f1: 0.8591\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.6664 - f1: 0.8504 - val_loss: 0.5425 - val_f1: 0.8945\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7010 - f1: 0.8381 - val_loss: 0.6512 - val_f1: 0.8462\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.6796 - f1: 0.8418 - val_loss: 0.5539 - val_f1: 0.8974\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 141us/sample - loss: 0.6478 - f1: 0.8591 - val_loss: 0.5580 - val_f1: 0.8907\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.6653 - f1: 0.8456 - val_loss: 0.5830 - val_f1: 0.8780\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.6231 - f1: 0.8612 - val_loss: 0.6024 - val_f1: 0.8711\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.6688 - f1: 0.8460 - val_loss: 0.7563 - val_f1: 0.8066\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 0.6498 - f1: 0.8556 - val_loss: 0.6353 - val_f1: 0.8466\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 0.6486 - f1: 0.8534 - val_loss: 0.8171 - val_f1: 0.7755\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 0.6468 - f1: 0.8560 - val_loss: 0.5675 - val_f1: 0.8867\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.6318 - f1: 0.8626 - val_loss: 0.6337 - val_f1: 0.8625\n",
      "Running through fold 1\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 1s 286us/sample - loss: 21.6133 - f1: 0.0998 - val_loss: 10.4486 - val_f1: 0.0934\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 7.9119 - f1: 0.1251 - val_loss: 5.8466 - val_f1: 0.1847\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 4.8915 - f1: 0.2071 - val_loss: 3.9208 - val_f1: 0.2643\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 3.3462 - f1: 0.3108 - val_loss: 2.6617 - val_f1: 0.3749\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 2.4239 - f1: 0.4588 - val_loss: 2.0155 - val_f1: 0.5262\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.8958 - f1: 0.5612 - val_loss: 1.5424 - val_f1: 0.6340\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.5705 - f1: 0.6358 - val_loss: 1.3272 - val_f1: 0.6872\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.3403 - f1: 0.6971 - val_loss: 1.1981 - val_f1: 0.7094\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.2750 - f1: 0.7082 - val_loss: 1.0952 - val_f1: 0.7658\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.1571 - f1: 0.7487 - val_loss: 0.9873 - val_f1: 0.7796\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.0552 - f1: 0.7759 - val_loss: 0.9480 - val_f1: 0.7835\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.9487 - f1: 0.7920 - val_loss: 0.7882 - val_f1: 0.8332\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.9084 - f1: 0.7945 - val_loss: 0.8473 - val_f1: 0.8015\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.8740 - f1: 0.8095 - val_loss: 0.7409 - val_f1: 0.8456\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.8368 - f1: 0.8187 - val_loss: 0.7336 - val_f1: 0.8332\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.8245 - f1: 0.8215 - val_loss: 0.6913 - val_f1: 0.8578\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 132us/sample - loss: 0.7670 - f1: 0.8337 - val_loss: 0.6686 - val_f1: 0.8658\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.8019 - f1: 0.8291 - val_loss: 0.7896 - val_f1: 0.8241\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.7469 - f1: 0.8403 - val_loss: 0.7426 - val_f1: 0.8301\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7213 - f1: 0.8435 - val_loss: 0.6411 - val_f1: 0.8701\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.7353 - f1: 0.8437 - val_loss: 0.6229 - val_f1: 0.8780\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.6975 - f1: 0.8561 - val_loss: 0.6708 - val_f1: 0.8520\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.6766 - f1: 0.8556 - val_loss: 0.7733 - val_f1: 0.8003\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.6983 - f1: 0.8483 - val_loss: 1.1800 - val_f1: 0.7132\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7739 - f1: 0.8257 - val_loss: 0.5997 - val_f1: 0.8905\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.6364 - f1: 0.8815 - val_loss: 0.5704 - val_f1: 0.8907\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.6182 - f1: 0.8818 - val_loss: 0.5166 - val_f1: 0.9131\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.6326 - f1: 0.8706 - val_loss: 0.6413 - val_f1: 0.8550\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.6430 - f1: 0.8687 - val_loss: 0.5127 - val_f1: 0.9159\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.5916 - f1: 0.8877 - val_loss: 0.5270 - val_f1: 0.9019\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 0.5822 - f1: 0.8853 - val_loss: 0.5008 - val_f1: 0.9073\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.5971 - f1: 0.8773 - val_loss: 0.5078 - val_f1: 0.9131\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.6100 - f1: 0.8722 - val_loss: 0.5088 - val_f1: 0.9131\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.6018 - f1: 0.8781 - val_loss: 0.5282 - val_f1: 0.8988\n",
      "Epoch 35/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.6056 - f1: 0.8734 - val_loss: 0.5954 - val_f1: 0.8573\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.5569 - f1: 0.8890 - val_loss: 0.5096 - val_f1: 0.9057\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.6137 - f1: 0.8685 - val_loss: 0.6194 - val_f1: 0.8750\n",
      "Running through fold 2\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 302us/sample - loss: 22.6617 - f1: 0.0862 - val_loss: 10.0876 - val_f1: 0.0336\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 7.7235 - f1: 0.0753 - val_loss: 5.9985 - val_f1: 0.1217\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 4.9905 - f1: 0.1435 - val_loss: 4.1756 - val_f1: 0.1692\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 3.6419 - f1: 0.1849 - val_loss: 3.2119 - val_f1: 0.1906\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 2.9413 - f1: 0.2006 - val_loss: 2.6384 - val_f1: 0.2208\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 2.4799 - f1: 0.2490 - val_loss: 2.1549 - val_f1: 0.3245\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 2.0427 - f1: 0.3775 - val_loss: 1.7822 - val_f1: 0.4342\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.7719 - f1: 0.4559 - val_loss: 1.6900 - val_f1: 0.4512\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.6545 - f1: 0.5073 - val_loss: 1.5992 - val_f1: 0.4976\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.5086 - f1: 0.5463 - val_loss: 1.2420 - val_f1: 0.6027\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 1.3158 - f1: 0.6215 - val_loss: 1.0864 - val_f1: 0.6929\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 133us/sample - loss: 1.1745 - f1: 0.6722 - val_loss: 1.0668 - val_f1: 0.6801\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 1.0972 - f1: 0.6912 - val_loss: 0.9884 - val_f1: 0.7331\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 1.1167 - f1: 0.6854 - val_loss: 0.9726 - val_f1: 0.7166\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 1.0044 - f1: 0.7208 - val_loss: 0.9261 - val_f1: 0.7322\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.9987 - f1: 0.7286 - val_loss: 0.8449 - val_f1: 0.7645\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.9731 - f1: 0.7332 - val_loss: 0.8110 - val_f1: 0.7704\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.9442 - f1: 0.7435 - val_loss: 0.8050 - val_f1: 0.7734\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.9233 - f1: 0.7488 - val_loss: 0.8595 - val_f1: 0.7528\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.9583 - f1: 0.7369 - val_loss: 0.8689 - val_f1: 0.7387\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.9019 - f1: 0.7576 - val_loss: 0.7293 - val_f1: 0.8160\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.8347 - f1: 0.7877 - val_loss: 0.7616 - val_f1: 0.7862\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.8505 - f1: 0.7784 - val_loss: 0.7180 - val_f1: 0.8274\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.8622 - f1: 0.7799 - val_loss: 0.7483 - val_f1: 0.7907\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.8481 - f1: 0.7920 - val_loss: 0.6863 - val_f1: 0.8251\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.7974 - f1: 0.8048 - val_loss: 0.6786 - val_f1: 0.8349\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.8246 - f1: 0.8012 - val_loss: 0.7015 - val_f1: 0.8183\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.8451 - f1: 0.7773 - val_loss: 0.6389 - val_f1: 0.8449\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.7934 - f1: 0.8151 - val_loss: 0.7809 - val_f1: 0.7844\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 137us/sample - loss: 0.7710 - f1: 0.8143 - val_loss: 0.6062 - val_f1: 0.8699\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.7318 - f1: 0.8320 - val_loss: 0.6045 - val_f1: 0.8667\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.7705 - f1: 0.8113 - val_loss: 0.6635 - val_f1: 0.8323\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 137us/sample - loss: 0.7890 - f1: 0.7975 - val_loss: 0.6153 - val_f1: 0.8682\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.7753 - f1: 0.8140 - val_loss: 0.6129 - val_f1: 0.8718\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.7590 - f1: 0.8194 - val_loss: 0.7494 - val_f1: 0.8050\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.7775 - f1: 0.8174 - val_loss: 0.5977 - val_f1: 0.8660\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.7180 - f1: 0.8340 - val_loss: 0.8208 - val_f1: 0.7475\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.7103 - f1: 0.8396 - val_loss: 0.5997 - val_f1: 0.8672\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.6931 - f1: 0.8448 - val_loss: 0.6739 - val_f1: 0.8335\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 136us/sample - loss: 0.7106 - f1: 0.8399 - val_loss: 0.5675 - val_f1: 0.8789\n",
      "Running through fold 3\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 358us/sample - loss: 21.7165 - f1: 0.0687 - val_loss: 11.0465 - val_f1: 0.0880\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 168us/sample - loss: 8.2655 - f1: 0.1442 - val_loss: 6.2383 - val_f1: 0.2039\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 167us/sample - loss: 5.1925 - f1: 0.2100 - val_loss: 4.2858 - val_f1: 0.1883\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 3.7181 - f1: 0.2539 - val_loss: 3.1357 - val_f1: 0.3046\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 128us/sample - loss: 2.8858 - f1: 0.3387 - val_loss: 2.4618 - val_f1: 0.4089\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 128us/sample - loss: 2.3496 - f1: 0.4041 - val_loss: 2.0241 - val_f1: 0.4608\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 148us/sample - loss: 1.9754 - f1: 0.4934 - val_loss: 1.8729 - val_f1: 0.5267\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 1.6995 - f1: 0.5605 - val_loss: 1.4730 - val_f1: 0.5880\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 1.5781 - f1: 0.5754 - val_loss: 1.4590 - val_f1: 0.6175\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 1.4190 - f1: 0.6295 - val_loss: 1.2925 - val_f1: 0.6207\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 1.2968 - f1: 0.6603 - val_loss: 1.1464 - val_f1: 0.6730\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 149us/sample - loss: 1.1621 - f1: 0.6969 - val_loss: 1.0114 - val_f1: 0.7231\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 146us/sample - loss: 1.0522 - f1: 0.7359 - val_loss: 0.9065 - val_f1: 0.7781\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 0.9951 - f1: 0.7678 - val_loss: 0.8071 - val_f1: 0.8058\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 128us/sample - loss: 0.9651 - f1: 0.7713 - val_loss: 1.0398 - val_f1: 0.6745\n",
      "Epoch 16/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 160us/sample - loss: 0.8953 - f1: 0.7914 - val_loss: 0.7983 - val_f1: 0.8129\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 168us/sample - loss: 0.8633 - f1: 0.8045 - val_loss: 0.7310 - val_f1: 0.8522\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.8581 - f1: 0.8079 - val_loss: 0.8681 - val_f1: 0.7766\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.8254 - f1: 0.8161 - val_loss: 0.7102 - val_f1: 0.8599\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 147us/sample - loss: 0.7857 - f1: 0.8318 - val_loss: 0.6872 - val_f1: 0.8443\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 147us/sample - loss: 0.7985 - f1: 0.8214 - val_loss: 0.6504 - val_f1: 0.8627\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 0.7735 - f1: 0.8328 - val_loss: 0.7018 - val_f1: 0.8358\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 0.7374 - f1: 0.8406 - val_loss: 0.6052 - val_f1: 0.8777\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 131us/sample - loss: 0.7178 - f1: 0.8446 - val_loss: 0.7824 - val_f1: 0.7939\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.7235 - f1: 0.8390 - val_loss: 0.6699 - val_f1: 0.8485\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.7319 - f1: 0.8383 - val_loss: 0.6315 - val_f1: 0.8757\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.7419 - f1: 0.8390 - val_loss: 0.6211 - val_f1: 0.8667\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.6602 - f1: 0.8600 - val_loss: 0.5667 - val_f1: 0.8979\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.7034 - f1: 0.8500 - val_loss: 0.6029 - val_f1: 0.8716\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.6722 - f1: 0.8580 - val_loss: 0.5322 - val_f1: 0.9115\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.7171 - f1: 0.8418 - val_loss: 0.5676 - val_f1: 0.8865\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.6188 - f1: 0.8747 - val_loss: 0.5872 - val_f1: 0.8624\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 144us/sample - loss: 0.6675 - f1: 0.8601 - val_loss: 0.5687 - val_f1: 0.8818\n",
      "Epoch 34/2000\n",
      "5000/5000 [==============================] - 1s 157us/sample - loss: 0.6847 - f1: 0.8540 - val_loss: 0.5203 - val_f1: 0.9093\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 0.6879 - f1: 0.8473 - val_loss: 0.5679 - val_f1: 0.8871\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 127us/sample - loss: 0.6768 - f1: 0.8532 - val_loss: 0.5071 - val_f1: 0.9098\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 127us/sample - loss: 0.6719 - f1: 0.8489 - val_loss: 0.4984 - val_f1: 0.9180\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 127us/sample - loss: 0.6329 - f1: 0.8655 - val_loss: 0.5817 - val_f1: 0.8788\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.6020 - f1: 0.8739 - val_loss: 0.5147 - val_f1: 0.9012\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.6211 - f1: 0.8664 - val_loss: 0.5585 - val_f1: 0.8816\n",
      "Running through fold 4\n",
      "Train on 5000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "5000/5000 [==============================] - 2s 306us/sample - loss: 24.4037 - f1: 0.1702 - val_loss: 14.9236 - val_f1: 0.1875\n",
      "Epoch 2/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 9.1511 - f1: 0.0534 - val_loss: 6.9196 - val_f1: 0.0563\n",
      "Epoch 3/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 5.9192 - f1: 0.0720 - val_loss: 5.0399 - val_f1: 0.1150\n",
      "Epoch 4/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 4.3280 - f1: 0.1363 - val_loss: 3.6713 - val_f1: 0.1626\n",
      "Epoch 5/2000\n",
      "5000/5000 [==============================] - 1s 167us/sample - loss: 3.3927 - f1: 0.1922 - val_loss: 3.1039 - val_f1: 0.2146\n",
      "Epoch 6/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 2.8778 - f1: 0.2193 - val_loss: 2.6006 - val_f1: 0.2495\n",
      "Epoch 7/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 2.4476 - f1: 0.2771 - val_loss: 2.2246 - val_f1: 0.3105\n",
      "Epoch 8/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 2.0632 - f1: 0.3623 - val_loss: 1.7663 - val_f1: 0.4519\n",
      "Epoch 9/2000\n",
      "5000/5000 [==============================] - 1s 145us/sample - loss: 1.8086 - f1: 0.4650 - val_loss: 1.5420 - val_f1: 0.5474\n",
      "Epoch 10/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 1.5462 - f1: 0.5694 - val_loss: 1.3214 - val_f1: 0.6255\n",
      "Epoch 11/2000\n",
      "5000/5000 [==============================] - 1s 138us/sample - loss: 1.3272 - f1: 0.6420 - val_loss: 1.1017 - val_f1: 0.7037\n",
      "Epoch 12/2000\n",
      "5000/5000 [==============================] - 1s 128us/sample - loss: 1.1927 - f1: 0.6840 - val_loss: 1.0810 - val_f1: 0.6896\n",
      "Epoch 13/2000\n",
      "5000/5000 [==============================] - 1s 128us/sample - loss: 1.1146 - f1: 0.7055 - val_loss: 0.9356 - val_f1: 0.7583\n",
      "Epoch 14/2000\n",
      "5000/5000 [==============================] - 1s 127us/sample - loss: 1.0606 - f1: 0.7218 - val_loss: 0.8907 - val_f1: 0.7433\n",
      "Epoch 15/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 1.0269 - f1: 0.7394 - val_loss: 0.8272 - val_f1: 0.8027\n",
      "Epoch 16/2000\n",
      "5000/5000 [==============================] - 1s 155us/sample - loss: 0.9874 - f1: 0.7559 - val_loss: 0.8251 - val_f1: 0.7997\n",
      "Epoch 17/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.9369 - f1: 0.7680 - val_loss: 0.8394 - val_f1: 0.7654\n",
      "Epoch 18/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.9020 - f1: 0.7844 - val_loss: 0.7466 - val_f1: 0.8265\n",
      "Epoch 19/2000\n",
      "5000/5000 [==============================] - 1s 168us/sample - loss: 0.9188 - f1: 0.7682 - val_loss: 0.7499 - val_f1: 0.8261\n",
      "Epoch 20/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.8833 - f1: 0.7829 - val_loss: 0.7117 - val_f1: 0.8367\n",
      "Epoch 21/2000\n",
      "5000/5000 [==============================] - 1s 163us/sample - loss: 0.8709 - f1: 0.7860 - val_loss: 0.7630 - val_f1: 0.8087\n",
      "Epoch 22/2000\n",
      "5000/5000 [==============================] - 1s 130us/sample - loss: 0.8612 - f1: 0.7881 - val_loss: 0.6645 - val_f1: 0.8531\n",
      "Epoch 23/2000\n",
      "5000/5000 [==============================] - 1s 152us/sample - loss: 0.8615 - f1: 0.8007 - val_loss: 0.7097 - val_f1: 0.8166\n",
      "Epoch 24/2000\n",
      "5000/5000 [==============================] - 1s 131us/sample - loss: 0.8206 - f1: 0.7988 - val_loss: 0.8278 - val_f1: 0.7841\n",
      "Epoch 25/2000\n",
      "5000/5000 [==============================] - 1s 130us/sample - loss: 0.7849 - f1: 0.8177 - val_loss: 0.6346 - val_f1: 0.8587\n",
      "Epoch 26/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7658 - f1: 0.8205 - val_loss: 0.6682 - val_f1: 0.8364\n",
      "Epoch 27/2000\n",
      "5000/5000 [==============================] - 1s 135us/sample - loss: 0.7635 - f1: 0.8168 - val_loss: 0.6961 - val_f1: 0.8359\n",
      "Epoch 28/2000\n",
      "5000/5000 [==============================] - 1s 134us/sample - loss: 0.7318 - f1: 0.8270 - val_loss: 0.6257 - val_f1: 0.8511\n",
      "Epoch 29/2000\n",
      "5000/5000 [==============================] - 1s 158us/sample - loss: 0.7654 - f1: 0.8122 - val_loss: 0.6261 - val_f1: 0.8549\n",
      "Epoch 30/2000\n",
      "5000/5000 [==============================] - 1s 168us/sample - loss: 0.7237 - f1: 0.8295 - val_loss: 0.6747 - val_f1: 0.8386\n",
      "Epoch 31/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.7605 - f1: 0.8225 - val_loss: 0.7078 - val_f1: 0.8012\n",
      "Epoch 32/2000\n",
      "5000/5000 [==============================] - 1s 167us/sample - loss: 0.7383 - f1: 0.8227 - val_loss: 0.5839 - val_f1: 0.8728\n",
      "Epoch 33/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.7109 - f1: 0.8322 - val_loss: 0.6653 - val_f1: 0.8381\n",
      "Epoch 34/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.7902 - f1: 0.8028 - val_loss: 0.6394 - val_f1: 0.8469\n",
      "Epoch 35/2000\n",
      "5000/5000 [==============================] - 1s 166us/sample - loss: 0.6854 - f1: 0.8424 - val_loss: 0.5941 - val_f1: 0.8625\n",
      "Epoch 36/2000\n",
      "5000/5000 [==============================] - 1s 164us/sample - loss: 0.7166 - f1: 0.8237 - val_loss: 0.5848 - val_f1: 0.8596\n",
      "Epoch 37/2000\n",
      "5000/5000 [==============================] - 1s 165us/sample - loss: 0.6750 - f1: 0.8427 - val_loss: 0.5781 - val_f1: 0.8670\n",
      "Epoch 38/2000\n",
      "5000/5000 [==============================] - 1s 146us/sample - loss: 0.6961 - f1: 0.8313 - val_loss: 0.6678 - val_f1: 0.8266\n",
      "Epoch 39/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 0.7107 - f1: 0.8306 - val_loss: 0.6832 - val_f1: 0.8220\n",
      "Epoch 40/2000\n",
      "5000/5000 [==============================] - 1s 154us/sample - loss: 0.7280 - f1: 0.8210 - val_loss: 0.6471 - val_f1: 0.8397\n",
      "Epoch 41/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 0.7334 - f1: 0.8221 - val_loss: 0.6773 - val_f1: 0.8231\n",
      "Epoch 42/2000\n",
      "5000/5000 [==============================] - 1s 129us/sample - loss: 0.6771 - f1: 0.8459 - val_loss: 0.5586 - val_f1: 0.8777\n",
      "\n",
      "\n",
      "Running through training size 10000\n",
      "Running through fold 0\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 2s 205us/sample - loss: 14.7295 - f1: 0.1043 - val_loss: 6.0347 - val_f1: 0.1861\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 1s 113us/sample - loss: 4.2665 - f1: 0.2601 - val_loss: 2.9115 - val_f1: 0.3628\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 116us/sample - loss: 2.2419 - f1: 0.4694 - val_loss: 1.7168 - val_f1: 0.5480\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 147us/sample - loss: 1.4352 - f1: 0.6736 - val_loss: 1.0862 - val_f1: 0.7846\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 1.1860 - f1: 0.7492 - val_loss: 0.9949 - val_f1: 0.8053\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 1.0274 - f1: 0.7937 - val_loss: 0.7947 - val_f1: 0.8598\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.8971 - f1: 0.8170 - val_loss: 0.7890 - val_f1: 0.8429\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.8530 - f1: 0.8204 - val_loss: 0.7703 - val_f1: 0.8578\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.7890 - f1: 0.8394 - val_loss: 0.6382 - val_f1: 0.8940\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.7411 - f1: 0.8504 - val_loss: 0.7088 - val_f1: 0.8518\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.7035 - f1: 0.8580 - val_loss: 0.6113 - val_f1: 0.8974\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6748 - f1: 0.8657 - val_loss: 0.6019 - val_f1: 0.8852\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 123us/sample - loss: 0.6477 - f1: 0.8682 - val_loss: 0.5783 - val_f1: 0.8929\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 114us/sample - loss: 0.6186 - f1: 0.8781 - val_loss: 0.6583 - val_f1: 0.8619\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 113us/sample - loss: 0.6212 - f1: 0.8754 - val_loss: 0.5959 - val_f1: 0.8719\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 114us/sample - loss: 0.5990 - f1: 0.8795 - val_loss: 0.5770 - val_f1: 0.8850\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 141us/sample - loss: 0.6322 - f1: 0.8665 - val_loss: 0.7254 - val_f1: 0.8038\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.5626 - f1: 0.8916 - val_loss: 0.4509 - val_f1: 0.9387\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.5985 - f1: 0.8784 - val_loss: 0.4289 - val_f1: 0.9474\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.5558 - f1: 0.8918 - val_loss: 0.5210 - val_f1: 0.8890\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 150us/sample - loss: 0.5519 - f1: 0.8918 - val_loss: 0.5125 - val_f1: 0.9021\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.5772 - f1: 0.8825 - val_loss: 0.4327 - val_f1: 0.9354\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 150us/sample - loss: 0.5786 - f1: 0.8833 - val_loss: 0.4692 - val_f1: 0.9272\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.5227 - f1: 0.9049 - val_loss: 0.4816 - val_f1: 0.9132\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.5945 - f1: 0.8766 - val_loss: 0.5527 - val_f1: 0.8761\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.5506 - f1: 0.8923 - val_loss: 0.4362 - val_f1: 0.9338\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 135us/sample - loss: 0.5473 - f1: 0.8879 - val_loss: 0.4886 - val_f1: 0.9157\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 122us/sample - loss: 0.5575 - f1: 0.8863 - val_loss: 0.4284 - val_f1: 0.9357\n",
      "Running through fold 1\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 2s 241us/sample - loss: 15.6524 - f1: 0.0638 - val_loss: 6.6412 - val_f1: 0.0607\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 4.7941 - f1: 0.1414 - val_loss: 3.3916 - val_f1: 0.2314\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 1s 145us/sample - loss: 2.6381 - f1: 0.3181 - val_loss: 1.8233 - val_f1: 0.4791\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 118us/sample - loss: 1.6709 - f1: 0.5540 - val_loss: 1.3315 - val_f1: 0.6298\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 1.4046 - f1: 0.6365 - val_loss: 1.2045 - val_f1: 0.6657\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 114us/sample - loss: 1.2442 - f1: 0.6650 - val_loss: 1.0733 - val_f1: 0.6861\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 112us/sample - loss: 1.1317 - f1: 0.6881 - val_loss: 0.8924 - val_f1: 0.7546\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 112us/sample - loss: 1.0382 - f1: 0.7151 - val_loss: 0.8519 - val_f1: 0.7657\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 124us/sample - loss: 0.9747 - f1: 0.7289 - val_loss: 0.8601 - val_f1: 0.7508\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.9711 - f1: 0.7271 - val_loss: 0.8096 - val_f1: 0.7792\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.9124 - f1: 0.7490 - val_loss: 0.7599 - val_f1: 0.7971\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.8841 - f1: 0.7544 - val_loss: 0.8372 - val_f1: 0.7513\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 150us/sample - loss: 0.8516 - f1: 0.7661 - val_loss: 0.7050 - val_f1: 0.8168\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.8494 - f1: 0.7712 - val_loss: 0.6878 - val_f1: 0.8217\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.8065 - f1: 0.7903 - val_loss: 0.7816 - val_f1: 0.7836\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 134us/sample - loss: 0.8132 - f1: 0.7897 - val_loss: 0.6387 - val_f1: 0.8495\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 122us/sample - loss: 0.7766 - f1: 0.7976 - val_loss: 0.6679 - val_f1: 0.8185\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.7567 - f1: 0.8062 - val_loss: 0.6136 - val_f1: 0.8649\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.7714 - f1: 0.8045 - val_loss: 0.6336 - val_f1: 0.8375\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.7509 - f1: 0.8106 - val_loss: 0.7088 - val_f1: 0.8029\n",
      "Epoch 21/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 123us/sample - loss: 0.7422 - f1: 0.8172 - val_loss: 0.5914 - val_f1: 0.8645\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 146us/sample - loss: 0.7613 - f1: 0.8133 - val_loss: 0.6473 - val_f1: 0.8431\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 0.7263 - f1: 0.8286 - val_loss: 0.6142 - val_f1: 0.8552\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.7257 - f1: 0.8226 - val_loss: 0.5871 - val_f1: 0.8513\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 0.7373 - f1: 0.8210 - val_loss: 0.7289 - val_f1: 0.7979\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 0.6833 - f1: 0.8386 - val_loss: 0.5729 - val_f1: 0.8691\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 147us/sample - loss: 0.6939 - f1: 0.8339 - val_loss: 0.6294 - val_f1: 0.8325\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 0.6784 - f1: 0.8418 - val_loss: 0.6401 - val_f1: 0.8467\n",
      "Running through fold 2\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 3s 252us/sample - loss: 14.8682 - f1: 0.1009 - val_loss: 6.2810 - val_f1: 0.1563\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 4.5570 - f1: 0.1859 - val_loss: 3.3705 - val_f1: 0.2246\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 2.6405 - f1: 0.3286 - val_loss: 2.0330 - val_f1: 0.4156\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 1.8396 - f1: 0.4461 - val_loss: 1.5353 - val_f1: 0.4806\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 1.5172 - f1: 0.5342 - val_loss: 1.1925 - val_f1: 0.6145\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 1.1952 - f1: 0.6492 - val_loss: 0.9630 - val_f1: 0.7243\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 1.0756 - f1: 0.6943 - val_loss: 0.9261 - val_f1: 0.7241\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 1.0059 - f1: 0.7279 - val_loss: 0.8785 - val_f1: 0.7425\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.9481 - f1: 0.7418 - val_loss: 0.8138 - val_f1: 0.7725\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.9384 - f1: 0.7489 - val_loss: 0.7407 - val_f1: 0.8054\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.8703 - f1: 0.7718 - val_loss: 0.7127 - val_f1: 0.8138\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.8514 - f1: 0.7793 - val_loss: 0.9090 - val_f1: 0.7119\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 126us/sample - loss: 0.8324 - f1: 0.7871 - val_loss: 0.6703 - val_f1: 0.8380\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 131us/sample - loss: 0.8059 - f1: 0.7955 - val_loss: 0.6608 - val_f1: 0.8310\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 116us/sample - loss: 0.7919 - f1: 0.7997 - val_loss: 0.6875 - val_f1: 0.8231\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 117us/sample - loss: 0.7937 - f1: 0.7980 - val_loss: 0.9656 - val_f1: 0.7122\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 119us/sample - loss: 0.7363 - f1: 0.8213 - val_loss: 0.6170 - val_f1: 0.8587\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 122us/sample - loss: 0.7395 - f1: 0.8175 - val_loss: 0.5851 - val_f1: 0.8605\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 149us/sample - loss: 0.7284 - f1: 0.8206 - val_loss: 0.6909 - val_f1: 0.7981\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 0.7210 - f1: 0.8173 - val_loss: 0.5770 - val_f1: 0.8604\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.7104 - f1: 0.8274 - val_loss: 0.6027 - val_f1: 0.8446\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.7254 - f1: 0.8184 - val_loss: 0.5212 - val_f1: 0.8901\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6924 - f1: 0.8354 - val_loss: 0.5855 - val_f1: 0.8615\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.6403 - f1: 0.8501 - val_loss: 0.4867 - val_f1: 0.8940\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.6597 - f1: 0.8431 - val_loss: 0.5005 - val_f1: 0.8958\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6341 - f1: 0.8515 - val_loss: 0.5614 - val_f1: 0.8576\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6908 - f1: 0.8312 - val_loss: 0.5362 - val_f1: 0.8824\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.6143 - f1: 0.8604 - val_loss: 0.4870 - val_f1: 0.8898\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6781 - f1: 0.8373 - val_loss: 0.4963 - val_f1: 0.8959\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6825 - f1: 0.8362 - val_loss: 0.4988 - val_f1: 0.9000\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.6233 - f1: 0.8536 - val_loss: 0.5541 - val_f1: 0.8748\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.6588 - f1: 0.8455 - val_loss: 0.5457 - val_f1: 0.8704\n",
      "Running through fold 3\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 3s 255us/sample - loss: 15.1459 - f1: 0.0746 - val_loss: 6.5036 - val_f1: 0.0950\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 4.9491 - f1: 0.1370 - val_loss: 3.9830 - val_f1: 0.1135\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 3.3557 - f1: 0.2035 - val_loss: 2.8061 - val_f1: 0.2441\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 2.3131 - f1: 0.2989 - val_loss: 1.8670 - val_f1: 0.3410\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 1.7288 - f1: 0.4540 - val_loss: 1.4737 - val_f1: 0.5627\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 1.3996 - f1: 0.5816 - val_loss: 1.1124 - val_f1: 0.6911\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 2s 154us/sample - loss: 1.2301 - f1: 0.6530 - val_loss: 1.0157 - val_f1: 0.7114\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 1.0378 - f1: 0.7233 - val_loss: 0.8673 - val_f1: 0.7744\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.9441 - f1: 0.7585 - val_loss: 0.7463 - val_f1: 0.8188\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.8769 - f1: 0.7779 - val_loss: 0.6741 - val_f1: 0.8493\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.8039 - f1: 0.8050 - val_loss: 0.6661 - val_f1: 0.8449\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.8061 - f1: 0.8042 - val_loss: 0.6497 - val_f1: 0.8543\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 122us/sample - loss: 0.7819 - f1: 0.8143 - val_loss: 0.6521 - val_f1: 0.8568\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 130us/sample - loss: 0.7626 - f1: 0.8216 - val_loss: 0.6456 - val_f1: 0.8544\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.7495 - f1: 0.8274 - val_loss: 0.6612 - val_f1: 0.8382\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 123us/sample - loss: 0.7264 - f1: 0.8327 - val_loss: 0.5827 - val_f1: 0.8847\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.7390 - f1: 0.8307 - val_loss: 0.5646 - val_f1: 0.8792\n",
      "Epoch 18/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6851 - f1: 0.8452 - val_loss: 0.5837 - val_f1: 0.8665\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 129us/sample - loss: 0.7087 - f1: 0.8374 - val_loss: 0.5366 - val_f1: 0.8858\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 2s 156us/sample - loss: 0.6814 - f1: 0.8450 - val_loss: 0.5505 - val_f1: 0.8744\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6908 - f1: 0.8409 - val_loss: 0.5950 - val_f1: 0.8515\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.6983 - f1: 0.8429 - val_loss: 0.5457 - val_f1: 0.8885\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6586 - f1: 0.8511 - val_loss: 0.5279 - val_f1: 0.8865\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6738 - f1: 0.8474 - val_loss: 0.5974 - val_f1: 0.8550\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 2s 150us/sample - loss: 0.6290 - f1: 0.8606 - val_loss: 0.4725 - val_f1: 0.9077\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6544 - f1: 0.8509 - val_loss: 0.8626 - val_f1: 0.7551\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6445 - f1: 0.8522 - val_loss: 0.7216 - val_f1: 0.8024\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6754 - f1: 0.8418 - val_loss: 0.5863 - val_f1: 0.8582\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 0.6392 - f1: 0.8576 - val_loss: 0.5057 - val_f1: 0.8936\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 2s 153us/sample - loss: 0.6318 - f1: 0.8588 - val_loss: 0.4985 - val_f1: 0.8922\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.6437 - f1: 0.8518 - val_loss: 0.5650 - val_f1: 0.8556\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 2s 151us/sample - loss: 0.5947 - f1: 0.8678 - val_loss: 0.4746 - val_f1: 0.9035\n",
      "Epoch 33/2000\n",
      "10000/10000 [==============================] - 1s 148us/sample - loss: 0.6243 - f1: 0.8606 - val_loss: 0.4520 - val_f1: 0.9051\n",
      "Epoch 34/2000\n",
      "10000/10000 [==============================] - 1s 114us/sample - loss: 0.6066 - f1: 0.8640 - val_loss: 0.5402 - val_f1: 0.8756\n",
      "Epoch 35/2000\n",
      "10000/10000 [==============================] - 1s 129us/sample - loss: 0.6204 - f1: 0.8594 - val_loss: 0.4648 - val_f1: 0.9148\n",
      "Running through fold 4\n",
      "Train on 10000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "10000/10000 [==============================] - 3s 256us/sample - loss: 14.8270 - f1: 0.0632 - val_loss: 6.1742 - val_f1: 0.1227\n",
      "Epoch 2/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 4.4581 - f1: 0.2142 - val_loss: 3.2600 - val_f1: 0.2751\n",
      "Epoch 3/2000\n",
      "10000/10000 [==============================] - 2s 152us/sample - loss: 2.5144 - f1: 0.3622 - val_loss: 1.7257 - val_f1: 0.5387\n",
      "Epoch 4/2000\n",
      "10000/10000 [==============================] - 1s 115us/sample - loss: 1.4983 - f1: 0.6297 - val_loss: 1.1270 - val_f1: 0.7326\n",
      "Epoch 5/2000\n",
      "10000/10000 [==============================] - 1s 132us/sample - loss: 1.2484 - f1: 0.7109 - val_loss: 1.0237 - val_f1: 0.7497\n",
      "Epoch 6/2000\n",
      "10000/10000 [==============================] - 1s 119us/sample - loss: 1.0692 - f1: 0.7409 - val_loss: 0.8648 - val_f1: 0.7892\n",
      "Epoch 7/2000\n",
      "10000/10000 [==============================] - 1s 117us/sample - loss: 0.9538 - f1: 0.7724 - val_loss: 0.7750 - val_f1: 0.8259\n",
      "Epoch 8/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.8953 - f1: 0.7845 - val_loss: 0.8659 - val_f1: 0.7528\n",
      "Epoch 9/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.8692 - f1: 0.7837 - val_loss: 0.7627 - val_f1: 0.8080\n",
      "Epoch 10/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.8287 - f1: 0.8010 - val_loss: 0.7629 - val_f1: 0.8047\n",
      "Epoch 11/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.7990 - f1: 0.8126 - val_loss: 0.6760 - val_f1: 0.8335\n",
      "Epoch 12/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.7805 - f1: 0.8093 - val_loss: 0.6832 - val_f1: 0.8323\n",
      "Epoch 13/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.7629 - f1: 0.8145 - val_loss: 0.6317 - val_f1: 0.8519\n",
      "Epoch 14/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.7473 - f1: 0.8225 - val_loss: 0.6146 - val_f1: 0.8554\n",
      "Epoch 15/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.7197 - f1: 0.8289 - val_loss: 0.6731 - val_f1: 0.8168\n",
      "Epoch 16/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.7286 - f1: 0.8237 - val_loss: 0.7266 - val_f1: 0.7921\n",
      "Epoch 17/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.7133 - f1: 0.8336 - val_loss: 0.5912 - val_f1: 0.8670\n",
      "Epoch 18/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6971 - f1: 0.8325 - val_loss: 0.5835 - val_f1: 0.8633\n",
      "Epoch 19/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6862 - f1: 0.8373 - val_loss: 0.5700 - val_f1: 0.8801\n",
      "Epoch 20/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6779 - f1: 0.8422 - val_loss: 0.7376 - val_f1: 0.7881\n",
      "Epoch 21/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6789 - f1: 0.8400 - val_loss: 0.5693 - val_f1: 0.8580\n",
      "Epoch 22/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6644 - f1: 0.8433 - val_loss: 0.5197 - val_f1: 0.8946\n",
      "Epoch 23/2000\n",
      "10000/10000 [==============================] - 1s 120us/sample - loss: 0.6328 - f1: 0.8554 - val_loss: 0.5350 - val_f1: 0.8737\n",
      "Epoch 24/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6362 - f1: 0.8523 - val_loss: 0.5977 - val_f1: 0.8551\n",
      "Epoch 25/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6462 - f1: 0.8507 - val_loss: 0.5505 - val_f1: 0.8659\n",
      "Epoch 26/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6504 - f1: 0.8541 - val_loss: 0.4730 - val_f1: 0.9019\n",
      "Epoch 27/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6642 - f1: 0.8410 - val_loss: 0.5027 - val_f1: 0.8893\n",
      "Epoch 28/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6367 - f1: 0.8563 - val_loss: 0.6053 - val_f1: 0.8371\n",
      "Epoch 29/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6069 - f1: 0.8641 - val_loss: 0.5594 - val_f1: 0.8513\n",
      "Epoch 30/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6303 - f1: 0.8529 - val_loss: 0.5888 - val_f1: 0.8467\n",
      "Epoch 31/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6042 - f1: 0.8609 - val_loss: 0.5436 - val_f1: 0.8691\n",
      "Epoch 32/2000\n",
      "10000/10000 [==============================] - 1s 121us/sample - loss: 0.6121 - f1: 0.8601 - val_loss: 0.4792 - val_f1: 0.8957\n",
      "\n",
      "\n",
      "Running through training size 15000\n",
      "Running through fold 0\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 185us/sample - loss: 11.8073 - f1: 0.0797 - val_loss: 4.2954 - val_f1: 0.1083\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 3.2646 - f1: 0.1788 - val_loss: 2.5754 - val_f1: 0.2022\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 2.2470 - f1: 0.2916 - val_loss: 2.0506 - val_f1: 0.3701\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 1.7485 - f1: 0.4306 - val_loss: 1.2027 - val_f1: 0.5984\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 1.1896 - f1: 0.6473 - val_loss: 0.9260 - val_f1: 0.7310\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 1.0156 - f1: 0.7239 - val_loss: 0.8994 - val_f1: 0.7364\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.9461 - f1: 0.7495 - val_loss: 0.7468 - val_f1: 0.8110\n",
      "Epoch 8/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.8509 - f1: 0.7855 - val_loss: 0.7485 - val_f1: 0.8000\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.8555 - f1: 0.7812 - val_loss: 0.6811 - val_f1: 0.8256\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7936 - f1: 0.8052 - val_loss: 0.6692 - val_f1: 0.8334\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7602 - f1: 0.8177 - val_loss: 0.5813 - val_f1: 0.8741\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7203 - f1: 0.8315 - val_loss: 0.5386 - val_f1: 0.8943\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 0.7221 - f1: 0.8283 - val_loss: 0.6060 - val_f1: 0.8517\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6904 - f1: 0.8380 - val_loss: 0.5283 - val_f1: 0.8870\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7043 - f1: 0.8278 - val_loss: 0.5173 - val_f1: 0.8954\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.6601 - f1: 0.8446 - val_loss: 0.5367 - val_f1: 0.8819\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.6679 - f1: 0.8426 - val_loss: 0.7289 - val_f1: 0.7947\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6628 - f1: 0.8420 - val_loss: 0.5483 - val_f1: 0.8738\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.6259 - f1: 0.8564 - val_loss: 0.4801 - val_f1: 0.9010\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6470 - f1: 0.8522 - val_loss: 0.5871 - val_f1: 0.8527\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.6342 - f1: 0.8527 - val_loss: 0.5265 - val_f1: 0.8773\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 115us/sample - loss: 0.6562 - f1: 0.8408 - val_loss: 0.7630 - val_f1: 0.7849\n",
      "Running through fold 1\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 186us/sample - loss: 11.6134 - f1: 0.1630 - val_loss: 3.9427 - val_f1: 0.2641\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 2.5707 - f1: 0.4375 - val_loss: 1.6579 - val_f1: 0.5993\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 1.4212 - f1: 0.6680 - val_loss: 1.1890 - val_f1: 0.7266\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 1.1391 - f1: 0.7368 - val_loss: 0.8860 - val_f1: 0.7845\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.9288 - f1: 0.7784 - val_loss: 0.7869 - val_f1: 0.8186\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.8408 - f1: 0.8035 - val_loss: 0.6708 - val_f1: 0.8576\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7997 - f1: 0.8139 - val_loss: 0.6353 - val_f1: 0.8638\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7362 - f1: 0.8336 - val_loss: 0.6929 - val_f1: 0.8370\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7087 - f1: 0.8399 - val_loss: 0.6316 - val_f1: 0.8512\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6931 - f1: 0.8437 - val_loss: 0.5548 - val_f1: 0.8955\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6733 - f1: 0.8512 - val_loss: 0.5571 - val_f1: 0.8855\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6645 - f1: 0.8519 - val_loss: 0.5387 - val_f1: 0.8914\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6399 - f1: 0.8579 - val_loss: 0.7562 - val_f1: 0.7706\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6245 - f1: 0.8645 - val_loss: 0.4868 - val_f1: 0.9100\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6126 - f1: 0.8655 - val_loss: 0.4864 - val_f1: 0.9148\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6086 - f1: 0.8669 - val_loss: 0.5463 - val_f1: 0.8842\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.5898 - f1: 0.8714 - val_loss: 0.4761 - val_f1: 0.9038\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.5911 - f1: 0.8709 - val_loss: 0.6435 - val_f1: 0.8364\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6033 - f1: 0.8675 - val_loss: 0.4940 - val_f1: 0.9104\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.5769 - f1: 0.8771 - val_loss: 0.4237 - val_f1: 0.9336\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.5617 - f1: 0.8794 - val_loss: 0.4241 - val_f1: 0.9364\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.6027 - f1: 0.8634 - val_loss: 0.5157 - val_f1: 0.8969\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.5530 - f1: 0.8819 - val_loss: 0.4668 - val_f1: 0.9145\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.5372 - f1: 0.8886 - val_loss: 0.5341 - val_f1: 0.8775\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.5165 - f1: 0.8959 - val_loss: 0.4197 - val_f1: 0.9360\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 0.5133 - f1: 0.8968 - val_loss: 0.3991 - val_f1: 0.9355\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.5260 - f1: 0.8921 - val_loss: 0.5116 - val_f1: 0.8882\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.5309 - f1: 0.8910 - val_loss: 0.5800 - val_f1: 0.8455\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5412 - f1: 0.8868 - val_loss: 0.3930 - val_f1: 0.9372\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5074 - f1: 0.8989 - val_loss: 0.4442 - val_f1: 0.9255\n",
      "Running through fold 2\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 189us/sample - loss: 12.5487 - f1: 0.1156 - val_loss: 4.7083 - val_f1: 0.1453\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 3.5413 - f1: 0.1846 - val_loss: 2.7611 - val_f1: 0.2387\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 2.2800 - f1: 0.3217 - val_loss: 1.8277 - val_f1: 0.4565\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 1.4753 - f1: 0.5765 - val_loss: 0.9981 - val_f1: 0.7212\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 1.0274 - f1: 0.7346 - val_loss: 0.9263 - val_f1: 0.7343\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.9273 - f1: 0.7655 - val_loss: 0.7423 - val_f1: 0.8084\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.8135 - f1: 0.8059 - val_loss: 0.6296 - val_f1: 0.8700\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 0.7708 - f1: 0.8191 - val_loss: 0.6468 - val_f1: 0.8533\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 116us/sample - loss: 0.7375 - f1: 0.8346 - val_loss: 0.6215 - val_f1: 0.8567\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 0.6876 - f1: 0.8490 - val_loss: 0.5452 - val_f1: 0.8912\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 0.6969 - f1: 0.8487 - val_loss: 0.7873 - val_f1: 0.7939\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.6935 - f1: 0.8452 - val_loss: 0.5344 - val_f1: 0.8981\n",
      "Epoch 13/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.6283 - f1: 0.8631 - val_loss: 0.6151 - val_f1: 0.8567\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.6098 - f1: 0.8710 - val_loss: 0.4836 - val_f1: 0.9089\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.6035 - f1: 0.8741 - val_loss: 0.5268 - val_f1: 0.8913\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.6395 - f1: 0.8637 - val_loss: 0.5098 - val_f1: 0.9004\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5965 - f1: 0.8738 - val_loss: 0.5013 - val_f1: 0.9080\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5585 - f1: 0.8901 - val_loss: 0.4867 - val_f1: 0.9030\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.5571 - f1: 0.8881 - val_loss: 0.4518 - val_f1: 0.9252\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5287 - f1: 0.8900 - val_loss: 0.4434 - val_f1: 0.9163\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5471 - f1: 0.8858 - val_loss: 0.4344 - val_f1: 0.9301\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5581 - f1: 0.8853 - val_loss: 0.5256 - val_f1: 0.8781\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5487 - f1: 0.8911 - val_loss: 0.3652 - val_f1: 0.9522\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 117us/sample - loss: 0.5382 - f1: 0.8940 - val_loss: 0.4291 - val_f1: 0.9186\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.5181 - f1: 0.9001 - val_loss: 0.4121 - val_f1: 0.9411\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 120us/sample - loss: 0.5388 - f1: 0.8897 - val_loss: 0.4113 - val_f1: 0.9351\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.5141 - f1: 0.8982 - val_loss: 0.3476 - val_f1: 0.9649\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.5209 - f1: 0.8954 - val_loss: 0.3947 - val_f1: 0.9338\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.4976 - f1: 0.9034 - val_loss: 0.3694 - val_f1: 0.9483\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.4909 - f1: 0.9058 - val_loss: 0.4026 - val_f1: 0.9388\n",
      "Epoch 31/2000\n",
      "15000/15000 [==============================] - 2s 120us/sample - loss: 0.5309 - f1: 0.8887 - val_loss: 0.3672 - val_f1: 0.9477\n",
      "Epoch 32/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5077 - f1: 0.9017 - val_loss: 0.4989 - val_f1: 0.8859\n",
      "Epoch 33/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.5321 - f1: 0.8932 - val_loss: 0.6178 - val_f1: 0.8466\n",
      "Epoch 34/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.4917 - f1: 0.9083 - val_loss: 0.3820 - val_f1: 0.9385\n",
      "Epoch 35/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.4774 - f1: 0.9087 - val_loss: 0.3510 - val_f1: 0.9483\n",
      "Epoch 36/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.5075 - f1: 0.8977 - val_loss: 0.6561 - val_f1: 0.8319\n",
      "Epoch 37/2000\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.5490 - f1: 0.8860 - val_loss: 0.4236 - val_f1: 0.9218\n",
      "Running through fold 3\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 205us/sample - loss: 12.4601 - f1: 0.0901 - val_loss: 5.0683 - val_f1: 0.0981\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 122us/sample - loss: 3.7823 - f1: 0.1080 - val_loss: 2.7364 - val_f1: 0.1618\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 2.3296 - f1: 0.1600 - val_loss: 2.0062 - val_f1: 0.2318\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 1.6853 - f1: 0.3978 - val_loss: 1.4152 - val_f1: 0.4651\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 1.3341 - f1: 0.5453 - val_loss: 1.0666 - val_f1: 0.6505\n",
      "Epoch 6/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 1.1435 - f1: 0.6528 - val_loss: 0.8641 - val_f1: 0.7652\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.9580 - f1: 0.7417 - val_loss: 0.7890 - val_f1: 0.7790\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 123us/sample - loss: 0.8816 - f1: 0.7710 - val_loss: 0.7954 - val_f1: 0.7776\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 122us/sample - loss: 0.8254 - f1: 0.7895 - val_loss: 0.6867 - val_f1: 0.8265\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 119us/sample - loss: 0.8293 - f1: 0.7883 - val_loss: 0.6283 - val_f1: 0.8623\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 118us/sample - loss: 0.7676 - f1: 0.8127 - val_loss: 0.5899 - val_f1: 0.8661\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.7494 - f1: 0.8204 - val_loss: 0.5581 - val_f1: 0.8803\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 121us/sample - loss: 0.7514 - f1: 0.8166 - val_loss: 0.6336 - val_f1: 0.8382\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.7116 - f1: 0.8341 - val_loss: 0.5408 - val_f1: 0.8812\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.6922 - f1: 0.8389 - val_loss: 0.5203 - val_f1: 0.8939\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 128us/sample - loss: 0.6595 - f1: 0.8477 - val_loss: 0.5257 - val_f1: 0.8992\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.6614 - f1: 0.8473 - val_loss: 0.4915 - val_f1: 0.9071\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.6630 - f1: 0.8467 - val_loss: 0.5909 - val_f1: 0.8685\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.6614 - f1: 0.8496 - val_loss: 0.4648 - val_f1: 0.9129\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.6441 - f1: 0.8551 - val_loss: 0.4865 - val_f1: 0.9099\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6496 - f1: 0.8552 - val_loss: 0.7213 - val_f1: 0.8327\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 120us/sample - loss: 0.6251 - f1: 0.8607 - val_loss: 0.5080 - val_f1: 0.8984\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.6231 - f1: 0.8558 - val_loss: 0.5506 - val_f1: 0.8691\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.6012 - f1: 0.8666 - val_loss: 0.4932 - val_f1: 0.9028\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 128us/sample - loss: 0.5988 - f1: 0.8671 - val_loss: 0.4720 - val_f1: 0.9000\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.6345 - f1: 0.8549 - val_loss: 0.6092 - val_f1: 0.8377\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 120us/sample - loss: 0.5810 - f1: 0.8716 - val_loss: 0.4383 - val_f1: 0.9114\n",
      "Running through fold 4\n",
      "Train on 15000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "15000/15000 [==============================] - 3s 209us/sample - loss: 10.7894 - f1: 0.1601 - val_loss: 4.1407 - val_f1: 0.2325\n",
      "Epoch 2/2000\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 2.7704 - f1: 0.3389 - val_loss: 2.1433 - val_f1: 0.4119\n",
      "Epoch 3/2000\n",
      "15000/15000 [==============================] - 2s 122us/sample - loss: 1.4997 - f1: 0.5974 - val_loss: 1.3649 - val_f1: 0.6447\n",
      "Epoch 4/2000\n",
      "15000/15000 [==============================] - 2s 131us/sample - loss: 1.1876 - f1: 0.7056 - val_loss: 0.9083 - val_f1: 0.7730\n",
      "Epoch 5/2000\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.9377 - f1: 0.7751 - val_loss: 0.7187 - val_f1: 0.8552\n",
      "Epoch 6/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.8310 - f1: 0.8190 - val_loss: 0.6985 - val_f1: 0.8477\n",
      "Epoch 7/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.7712 - f1: 0.8310 - val_loss: 0.6065 - val_f1: 0.8951\n",
      "Epoch 8/2000\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.7182 - f1: 0.8473 - val_loss: 0.6258 - val_f1: 0.8640\n",
      "Epoch 9/2000\n",
      "15000/15000 [==============================] - 2s 132us/sample - loss: 0.7127 - f1: 0.8473 - val_loss: 0.6444 - val_f1: 0.8561\n",
      "Epoch 10/2000\n",
      "15000/15000 [==============================] - 2s 129us/sample - loss: 0.6744 - f1: 0.8569 - val_loss: 0.5575 - val_f1: 0.8992\n",
      "Epoch 11/2000\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.6645 - f1: 0.8591 - val_loss: 0.5537 - val_f1: 0.8869\n",
      "Epoch 12/2000\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.6306 - f1: 0.8693 - val_loss: 0.5313 - val_f1: 0.9036\n",
      "Epoch 13/2000\n",
      "15000/15000 [==============================] - 2s 134us/sample - loss: 0.6409 - f1: 0.8634 - val_loss: 0.4819 - val_f1: 0.9226\n",
      "Epoch 14/2000\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.6515 - f1: 0.8598 - val_loss: 0.5098 - val_f1: 0.9182\n",
      "Epoch 15/2000\n",
      "15000/15000 [==============================] - 2s 136us/sample - loss: 0.6079 - f1: 0.8747 - val_loss: 0.4882 - val_f1: 0.9217\n",
      "Epoch 16/2000\n",
      "15000/15000 [==============================] - 2s 128us/sample - loss: 0.6244 - f1: 0.8702 - val_loss: 0.4958 - val_f1: 0.9163\n",
      "Epoch 17/2000\n",
      "15000/15000 [==============================] - 2s 132us/sample - loss: 0.5992 - f1: 0.8738 - val_loss: 0.5812 - val_f1: 0.8679\n",
      "Epoch 18/2000\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.5696 - f1: 0.8892 - val_loss: 0.4794 - val_f1: 0.9014\n",
      "Epoch 19/2000\n",
      "15000/15000 [==============================] - 2s 128us/sample - loss: 0.5836 - f1: 0.8803 - val_loss: 0.5186 - val_f1: 0.8985\n",
      "Epoch 20/2000\n",
      "15000/15000 [==============================] - 2s 128us/sample - loss: 0.5681 - f1: 0.8857 - val_loss: 0.4016 - val_f1: 0.9496\n",
      "Epoch 21/2000\n",
      "15000/15000 [==============================] - 2s 137us/sample - loss: 0.5828 - f1: 0.8774 - val_loss: 0.6459 - val_f1: 0.8574\n",
      "Epoch 22/2000\n",
      "15000/15000 [==============================] - 2s 129us/sample - loss: 0.5751 - f1: 0.8879 - val_loss: 0.4279 - val_f1: 0.9389\n",
      "Epoch 23/2000\n",
      "15000/15000 [==============================] - 2s 125us/sample - loss: 0.5352 - f1: 0.8934 - val_loss: 0.6476 - val_f1: 0.8283\n",
      "Epoch 24/2000\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.5881 - f1: 0.8814 - val_loss: 0.4289 - val_f1: 0.9371\n",
      "Epoch 25/2000\n",
      "15000/15000 [==============================] - 2s 127us/sample - loss: 0.5362 - f1: 0.8956 - val_loss: 0.4629 - val_f1: 0.9134\n",
      "Epoch 26/2000\n",
      "15000/15000 [==============================] - 2s 139us/sample - loss: 0.5373 - f1: 0.8938 - val_loss: 0.4219 - val_f1: 0.9350\n",
      "Epoch 27/2000\n",
      "15000/15000 [==============================] - 2s 124us/sample - loss: 0.5709 - f1: 0.8859 - val_loss: 0.4042 - val_f1: 0.9402\n",
      "Epoch 28/2000\n",
      "15000/15000 [==============================] - 2s 126us/sample - loss: 0.5464 - f1: 0.8900 - val_loss: 0.4349 - val_f1: 0.9387\n",
      "Epoch 29/2000\n",
      "15000/15000 [==============================] - 2s 130us/sample - loss: 0.5281 - f1: 0.8984 - val_loss: 0.4768 - val_f1: 0.9196\n",
      "Epoch 30/2000\n",
      "15000/15000 [==============================] - 2s 128us/sample - loss: 0.5401 - f1: 0.8933 - val_loss: 0.3810 - val_f1: 0.9488\n",
      "\n",
      "\n",
      "Running through training size 20000\n",
      "Running through fold 0\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 191us/sample - loss: 9.6346 - f1: 0.1104 - val_loss: 3.5869 - val_f1: 0.1780\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 2.4991 - f1: 0.3174 - val_loss: 1.4879 - val_f1: 0.5273\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 1.3161 - f1: 0.6244 - val_loss: 1.0281 - val_f1: 0.6817\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.9830 - f1: 0.7278 - val_loss: 0.7626 - val_f1: 0.8147\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 2s 118us/sample - loss: 0.8557 - f1: 0.7847 - val_loss: 0.6395 - val_f1: 0.8582\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.7793 - f1: 0.8070 - val_loss: 0.8057 - val_f1: 0.7590\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.7444 - f1: 0.8161 - val_loss: 0.5722 - val_f1: 0.8870\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.7046 - f1: 0.8284 - val_loss: 0.6232 - val_f1: 0.8495\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6904 - f1: 0.8304 - val_loss: 0.6766 - val_f1: 0.8200\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6659 - f1: 0.8401 - val_loss: 0.5679 - val_f1: 0.8678\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 0.6507 - f1: 0.8462 - val_loss: 0.4992 - val_f1: 0.8904\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.6276 - f1: 0.8505 - val_loss: 0.4944 - val_f1: 0.8993\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 2s 124us/sample - loss: 0.6477 - f1: 0.8471 - val_loss: 0.4553 - val_f1: 0.9153\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.6143 - f1: 0.8594 - val_loss: 0.4973 - val_f1: 0.8996\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.6142 - f1: 0.8561 - val_loss: 0.4909 - val_f1: 0.8938\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5874 - f1: 0.8651 - val_loss: 0.5788 - val_f1: 0.8590\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.6276 - f1: 0.8517 - val_loss: 0.4410 - val_f1: 0.9230\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5970 - f1: 0.8654 - val_loss: 0.5664 - val_f1: 0.8559\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 0.5986 - f1: 0.8620 - val_loss: 0.4994 - val_f1: 0.9044\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5701 - f1: 0.8700 - val_loss: 0.4511 - val_f1: 0.9084\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6072 - f1: 0.8608 - val_loss: 0.4695 - val_f1: 0.9072\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5690 - f1: 0.8732 - val_loss: 0.5537 - val_f1: 0.8557\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5669 - f1: 0.8705 - val_loss: 0.4816 - val_f1: 0.8980\n",
      "Running through fold 1\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 193us/sample - loss: 9.5615 - f1: 0.0913 - val_loss: 3.7195 - val_f1: 0.1466\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 2.8415 - f1: 0.1793 - val_loss: 2.0918 - val_f1: 0.2507\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 1.7066 - f1: 0.4125 - val_loss: 1.2971 - val_f1: 0.5529\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 1.0922 - f1: 0.6918 - val_loss: 0.8611 - val_f1: 0.7533\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.8807 - f1: 0.7773 - val_loss: 0.6954 - val_f1: 0.8310\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.8126 - f1: 0.7993 - val_loss: 0.8821 - val_f1: 0.7207\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.7588 - f1: 0.8196 - val_loss: 0.6495 - val_f1: 0.8346\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 2s 121us/sample - loss: 0.7264 - f1: 0.8292 - val_loss: 0.5376 - val_f1: 0.8971\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.7058 - f1: 0.8354 - val_loss: 0.8455 - val_f1: 0.7549\n",
      "Epoch 10/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6848 - f1: 0.8430 - val_loss: 0.5240 - val_f1: 0.8909\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6599 - f1: 0.8476 - val_loss: 0.5247 - val_f1: 0.8891\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.6600 - f1: 0.8453 - val_loss: 0.5063 - val_f1: 0.9026\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6561 - f1: 0.8499 - val_loss: 0.4533 - val_f1: 0.9215\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.6443 - f1: 0.8509 - val_loss: 0.4742 - val_f1: 0.9026\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6057 - f1: 0.8628 - val_loss: 0.5784 - val_f1: 0.8511\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.6024 - f1: 0.8658 - val_loss: 0.4823 - val_f1: 0.9101\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.6064 - f1: 0.8588 - val_loss: 0.4183 - val_f1: 0.9269\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5884 - f1: 0.8637 - val_loss: 0.4385 - val_f1: 0.9157\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 2s 118us/sample - loss: 0.5730 - f1: 0.8696 - val_loss: 0.4650 - val_f1: 0.9035\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 2s 119us/sample - loss: 0.6201 - f1: 0.8518 - val_loss: 0.4945 - val_f1: 0.8905\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5697 - f1: 0.8663 - val_loss: 0.4199 - val_f1: 0.9276\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.6085 - f1: 0.8561 - val_loss: 0.4402 - val_f1: 0.9174\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5920 - f1: 0.8625 - val_loss: 0.4742 - val_f1: 0.9074\n",
      "Running through fold 2\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 192us/sample - loss: 9.1218 - f1: 0.1295 - val_loss: 3.1172 - val_f1: 0.2663\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 2.1719 - f1: 0.4199 - val_loss: 1.3079 - val_f1: 0.6484\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 1.2334 - f1: 0.6866 - val_loss: 0.8753 - val_f1: 0.7828\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.9526 - f1: 0.7575 - val_loss: 0.8499 - val_f1: 0.7705\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.8666 - f1: 0.7801 - val_loss: 0.7983 - val_f1: 0.7868\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.8097 - f1: 0.8044 - val_loss: 0.7760 - val_f1: 0.7772\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 2s 124us/sample - loss: 0.7349 - f1: 0.8340 - val_loss: 0.7350 - val_f1: 0.8066\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.7002 - f1: 0.8426 - val_loss: 0.5973 - val_f1: 0.8677\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 2s 122us/sample - loss: 0.6675 - f1: 0.8532 - val_loss: 0.4953 - val_f1: 0.9091\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 2s 119us/sample - loss: 0.6335 - f1: 0.8664 - val_loss: 0.4818 - val_f1: 0.9143\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.6234 - f1: 0.8679 - val_loss: 0.5344 - val_f1: 0.8949\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.6345 - f1: 0.8639 - val_loss: 0.4627 - val_f1: 0.9343\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.6073 - f1: 0.8773 - val_loss: 0.5301 - val_f1: 0.8965\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.6086 - f1: 0.8745 - val_loss: 0.5180 - val_f1: 0.8936\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5869 - f1: 0.8776 - val_loss: 0.4739 - val_f1: 0.9088\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5734 - f1: 0.8843 - val_loss: 0.4257 - val_f1: 0.9314\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5906 - f1: 0.8790 - val_loss: 0.4539 - val_f1: 0.9298\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5542 - f1: 0.8909 - val_loss: 0.4009 - val_f1: 0.9430\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.5868 - f1: 0.8760 - val_loss: 0.3822 - val_f1: 0.9607\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.5677 - f1: 0.8839 - val_loss: 0.4128 - val_f1: 0.9331\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5959 - f1: 0.8746 - val_loss: 0.5142 - val_f1: 0.8884\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.5661 - f1: 0.8906 - val_loss: 0.4360 - val_f1: 0.9303\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 2s 124us/sample - loss: 0.5549 - f1: 0.8910 - val_loss: 0.4557 - val_f1: 0.9178\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.5610 - f1: 0.8846 - val_loss: 0.5139 - val_f1: 0.8906\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5393 - f1: 0.8940 - val_loss: 0.3554 - val_f1: 0.9667\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5322 - f1: 0.8940 - val_loss: 0.4446 - val_f1: 0.9186\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5904 - f1: 0.8762 - val_loss: 0.4249 - val_f1: 0.9352\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 0.5406 - f1: 0.8920 - val_loss: 0.4203 - val_f1: 0.9368\n",
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5379 - f1: 0.8923 - val_loss: 0.3600 - val_f1: 0.9539\n",
      "Running through fold 3\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 201us/sample - loss: 9.4018 - f1: 0.1188 - val_loss: 3.1895 - val_f1: 0.2280\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 2.0971 - f1: 0.4674 - val_loss: 1.1911 - val_f1: 0.6938\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 1.1192 - f1: 0.7423 - val_loss: 0.8182 - val_f1: 0.8220\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.8443 - f1: 0.8042 - val_loss: 0.6558 - val_f1: 0.8717\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.7363 - f1: 0.8361 - val_loss: 0.6742 - val_f1: 0.8332\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.7014 - f1: 0.8454 - val_loss: 0.5643 - val_f1: 0.8967\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.6570 - f1: 0.8563 - val_loss: 0.5953 - val_f1: 0.8817\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.6383 - f1: 0.8656 - val_loss: 0.5261 - val_f1: 0.9138\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.6050 - f1: 0.8735 - val_loss: 0.5067 - val_f1: 0.9046\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.5976 - f1: 0.8721 - val_loss: 0.5537 - val_f1: 0.8823\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5706 - f1: 0.8825 - val_loss: 0.4357 - val_f1: 0.9323\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5630 - f1: 0.8842 - val_loss: 0.5233 - val_f1: 0.8938\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5567 - f1: 0.8866 - val_loss: 0.4360 - val_f1: 0.9252\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.5379 - f1: 0.8912 - val_loss: 0.4203 - val_f1: 0.9396\n",
      "Epoch 15/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000/20000 [==============================] - 2s 124us/sample - loss: 0.5279 - f1: 0.8945 - val_loss: 0.4524 - val_f1: 0.9252\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.5514 - f1: 0.8860 - val_loss: 0.5286 - val_f1: 0.8853\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5295 - f1: 0.8928 - val_loss: 0.4008 - val_f1: 0.9462\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5185 - f1: 0.8951 - val_loss: 0.4272 - val_f1: 0.9255\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 132us/sample - loss: 0.5167 - f1: 0.8952 - val_loss: 0.3988 - val_f1: 0.9389\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 0.5311 - f1: 0.8911 - val_loss: 0.4334 - val_f1: 0.9190\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5387 - f1: 0.8896 - val_loss: 0.3780 - val_f1: 0.9491\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 2s 122us/sample - loss: 0.4927 - f1: 0.9037 - val_loss: 0.4970 - val_f1: 0.8932\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5195 - f1: 0.8952 - val_loss: 0.4228 - val_f1: 0.9312\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5170 - f1: 0.8969 - val_loss: 0.4381 - val_f1: 0.9251\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.4884 - f1: 0.9051 - val_loss: 0.3745 - val_f1: 0.9459\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5341 - f1: 0.8895 - val_loss: 0.3792 - val_f1: 0.9571\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.4785 - f1: 0.9097 - val_loss: 0.3490 - val_f1: 0.9550\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.4999 - f1: 0.9020 - val_loss: 0.4520 - val_f1: 0.8985\n",
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.4574 - f1: 0.9125 - val_loss: 0.4706 - val_f1: 0.9106\n",
      "Epoch 30/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.4851 - f1: 0.9044 - val_loss: 0.9084 - val_f1: 0.7343\n",
      "Epoch 31/2000\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 0.5037 - f1: 0.9008 - val_loss: 0.3983 - val_f1: 0.9267\n",
      "Epoch 32/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5237 - f1: 0.8920 - val_loss: 0.3515 - val_f1: 0.9577\n",
      "Epoch 33/2000\n",
      "20000/20000 [==============================] - 2s 122us/sample - loss: 0.5170 - f1: 0.8963 - val_loss: 0.3735 - val_f1: 0.9405\n",
      "Epoch 34/2000\n",
      "20000/20000 [==============================] - 2s 124us/sample - loss: 0.4896 - f1: 0.9046 - val_loss: 0.4596 - val_f1: 0.9066\n",
      "Epoch 35/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.4853 - f1: 0.9070 - val_loss: 0.3741 - val_f1: 0.9320\n",
      "Epoch 36/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.4708 - f1: 0.9095 - val_loss: 0.4105 - val_f1: 0.9395\n",
      "Running through fold 4\n",
      "Train on 20000 samples, validate on 3000 samples\n",
      "Epoch 1/2000\n",
      "20000/20000 [==============================] - 4s 206us/sample - loss: 9.9659 - f1: 0.1071 - val_loss: 3.3592 - val_f1: 0.1905\n",
      "Epoch 2/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 1.9482 - f1: 0.5153 - val_loss: 1.1510 - val_f1: 0.7125\n",
      "Epoch 3/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 1.1795 - f1: 0.7345 - val_loss: 0.8982 - val_f1: 0.7819\n",
      "Epoch 4/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.9291 - f1: 0.7835 - val_loss: 0.8010 - val_f1: 0.8248\n",
      "Epoch 5/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.8324 - f1: 0.8053 - val_loss: 0.6967 - val_f1: 0.8464\n",
      "Epoch 6/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.7625 - f1: 0.8224 - val_loss: 0.6072 - val_f1: 0.8818\n",
      "Epoch 7/2000\n",
      "20000/20000 [==============================] - 3s 129us/sample - loss: 0.7027 - f1: 0.8422 - val_loss: 0.6062 - val_f1: 0.8692\n",
      "Epoch 8/2000\n",
      "20000/20000 [==============================] - 2s 122us/sample - loss: 0.6648 - f1: 0.8526 - val_loss: 0.6190 - val_f1: 0.8601\n",
      "Epoch 9/2000\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.6589 - f1: 0.8542 - val_loss: 0.4993 - val_f1: 0.9160\n",
      "Epoch 10/2000\n",
      "20000/20000 [==============================] - 2s 121us/sample - loss: 0.6267 - f1: 0.8616 - val_loss: 0.5177 - val_f1: 0.9006\n",
      "Epoch 11/2000\n",
      "20000/20000 [==============================] - 2s 125us/sample - loss: 0.6080 - f1: 0.8685 - val_loss: 0.4979 - val_f1: 0.9122\n",
      "Epoch 12/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5935 - f1: 0.8732 - val_loss: 0.5492 - val_f1: 0.8886\n",
      "Epoch 13/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.6052 - f1: 0.8683 - val_loss: 0.4345 - val_f1: 0.9373\n",
      "Epoch 14/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5684 - f1: 0.8832 - val_loss: 0.5413 - val_f1: 0.8911\n",
      "Epoch 15/2000\n",
      "20000/20000 [==============================] - 3s 125us/sample - loss: 0.5550 - f1: 0.8867 - val_loss: 0.4619 - val_f1: 0.9198\n",
      "Epoch 16/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5320 - f1: 0.8912 - val_loss: 0.4165 - val_f1: 0.9349\n",
      "Epoch 17/2000\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.5316 - f1: 0.8931 - val_loss: 0.4549 - val_f1: 0.9153\n",
      "Epoch 18/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5519 - f1: 0.8856 - val_loss: 0.4788 - val_f1: 0.9079\n",
      "Epoch 19/2000\n",
      "20000/20000 [==============================] - 3s 126us/sample - loss: 0.5320 - f1: 0.8950 - val_loss: 0.3710 - val_f1: 0.9527\n",
      "Epoch 20/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5132 - f1: 0.8974 - val_loss: 0.5211 - val_f1: 0.8904\n",
      "Epoch 21/2000\n",
      "20000/20000 [==============================] - 3s 127us/sample - loss: 0.5615 - f1: 0.8813 - val_loss: 0.4058 - val_f1: 0.9418\n",
      "Epoch 22/2000\n",
      "20000/20000 [==============================] - 2s 124us/sample - loss: 0.4800 - f1: 0.9113 - val_loss: 0.4217 - val_f1: 0.9266\n",
      "Epoch 23/2000\n",
      "20000/20000 [==============================] - 3s 131us/sample - loss: 0.5106 - f1: 0.8973 - val_loss: 0.4219 - val_f1: 0.9220\n",
      "Epoch 24/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5032 - f1: 0.9010 - val_loss: 0.3877 - val_f1: 0.9418\n",
      "Epoch 25/2000\n",
      "20000/20000 [==============================] - 3s 130us/sample - loss: 0.5067 - f1: 0.8985 - val_loss: 0.4406 - val_f1: 0.9223\n",
      "Epoch 26/2000\n",
      "20000/20000 [==============================] - 2s 123us/sample - loss: 0.5083 - f1: 0.8994 - val_loss: 0.4033 - val_f1: 0.9479\n",
      "Epoch 27/2000\n",
      "20000/20000 [==============================] - 2s 124us/sample - loss: 0.5122 - f1: 0.8999 - val_loss: 0.3814 - val_f1: 0.9403\n",
      "Epoch 28/2000\n",
      "20000/20000 [==============================] - 2s 121us/sample - loss: 0.5196 - f1: 0.8960 - val_loss: 0.4356 - val_f1: 0.9222\n",
      "Epoch 29/2000\n",
      "20000/20000 [==============================] - 3s 128us/sample - loss: 0.5064 - f1: 0.8998 - val_loss: 0.4048 - val_f1: 0.9349\n"
     ]
    }
   ],
   "source": [
    "mlb=LabelBinarizer()\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    print('\\n\\nRunning through training size '+str(train_size))\n",
    "    k_folds_errors = []\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=5, train_size=train_size)\n",
    "    k = 0\n",
    "    for train_index, _ in sss.split(training_spectra, training_keys):\n",
    "        print('Running through fold '+str(k))\n",
    "        training_keys_binarized = mlb.fit_transform(training_keys.reshape([training_keys.shape[0],1]))\n",
    "        testing_keys_binarized = mlb.transform(testing_keys)\n",
    "        \n",
    "        model = compile_model(\n",
    "            build_dnn_model,\n",
    "            model_features)\n",
    "        model_weights = model.get_weights()\n",
    "        model_weights_updated = model_weights[:]\n",
    "        model_weights_updated[0:2] = dae_model.get_weights()[0:2]\n",
    "        model.set_weights(model_weights_updated)\n",
    "\n",
    "        csv_logger = tf.keras.callbacks.CSVLogger('./final-models-keras/'+model_id_save_as+'_trainsize'+str(train_size)+'_fold'+str(k)+'.log')\n",
    "\n",
    "        output = model.fit(\n",
    "            x=training_spectra_scaled[train_index],\n",
    "            y=training_keys_binarized[train_index],\n",
    "            epochs=num_epochs,\n",
    "            verbose=1,\n",
    "            validation_data=(testing_spectra_scaled,\n",
    "                             testing_keys_binarized),\n",
    "            shuffle=True,\n",
    "            callbacks=[earlystop_callback, csv_logger],\n",
    "        )\n",
    "        model.save('./final-models-keras/'+model_id_save_as+'_trainsize'+str(train_size)+'_fold'+str(k)+'.hdf5')\n",
    "        k += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36_update)",
   "language": "python",
   "name": "conda_tensorflow_p36_update"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

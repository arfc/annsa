{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "\n",
    "from annsa.template_sampling import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import model, training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annsa.model_classes import (dnn_model_features,\n",
    "                                 DNN,\n",
    "                                 save_model,\n",
    "                                 train_earlystop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_dataset = pd.read_csv('../../source-interdiction/training_testing_data/background_template_dataset.csv')\n",
    "source_dataset = pd.read_csv('../../source-interdiction/training_testing_data/shielded_templates_200kev_dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sourcedist: [50.0, 112.5, 175.0, 237.5, 300.0]\n",
      "sourceheight: [50.0, 75.0, 100.0, 125.0, 150.0]\n",
      "alum shieldingdensity: [1.82, 4.18, 7.49, 13.16]\n",
      "iron shieldingdensity: [1.53, 3.5, 6.28, 11.02]\n",
      "lead shieldingdensity: [0.22, 0.51, 0.92, 1.61]\n",
      "fwhm: [6.0, 6.5, 7.0, 7.5, 8.0, 8.5, 9.0]\n"
     ]
    }
   ],
   "source": [
    "print('sourcedist: ' + str(sorted(set(source_dataset['sourcedist']))))\n",
    "print('sourceheight: ' + str(sorted(set(source_dataset['sourceheight']))))\n",
    "print('alum shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='alum']['shieldingdensity']))))\n",
    "print('iron shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='iron']['shieldingdensity']))))\n",
    "print('lead shieldingdensity: ' + str(sorted(set(source_dataset[source_dataset['shielding']=='lead']['shieldingdensity']))))\n",
    "print('fwhm: ' + str(sorted(set(source_dataset['fwhm']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Customize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dataset = source_dataset[(source_dataset['fwhm']==7.0) | \n",
    "                                (source_dataset['fwhm']==7.5) |\n",
    "                                (source_dataset['fwhm']==8.0)]\n",
    "\n",
    "source_dataset = source_dataset[(source_dataset['sourcedist']==50.5) | \n",
    "                                (source_dataset['sourcedist']==175.0) | \n",
    "                                (source_dataset['sourcedist']==300.0)]\n",
    "\n",
    "source_dataset = source_dataset[(source_dataset['sourceheight']==50.0) |\n",
    "                                (source_dataset['sourceheight']==100.0) |\n",
    "                                (source_dataset['sourceheight']==150.0)]\n",
    "\n",
    "# remove 80% shielding\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=13.16]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=11.02]\n",
    "source_dataset = source_dataset[source_dataset['shieldingdensity']!=1.61]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove empty spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indicies dropped: [552 553 554 555 556 557 564 565 566 582 583 584 585 586 587 594 595 596\n",
      " 612 613 614 615 616 617 624 625 626 642 643 644 645 646 647 654 655 656\n",
      " 672 673 674 675 676 677 684 685 686 702 703 704 705 706 707 714 715 716]\n"
     ]
    }
   ],
   "source": [
    "zero_count_indicies = np.argwhere(np.sum(source_dataset.values[:,6:],axis=1) == 0).flatten()\n",
    "\n",
    "print('indicies dropped: ' +str(zero_count_indicies))\n",
    "\n",
    "source_dataset.drop(source_dataset.index[zero_count_indicies], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add empty spectra for background "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank_spectra = []\n",
    "for fwhm in set(source_dataset['fwhm']):\n",
    "    num_examples = source_dataset[(source_dataset['fwhm']==fwhm) &\n",
    "                                  (source_dataset['isotope']==source_dataset['isotope'].iloc()[0])].shape[0]\n",
    "    for k in range(num_examples):\n",
    "        blank_spectra_tmp = [0]*1200\n",
    "        blank_spectra_tmp[5] = fwhm\n",
    "        blank_spectra_tmp[0] = 'background'\n",
    "        blank_spectra_tmp[3] = 'background'\n",
    "        blank_spectra.append(blank_spectra_tmp)\n",
    "\n",
    "source_dataset = source_dataset.append(pd.DataFrame(blank_spectra,\n",
    "                                                    columns=source_dataset.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dataset from spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_dataset = source_dataset.values[:,5:].astype('float64')\n",
    "all_keys = source_dataset['isotope'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define online data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integration_time():\n",
    "    return np.random.uniform(10,600)\n",
    "\n",
    "def background_cps():\n",
    "    return np.random.poisson(200)\n",
    "\n",
    "def signal_to_background():\n",
    "    return np.random.uniform(0.5,2)\n",
    "\n",
    "def calibration():\n",
    "    return [np.random.uniform(0,20),\n",
    "            np.random.uniform(2500/3000,3500/3000),\n",
    "            0]\n",
    "\n",
    "online_data_augmentation = online_data_augmentation_vanilla(background_dataset,\n",
    "                                background_cps,\n",
    "                                integration_time,\n",
    "                                signal_to_background,\n",
    "                                calibration,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load testing dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = np.load('../dataset_generation/testing_dataset_full_200keV_1000.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_spectra = np.add(testing_dataset.item()['sources'], testing_dataset.item()['backgrounds'])\n",
    "testing_keys = testing_dataset.item()['keys']\n",
    "\n",
    "mlb=LabelBinarizer()\n",
    "\n",
    "all_keys_binarized = mlb.fit_transform(all_keys.reshape([all_keys.shape[0],1]))\n",
    "testing_keys_binarized = mlb.transform(testing_keys)\n",
    "training_keys_binarized = mlb.transform(all_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_hyperparameters_to_search = 256\n",
    "earlystop_errors_test = []\n",
    "model_id='DNN-onlinedataaugfull'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    '''\n",
    "    Makes a random model given some parameters.\n",
    "\n",
    "    '''\n",
    "    number_layers = choice([1, 2, 3])\n",
    "    dense_nodes = 2**np.random.randint(5, 10, number_layers)\n",
    "    dense_nodes = np.sort(dense_nodes)\n",
    "    dense_nodes = np.flipud(dense_nodes)\n",
    "    model_features = dnn_model_features(\n",
    "        learining_rate=10**np.random.uniform(-4,-1),\n",
    "        l2_regularization_scale=10**np.random.uniform(-2,0),\n",
    "        dropout_probability=np.random.uniform(0,1),\n",
    "        batch_size=2**np.random.randint(4,10),\n",
    "        output_size=training_keys_binarized.shape[1],\n",
    "        dense_nodes=dense_nodes,\n",
    "        activation_function=choice([tf.nn.tanh,tf.nn.relu,tf.nn.sigmoid]),\n",
    "        scaler=choice([make_pipeline(FunctionTransformer(np.log1p, validate=True)),\n",
    "                       make_pipeline(FunctionTransformer(np.sqrt, validate=True))]))\n",
    "\n",
    "    model = DNN(model_features)\n",
    "\n",
    "    return model, model_features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: CostFunc loss: 0.00 11.14, EarlyStop loss: 0.00 0.81\n",
      "Epoch 2: CostFunc loss: 0.00 9.12, EarlyStop loss: 0.00 0.72\n",
      "Epoch 3: CostFunc loss: 0.00 7.64, EarlyStop loss: 0.00 0.67\n",
      "Epoch 4: CostFunc loss: 0.00 6.52, EarlyStop loss: 0.00 0.65\n",
      "Epoch 5: CostFunc loss: 0.00 5.68, EarlyStop loss: 0.00 0.62\n",
      "Epoch 6: CostFunc loss: 0.00 4.99, EarlyStop loss: 0.00 0.53\n",
      "Epoch 7: CostFunc loss: 0.00 4.51, EarlyStop loss: 0.00 0.58\n",
      "Epoch 8: CostFunc loss: 0.00 4.10, EarlyStop loss: 0.00 0.51\n",
      "Epoch 9: CostFunc loss: 0.00 3.77, EarlyStop loss: 0.00 0.51\n",
      "Epoch 10: CostFunc loss: 0.00 3.49, EarlyStop loss: 0.00 0.46\n",
      "Epoch 11: CostFunc loss: 0.00 3.26, EarlyStop loss: 0.00 0.47\n",
      "Epoch 12: CostFunc loss: 0.00 3.05, EarlyStop loss: 0.00 0.44\n",
      "Epoch 13: CostFunc loss: 0.00 2.93, EarlyStop loss: 0.00 0.45\n",
      "Epoch 14: CostFunc loss: 0.00 2.79, EarlyStop loss: 0.00 0.41\n",
      "Epoch 15: CostFunc loss: 0.00 2.67, EarlyStop loss: 0.00 0.40\n",
      "Epoch 16: CostFunc loss: 0.00 2.58, EarlyStop loss: 0.00 0.40\n",
      "Epoch 17: CostFunc loss: 0.00 2.50, EarlyStop loss: 0.00 0.40\n",
      "Epoch 18: CostFunc loss: 0.00 2.41, EarlyStop loss: 0.00 0.36\n",
      "Epoch 19: CostFunc loss: 0.00 2.32, EarlyStop loss: 0.00 0.37\n",
      "Epoch 20: CostFunc loss: 0.00 2.24, EarlyStop loss: 0.00 0.35\n",
      "Epoch 21: CostFunc loss: 0.00 2.22, EarlyStop loss: 0.00 0.39\n",
      "Epoch 22: CostFunc loss: 0.00 2.12, EarlyStop loss: 0.00 0.35\n",
      "Epoch 23: CostFunc loss: 0.00 2.07, EarlyStop loss: 0.00 0.34\n",
      "Epoch 24: CostFunc loss: 0.00 2.03, EarlyStop loss: 0.00 0.34\n",
      "Epoch 25: CostFunc loss: 0.00 1.97, EarlyStop loss: 0.00 0.32\n",
      "Epoch 26: CostFunc loss: 0.00 1.95, EarlyStop loss: 0.00 0.34\n",
      "Epoch 27: CostFunc loss: 0.00 1.89, EarlyStop loss: 0.00 0.32\n",
      "Epoch 28: CostFunc loss: 0.00 1.85, EarlyStop loss: 0.00 0.34\n",
      "Epoch 29: CostFunc loss: 0.00 1.82, EarlyStop loss: 0.00 0.31\n",
      "Epoch 30: CostFunc loss: 0.00 1.79, EarlyStop loss: 0.00 0.32\n",
      "Epoch 31: CostFunc loss: 0.00 1.76, EarlyStop loss: 0.00 0.32\n",
      "Epoch 32: CostFunc loss: 0.00 1.73, EarlyStop loss: 0.00 0.31\n",
      "Epoch 33: CostFunc loss: 0.00 1.69, EarlyStop loss: 0.00 0.31\n",
      "Epoch 34: CostFunc loss: 0.00 1.65, EarlyStop loss: 0.00 0.29\n",
      "Epoch 35: CostFunc loss: 0.00 1.65, EarlyStop loss: 0.00 0.31\n",
      "Epoch 36: CostFunc loss: 0.00 1.62, EarlyStop loss: 0.00 0.31\n",
      "Epoch 37: CostFunc loss: 0.00 1.58, EarlyStop loss: 0.00 0.29\n",
      "Epoch 38: CostFunc loss: 0.00 1.57, EarlyStop loss: 0.00 0.29\n",
      "Epoch 39: CostFunc loss: 0.00 1.53, EarlyStop loss: 0.00 0.28\n",
      "Epoch 40: CostFunc loss: 0.00 1.52, EarlyStop loss: 0.00 0.28\n",
      "Epoch 41: CostFunc loss: 0.00 1.52, EarlyStop loss: 0.00 0.29\n",
      "Epoch 42: CostFunc loss: 0.00 1.49, EarlyStop loss: 0.00 0.28\n",
      "Epoch 43: CostFunc loss: 0.00 1.47, EarlyStop loss: 0.00 0.27\n",
      "Epoch 44: CostFunc loss: 0.00 1.44, EarlyStop loss: 0.00 0.27\n",
      "Epoch 45: CostFunc loss: 0.00 1.43, EarlyStop loss: 0.00 0.27\n",
      "Epoch 46: CostFunc loss: 0.00 1.40, EarlyStop loss: 0.00 0.27\n",
      "Epoch 47: CostFunc loss: 0.00 1.40, EarlyStop loss: 0.00 0.26\n",
      "Epoch 48: CostFunc loss: 0.00 1.39, EarlyStop loss: 0.00 0.28\n",
      "Epoch 49: CostFunc loss: 0.00 1.37, EarlyStop loss: 0.00 0.26\n",
      "Epoch 50: CostFunc loss: 0.00 1.37, EarlyStop loss: 0.00 0.27\n",
      "Epoch 51: CostFunc loss: 0.00 1.34, EarlyStop loss: 0.00 0.24\n",
      "Epoch 52: CostFunc loss: 0.00 1.31, EarlyStop loss: 0.00 0.25\n",
      "Epoch 53: CostFunc loss: 0.00 1.31, EarlyStop loss: 0.00 0.25\n",
      "Epoch 54: CostFunc loss: 0.00 1.31, EarlyStop loss: 0.00 0.25\n",
      "Epoch 55: CostFunc loss: 0.00 1.28, EarlyStop loss: 0.00 0.23\n",
      "Epoch 56: CostFunc loss: 0.00 1.30, EarlyStop loss: 0.00 0.25\n",
      "Epoch 57: CostFunc loss: 0.00 1.28, EarlyStop loss: 0.00 0.26\n",
      "Epoch 58: CostFunc loss: 0.00 1.26, EarlyStop loss: 0.00 0.25\n",
      "Epoch 59: CostFunc loss: 0.00 1.25, EarlyStop loss: 0.00 0.23\n",
      "Epoch 60: CostFunc loss: 0.00 1.24, EarlyStop loss: 0.00 0.25\n",
      "Epoch 61: CostFunc loss: 0.00 1.24, EarlyStop loss: 0.00 0.25\n",
      "Epoch 62: CostFunc loss: 0.00 1.21, EarlyStop loss: 0.00 0.23\n",
      "Epoch 63: CostFunc loss: 0.00 1.21, EarlyStop loss: 0.00 0.24\n",
      "Epoch 64: CostFunc loss: 0.00 1.19, EarlyStop loss: 0.00 0.24\n",
      "Test error at early stop: Objectives fctn: 1.28 Early stopfctn: 1.28\n",
      "Epoch 1: CostFunc loss: 0.00 86.32, EarlyStop loss: 0.00 0.78\n",
      "Epoch 2: CostFunc loss: 0.00 34.80, EarlyStop loss: 0.00 0.66\n",
      "Epoch 3: CostFunc loss: 0.00 16.53, EarlyStop loss: 0.00 0.66\n",
      "Epoch 4: CostFunc loss: 0.00 9.28, EarlyStop loss: 0.00 0.54\n",
      "Epoch 5: CostFunc loss: 0.00 6.13, EarlyStop loss: 0.00 0.53\n",
      "Epoch 6: CostFunc loss: 0.00 4.57, EarlyStop loss: 0.00 0.54\n",
      "Epoch 7: CostFunc loss: 0.00 3.71, EarlyStop loss: 0.00 0.51\n",
      "Epoch 8: CostFunc loss: 0.00 3.15, EarlyStop loss: 0.00 0.47\n",
      "Epoch 9: CostFunc loss: 0.00 2.89, EarlyStop loss: 0.00 0.48\n",
      "Epoch 10: CostFunc loss: 0.00 2.67, EarlyStop loss: 0.00 0.49\n",
      "Epoch 11: CostFunc loss: 0.00 2.54, EarlyStop loss: 0.00 0.46\n",
      "Epoch 12: CostFunc loss: 0.00 2.34, EarlyStop loss: 0.00 0.40\n",
      "Epoch 13: CostFunc loss: 0.00 2.27, EarlyStop loss: 0.00 0.41\n",
      "Epoch 14: CostFunc loss: 0.00 2.17, EarlyStop loss: 0.00 0.42\n",
      "Epoch 15: CostFunc loss: 0.00 2.24, EarlyStop loss: 0.00 0.49\n",
      "Epoch 16: CostFunc loss: 0.00 2.22, EarlyStop loss: 0.00 0.43\n",
      "Epoch 17: CostFunc loss: 0.00 2.06, EarlyStop loss: 0.00 0.46\n",
      "Epoch 18: CostFunc loss: 0.00 2.00, EarlyStop loss: 0.00 0.43\n",
      "Epoch 19: CostFunc loss: 0.00 1.95, EarlyStop loss: 0.00 0.39\n",
      "Epoch 20: CostFunc loss: 0.00 1.97, EarlyStop loss: 0.00 0.43\n",
      "Epoch 21: CostFunc loss: 0.00 2.01, EarlyStop loss: 0.00 0.41\n",
      "Epoch 22: CostFunc loss: 0.00 1.98, EarlyStop loss: 0.00 0.42\n",
      "Epoch 23: CostFunc loss: 0.00 1.91, EarlyStop loss: 0.00 0.44\n",
      "Epoch 24: CostFunc loss: 0.00 1.78, EarlyStop loss: 0.00 0.35\n",
      "Epoch 25: CostFunc loss: 0.00 1.83, EarlyStop loss: 0.00 0.40\n",
      "Epoch 26: CostFunc loss: 0.00 1.78, EarlyStop loss: 0.00 0.36\n",
      "Epoch 27: CostFunc loss: 0.00 1.95, EarlyStop loss: 0.00 0.48\n"
     ]
    }
   ],
   "source": [
    "testing_errors = []\n",
    "\n",
    "for network_id in range(number_hyperparameters_to_search):\n",
    "    # reset model on each iteration\n",
    "    model, model_features = make_model()\n",
    "    \n",
    "    model_features.activation_function = tf.nn.sigmoid\n",
    "    model_features.batch_size = 32\n",
    "    model_features.dense_nodes = [256]\n",
    "    model_features.dropout_probability = 0.63\n",
    "    model_features.l2_regularization_scale = 0.04293\n",
    "    model_features.learining_rate = 0.0001212\n",
    "    model_features.scaler = make_pipeline(FunctionTransformer(np.sqrt, validate=True))\n",
    "    \n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(model_features.learining_rate)\n",
    "    \n",
    "    costfunction_errors_tmp, earlystop_errors_tmp = train_earlystop(\n",
    "            training_data=spectra_dataset,\n",
    "            training_keys=training_keys_binarized,\n",
    "            testing_data=testing_spectra,\n",
    "            testing_keys=testing_keys_binarized,\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            num_epochs=500,\n",
    "            obj_cost=model.cross_entropy,\n",
    "            earlystop_cost_fn=model.f1_error,\n",
    "            earlystop_patience=10,\n",
    "            verbose=True,\n",
    "            fit_batch_verbose=1,\n",
    "            data_augmentation=online_data_augmentation)\n",
    "    testing_errors.append(earlystop_errors_tmp)\n",
    "\n",
    "    # np.save('./final-models/final_test_errors_'+model_id, training_errors)\n",
    "    # model.save_weights('./final-models/'+model_id+'_checkpoint_'+str(network_id))\n",
    "    network_id += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
